{"meta":{"title":"张先森的代码小屋","subtitle":null,"description":"明镜止水","author":"nullcc","url":"https://nullcc.github.io"},"pages":[{"title":"关于","date":"2022-04-15T03:41:13.043Z","updated":"2022-04-15T03:41:13.043Z","comments":true,"path":"about/index.html","permalink":"https://nullcc.github.io/about/index.html","excerpt":"","text":"About Me前CSO (Chan Shi Officer) 一枚乐高建筑/科技系列爱好者脚本语言爱好者乌龙茶爱好者非湖北籍热干面爱好者桌球手残选手FPS游戏手残选手 Programming Language:Node.jsPythonGoLua Position后端开发工程师 Githubhttps://github.com/nullcc Bloghttps://nullcc.github.io Emailnullcc#gmail.com (请用 ‘@’ 替换 ‘#’)"},{"title":"Tagcloud","date":"2017-04-26T09:47:43.000Z","updated":"2022-04-15T03:41:13.193Z","comments":true,"path":"tags/index.html","permalink":"https://nullcc.github.io/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2017-11-30T16:00:00.000Z","updated":"2022-04-15T03:41:13.192Z","comments":true,"path":"categories/index.html","permalink":"https://nullcc.github.io/categories/index.html","excerpt":"","text":""},{"title":"todo","date":"2022-04-15T03:41:13.193Z","updated":"2022-04-15T03:41:13.193Z","comments":true,"path":"todo/index.html","permalink":"https://nullcc.github.io/todo/index.html","excerpt":"","text":"MQ 分布式监控系统 docker 自动化部署 单元测试 微服务 软件团队建设"}],"posts":[{"title":"领域驱动设计实践(0)-基础概念","slug":"领域驱动设计实践(0)-基础概念","date":"2022-07-18T16:00:00.000Z","updated":"2022-07-29T06:26:20.599Z","comments":true,"path":"2022/07/19/领域驱动设计实践(0)-基础概念/","link":"","permalink":"https://nullcc.github.io/2022/07/19/领域驱动设计实践(0)-基础概念/","excerpt":"本文解释了领域驱动设计的一些基础概念。","text":"本文解释了领域驱动设计的一些基础概念。 传统三层架构的开发困境进入计算机行业有些年头了，在大大小小的项目中工作过，大部分后端项目使用的开发模式都是三层架构。三层架构将整个系统的分成三部分： UI (表现层) BLL (业务逻辑层) DAL (数据访问层) 三层架构的一种常见实现方式如下图： 在三层架构中，表现层的 Controller 负责和用户（用户可能是真人或机器）交互，负责验证请求数据的有效性并将数据包装成业务逻辑层容易处理的形式。业务逻辑层中的 Service 从 Controller 接收数据，然后处理业务逻辑，过程中可能会和数据访问层交互来获取数据，数据访问层使用 DAO 从持久化系统中获取和更新数据。一些和第三方系统的交互（比如集成的 SDK、邮件通知等）一般也会放在业务逻辑层的 Service 中。 三层架构对整个系统的切分方式很好理解，上手简单，但同时也存在不少缺点。随着项目的迭代，该架构中间的业务逻辑层会变得越来越厚，表现层和数据访问层相对于业务逻辑层是相当薄的一层。整个系统两头薄中间厚，显得很不协调。由于所有业务逻辑全都聚集在业务逻辑层，单个 Service 文件动辄上千行相当常见，多个程序员协作在同一个文件上，其实是很不利于增加新特性和维护的。除此之外，系统中还会出现大量业务对象，这些对象的名字都和业务相关，但内部除了一些属性和 getter setter 之外，几乎没有任何业务逻辑，我们称这些对象为贫血领域对象。这些对象的最大用处就是承载数据让业务逻辑层中的具体业务方法处理。除此之外，如果有对接第三方系统的需求，业务逻辑层中还会出现各种外部 SDK 或对外部系统调用的封装，当这些东西和我们的核心业务逻辑混杂在一起时是不利于维护的。 这种实现方式不协调的地方不仅仅体现在代码体量和内部大量的贫血领域对象上，还体现在依赖关系上。基于三层架构的传统 MVC 开发方式中的依赖关系一般是这样的（-&gt;表示依赖方向）：用户接口层 -&gt; 应用层 -&gt; 领域层 -&gt; 基础设施层。举一个例子，我们一般在 Service 中调用 DAO 来进行数据操作，如从数据库读取或向数据库写入，依赖方向是 Service -&gt; DAO。如果之后出于性能优化目的我们希望先尝试访问缓存获取数据，如果没有再从数据库获取，则此时需要修改 Service。之前提到 Service 是承载业务逻辑的一层，而从数据源获取数据属于技术细节，技术细节不应该对业务逻辑造成影响，这里有一个阻抗失调的问题。 为了解决这种依赖关系失调，我们需要使用一种全新的软件建模方式。 Clean Architecture 与领域驱动设计Robert C. Martin (Uncle Bob) 在 the-clean-architecture 这篇文章中提到了 clean architecture 的概念。其中列出了几种架构选型，下面将一些重点信息引用并翻译如下： 这些架构尽管在细节上可能有所不同，但却非常相似。它们的目标是一致的，即关注点分离。它们都通过将软件分层来实现这种分离，每种架构都至少有一层用于业务规则，另一层作为接口。 这些架构产生的系统具有这样的属性： 独立于框架。架构不依赖于某些功能丰富的软件库的存在。这使你可以将这些框架用作工具，而不必使系统被它们所限制。 可测试的。可以在没有UI、数据库、Web 服务器或其他任何外部元素的情况下测试业务规则。 独立于UI。UI 可以被轻松更改，而无需更改系统的其余部分。例如，可以用控制台 UI 替换 Web UI，而无需更改业务规则。 独立于数据库。你可以将 Oracle 或 SQL Server 换成 Mongo、BigTable、CouchDB 或其他东西。你的业务规则不与数据库绑定。 独立于任何外部事物。事实上，你的业务规则根本不了解外部世界。 文章配了一张图来表达 Clean Architecture 的思想： 另外，文中还提到了软件开发中依赖关系的一些规则： 图中的每个同心圆表示软件中的一层，越往内部软件的层次越高，也越抽象。外圈是机制（实现），内圈是政策（规则）。 使这个架构工作的最重要的规则是依赖规则。该规则表示源代码依赖项只能指向内部。内圈中的任何东西都对外圈中的事物一无所知 特别是，在外圈中声明的事物的名称不能被内圈中的代码提及。这包括函数、类、变量或任何其他命名的软件实体。 同样，在外圈中使用的数据格式也不应该被内圈使用，特别是如果这些格式是由外圈中的框架生成的。我们不希望外圈的任何东西影响内圈。 上面的所有要点全都指向两个核心概念：软件的隔离和抽象。 文章中提到的架构选型中有一种叫作 Hexagonal Architecture(六边形架构，又叫端口和适配器)，架构风格的示意图如下： 我们分析一下这幅图中的元素。 接口与基础设施层最外层是接口和基础设施层。Controller 位于接口层，Controller 有很多种类型，包括但不限于 HTTP Controller, CLI Controller 和 Event Controller。Controller 的外部是各种调用方，Controller 主要负责和调用方通信，调用方可以是真实用户、外部服务或其他任何主动对当前系统产生影响的事物。 基础设施层包含各种 Adapters，比如 Repository Adapter 实现了如何从数据库中存取数据的细节。External Resources Adapters 实现了如何和外部资源沟通的细节。Microservices Adapters 实现了微服务间的通信细节等等。 接口与基础设施层与核心层的边界注意看接口与基础设施层和它相邻的一层核心层的边界，有三种组件： Queries (查询) Commands (命令) Ports (端口) 对于外部任何事物对当前系统的主动调用，可以分两种类型：Queries (查询)和 Commands (命令)。Queries 是读操作，一般是幂等的，Commands 是更新操作，会修改系统中的数据。Ports 是在核心层声明的抽象概念，需要由基础设施层中的各种 Adapters 实现。 部分 Queries 可以直接跳过核心层使用 Repository Adapter 读取数据返回。 Controller 会对进来 DTO 进行简单包装，生成 Commands 对象。Commands 对象承载了具体 Use Case 需要的数据，它穿过接口与基础设施层与核心层的边界进入核心层。 领域层中的业务逻辑只能依赖 Ports，具体的实现在对应的 Adapters 中。 在传统的设计中是在业务规则中调用具体实现，这相当于让抽象程度高的组件依赖抽象程度低的组件。此时如果技术细节变更时（比如换了一种数据数据库或外部服务接口变了），核心层也需要修改。如果在设计上实现依赖倒置，则一般只需要修改基础设施层（除非在核心层中的接口本身发生变化）。这样的设计实现了业务逻辑和具体实现的解耦，在核心层接口不变的情况下，外部的变更对核心层几乎没有影响。 核心层核心层的抽象程度比接口与基础设施层与核心层更高，我们在这里定义业务规则。图中核心层内部还有三层： Application Services (应用服务) Domain Services (领域服务) Entities (实体) Application Services (应用服务)Application Services 是基于用例的，实际上就是一个个 Use Cases，它负责编排用例执行流程。Application Services 不包含任何特定领域的业务逻辑，它应该是相对较薄的一层。Application Services 通过使用 Ports 声明其为了执行领域逻辑需要的基础设施层的依赖。当需要与外部世界通信（比如发送 email）时，也使用 Ports。Ports 还相当一个防腐层(Anti-corruption layer, ACL)，由于我们核心层的抽象程度较高，让它们直接依赖于具体实现细节（比如一个 SDK 或对外部服务的调用就是一种具体实现细节）显然不合适。好的实践是，当核心层需要某种能力时，我们声明这种能力作为 Ports，外部的基础设施层的某个 Adapter 实现这个 Ports 的能力，然后在需要时我们将这个 Adapter 注入到 核心层中。当我们想要替换具体实现时，只要换一个 Adapter 注入进去即可。 Domain Services (领域服务)Domain Services 被用于处理那些“领域中不属于实体或值对象的天然职责的那些重要的过程或转换”。Domain Services 是一种特定类型的领域层类，用于执行依赖于两个或更多实体的领域逻辑。如果有一些逻辑，将它们放在某个 Entities 中时会破坏它们的封装性，那就提取出来放到 Domain Services 中。 Entities (实体)Entities 是领域的核心，它们封装了业务规则。Entities 代表业务模型，表达特定模型具有的属性。 在各个层中穿越要让整个系统运行起来，除了分层我们还需要让数据真正流动起来，这就涉及在不同层之间的数据流传。数据会在各层间穿越，这里的问题是数据以什么样的形式穿过各层。一般来说，我们需要在各层的边界上创建 DTO 来包装其他层穿越过来数据，只包装需要的数据字段。 参考资料The Clean Architecturedomain-vs-application-services领域驱动的六边形架构","categories":[{"name":"领域驱动设计","slug":"领域驱动设计","permalink":"https://nullcc.github.io/categories/领域驱动设计/"}],"tags":[{"name":"领域驱动设计","slug":"领域驱动设计","permalink":"https://nullcc.github.io/tags/领域驱动设计/"}]},{"title":"构建可观察的动态命令行程序","slug":"构建可观察的动态命令行程序","date":"2022-04-24T16:00:00.000Z","updated":"2022-05-12T06:55:08.386Z","comments":true,"path":"2022/04/25/构建可观察的动态命令行程序/","link":"","permalink":"https://nullcc.github.io/2022/04/25/构建可观察的动态命令行程序/","excerpt":"本文讨论了构建可观察的动态命令行程序的一些实践。","text":"本文讨论了构建可观察的动态命令行程序的一些实践。 需求有些时候我们会开发出具有如下特征的命令行程序： * 需要运行较长时间（几分钟至数小时，甚至数天），并有可能会放在持续集成系统中运行（例如Jenkins） * 参数较多，且可以灵活配置 这种命令行程序在自动化测试领域很常见，例如用来批量生成某些数据、批量运行自动化测试用例等。这些程序的运行方式一般会从开发者在本地运行慢慢变成在持续集成系统中运行，从开发者主动触发运行慢慢变成定时地或被某些上游系统以调用持续集成系统API的方式自动运行。 动态修改命令行程序的参数一般来说，一旦一个命令行程序开始运行，此时如果用户想修改某些参数，那就必须停掉正在运行的程序，修改参数后再重新运行这个程序。这在大部分情况下是没问题的，但如果用户希望在不停止程序运行的情况下动态地修改参数怎么办呢？有一些情形下我们希望具备这种能力，这里列举几个： 1. 一个较大的自动化测试用例集已经运行了一段时间，此时用户发现原来的用例集不全，还需要再加入一些新用例。 2. 一个数据准备程序已经运行了一段时间，此时用户希望加入一些新数据。 3. 一个会向后端组件发送大量请求的命令行程序，刚开始时由于后端资源紧张并发数很低，之后后端资源充裕了想要提高并发数。 上述情况下，最直接的做法肯定是停掉正在运行的程序重新运行，但这么做也是有成本的，需要花费更多时间重启程序、或者在最后需要聚合多份自动化测试报告。重启程序的成本根据实际情况或高或低，不过无论如何，成本确实存在。 监控命令行程序还有一种情形是用户希望看到长时间运行的命令行程序的状态，比如进度、成功率、并发数等信息。这些信息是动态的，传统方式下一般通过输出日志来实现。那还有没有其他的方式呢？能否把这些数据集成到 dashboard 里进行监控？ 什么是可观察的动态命令行程序？可观察的命令行程序有了上面的需求，很自然地会去思考如何解决。先来看观察命令行程序的可能方式： 1. 输出日志/文件 2. 将数据写到外部系统(数据库/信息收集系统) 3. 在本地开放 HTTP API 1. 输出日志/文件这是最简单直接的方式，优点是一目了然，方便保留过程日志，缺点是不利于提供即时信息，对监控和集成不友好。 2. 将数据写到外部系统(数据库/信息收集系统)这种方式的优点是既可以保留过程日志也可以提供实时信息，缺点是开销较大，而且引入了外部依赖。 3. 在本地开放 HTTP API这种方式会在命令行运行的本地启动一个 HTTP Server，对外提供 HTTP API。优点是方便获取及时信息，对监控和集成友好，缺点是不利于保留过程日志。 如何选择？我们会比较希望既可以保留过程日志又可以监控即时信息，那么可以组合使用1和3，或者2和3。 动态再来看可动态这个特性。一般来说，动态意味着可在运行时修改行为。对于命令行程序来说，有几种可能的方式： 1. 通过一个配置文件，命令行程序定期去读，用来更新内部数据 2. 监听消息队列 3. 通过本地开放的 HTTP API 定期读配置文件虽然可以达到目的，但不太方便，尤其在 Jenkins 上执行时，用户需要远程连接到执行机来修改配置。很多情况下用户是没有权限这样做的。 监听消息队列监听消息队列需要引入外部依赖，成本较高。 通过本地开放的 HTTP API这种方式适用于本地和远程执行，也可以和上面提到的监控即时信息的做法相契合。 如何选择？综合来看，通过在本地开放 HTTP API 来提供可观察的即时数据和动态修改程序行为是一种比较靠谱的方式。 实现程序入口通过在本地开放 HTTP API 来构建可观察的动态命令行程序有很多方式，这里我给出一个我觉得比较优雅的实现方式以供参考。 首先要说明的是，实现这个想法本身是框架无关的，可以基于任何框架去实现，甚至不用框架。不过既然有合适的工具，我们没理由置之不理，因此下面讨论的实现还是使用了框架来做。先简要介绍下这种方式，我使用了 nest.js 这个框架，这是个类 Spring 的 TS/JS 框架，内建了依赖注入支持，还有不少官方和第三方的优质插件。 我创建了一个示例工程 any-factory 来展示如何实现。这个示例展示了一个工厂，我们可以通过命令行指定需要生成哪些产品，以及它们的个数。另外还提供了可选的 HTTP Server 来实时获取工厂内部数据和动态修改待生产的产品参数的能力。 先来看入口文件 main.ts: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667// src/main.tsimport &#123; NestFactory &#125; from '@nestjs/core';import &#123; ConsoleLogger &#125; from '@nestjs/common';import &#123; BootstrapConsole &#125; from 'nestjs-console';import &#123; DocumentBuilder, SwaggerModule &#125; from '@nestjs/swagger';import &#123; AppModule &#125; from './app.module';import &#123; ExceptionInterceptor &#125; from '@infrastructure/interceptors/exception.interceptor';export class CustomBootstrapConsole extends BootstrapConsole &#123; async create() &#123; const app = await NestFactory.create(AppModule); const appLogger = new ConsoleLogger('Any Factory'); const config = new DocumentBuilder() .setTitle('Any Factory') .setDescription('The any factory web API description') .setVersion('1.0') .addTag('Any Factory') .build(); const document = SwaggerModule.createDocument(app, config); SwaggerModule.setup('docs', app, document); app.useGlobalInterceptors(new ExceptionInterceptor()); app.enableShutdownHooks(); app.getHttpServer().on('listening', () =&gt; &#123; const port = app.getHttpServer().address().port; appLogger.log(`Application is listening on port $&#123;port&#125;`); &#125;); await app.listen(0); return app; &#125;&#125;const withServer = (): boolean =&gt; &#123; return !!process.argv.find((e) =&gt; e === '--with-server');&#125;;const getBootstrap = (): BootstrapConsole =&gt; &#123; if (withServer()) &#123; return new CustomBootstrapConsole(&#123; module: AppModule, withContainer: true, // This is the key that will give you access to the app container from the service cli useDecorators: true, contextOptions: &#123; logger: false &#125;, &#125; as any); &#125; return new BootstrapConsole(&#123; module: AppModule, useDecorators: true, &#125;);&#125;;const bootstrap = getBootstrap();bootstrap .init() .then(async (app) =&gt; &#123; await app.init(); await bootstrap.boot(); &#125;) .catch((e) =&gt; &#123; process.exit(1); &#125;); 入口文件有几个值得一提的地方，首先是使用了 nestjs-console 这个 npm package，由于 nest.js 默认不提供命令行的使用方式，因此我们需要这个库来提供命令行的支持。我们实现了一个 CustomBootstrapConsole 类，该类在 create 方法里创建了一个 nest.js HTTP server，并内部配置了一个 swagger module，最后监听在0端口。这里有个小技巧，如果监听0端口，则操作系统会随机分配一个可用的端口号。这么做的好处是我们不需要显式指定监听的端口号，可以在机器上同时运行多个该程序而不用担心端口号冲突。 withServer 方法用来判断是否需要启动一个 HTTP server：当命令行参数中存在 --with-server 选项时启动一个 HTTP server。 getBootstrap 方法作为 main.ts 的入口方法，负责判断命令行程序的启动方式：带 HTTP Server 或者不带。注意看 getBootstrap 的实现，当 withServer() 为 true 时，我们创建了一个 CustomBootstrapConsole 实例，参数 withContainer: true 非常重要，它允许我们从 CLI 中访问 app container，在这里就是 module: AppModule。这个特性非常重要，它允许 CLI 和 我们的应用程序通信。这里我们倒不需要去追究它的实现方式。接着就是初始化了，这部分 nest.js 框架的方法会帮我们处理。 有一幅图帮助理解这个项目的基本结构： 接着看 app.module.ts 里的 AppModule： 1234567891011121314151617// src/app.module.tsimport &#123; Module &#125; from '@nestjs/common';import &#123; ConfigModule &#125; from 'nestjs-config';import * as path from 'path';import &#123; MonitorModule &#125; from '@modules/monitor/monitor.module';import &#123; ProductionModule &#125; from '@modules/production/production.module';@Module(&#123; imports: [ ConfigModule.load(path.resolve(__dirname, 'config', '**/!(*.d).&#123;ts,js&#125;')), MonitorModule, ProductionModule, ], controllers: [], providers: [],&#125;)export class AppModule &#123;&#125; 抛开 nest.js 具体的使用细节不谈（尽管这也很重要，但目前它们对我们来说是手段而不是目的，故先避开），在 AppModule 中我们导入了3个 module: ProductionModule, MonitorModule 和 ConfigModule。ProductionModule 是这个项目的核心上下文，负责生产产品。MonitorModule 负责提供可观察、动态修改命令行程序的入口。ConfigModule 负责配置处理文件。为了保持示例的简单，这里只导入3个modules，但已足够说明问题了。 生产模块接着来看 ProductionModule: 1234567891011121314151617181920212223// src/modules/production/production.module.tsimport &#123; Module, Global &#125; from '@nestjs/common';import &#123; CqrsModule &#125; from '@nestjs/cqrs';import &#123; ConsoleModule &#125; from 'nestjs-console';import &#123; ProduceProductCliController &#125; from './commands/produce-product/produce-product.cli.controller';import &#123; ProduceProductService &#125; from './commands/produce-product/produce-product.service';import &#123; UpdateSchedulerService &#125; from '../monitor/commands/update-scheduler/update-scheduler.service';import &#123; productionServiceLoggerProvider &#125; from './providers/production.providers';const cliControllers = [ProduceProductCliController];const commandHandlers = [ProduceProductService, UpdateSchedulerService];const customProviders = [productionServiceLoggerProvider];@Global()@Module(&#123; imports: [CqrsModule, ConsoleModule], controllers: [], providers: [...cliControllers, ...commandHandlers, ...customProviders], exports: [...commandHandlers],&#125;)export class ProductionModule &#123;&#125; production.module.ts 这种模块文件本身没有任何逻辑，它只是一个模块的入口，负责组装模块。imports 字段指明该模块依赖两个外部模块 CqrsModule 和 ConsoleModule，稍后会解释其含义。controllers 字段本来是用来声明需要注入的 HTTP controllers，但在这里 ProductionModule 并不直接提供 HTTP API，所以是个空数组。我们稍后会看到这个字段在 MonitorModule 是有用的。在 providers 字段中声明了一些东西： cliControllers: 命令行相关的 controllers，即用户和应用程序核心之间的命令行控制器。 commandHandlers: 负责执行命令行控制器接收到的命令，cliControllers 是 commandHandlers 的直接用户。 customProviders: 一些自定义 providers，这里只有一个 logger。 接着看 produce-product.cli.controller.ts: 123456789101112131415161718192021222324252627282930313233343536373839404142// src/modules/production/commands/produce-product/produce-product.cli.controller.tsimport &#123; Injectable &#125; from '@nestjs/common';import &#123; CommandBus &#125; from '@nestjs/cqrs';import &#123; Command, Console &#125; from 'nestjs-console';import &#123; ProduceProductCommand &#125; from './produce-product.command';import &#123; Production &#125; from '@src/interface-adapters/interfaces/production/production.interface';@Console()@Injectable()export class ProduceProductCliController &#123; constructor(private readonly commandBus: CommandBus) &#123;&#125; @Command(&#123; command: 'produce-products', description: 'Produce products', options: [ &#123; flags: '-s, --specs &lt;specs&gt;', required: true, fn: (value) =&gt; value.split(';'), description: 'Product specs', &#125;, &#123; flags: '-c, --concurrency &lt;concurrency&gt;', required: false, defaultValue: 1, fn: (value) =&gt; parseInt(value), description: 'Concurrency of pipeline', &#125;, &#123; flags: '--with-server', required: false, description: 'Will start a HTTP server to provide a way to inspect some internal data if specified', &#125;, ], &#125;) async produceProducts(opts: Production): Promise&lt;void&gt; &#123; const command = new ProduceProductCommand(opts); await this.commandBus.execute(command); &#125;&#125; @Console() 装饰器声明这是一个命令行控制器，另外在构造参数中有一个 CommandBus，这个东西来自 @nestjs/cqrs 这个 package，CQRS 表示命令查询职责分离，也就是我们将读操作和写操作分开处理，这在微服务和领域驱动设计(DDD)的实践中有非常多的应用，由于这次的主题并不是关于 CQRS 的，这里只是顺带一提。简单来说在 CQRS 将写操作视为一种命令(command)，这里的 CommandBus 提供了一种执行命令的一致性方法。在 produceProducts 方法中，我们使用 @Command() 装饰器声明了一个命令，用来生产产品，里面的参数简单易懂，这里就不细说了。继续看这个方法的实现，我们使用方法入参 opt 实例化了一个 ProduceProductCommand 命令对象，这个对象其实只是一个命令信息的载体而已，没什么特别的： 1234567891011121314151617// src/modules/production/commands/produce-product/produce-product.command.tsimport &#123; Command, CommandProps,&#125; from '@src/libs/ddd/domain/base-classes/command.base';export class ProduceProductCommand extends Command &#123; constructor(props: CommandProps&lt;ProduceProductCommand&gt;) &#123; super(props); this.specs = props.specs; this.concurrency = props.concurrency; &#125; readonly specs: string[]; readonly concurrency: number;&#125; 真正有意思的地方是： 1await this.commandBus.execute(command); 这就是上面提到的 执行命令的一致性方法。它的神奇之处在于在 produceProducts 方法中，我们不需要了解谁会负责处理这个 ProduceProductCommand 命令对象，我们只需要知道把这个命令对象传给 this.commandBus 的 execute 方法就行了。 那谁会来处理 ProduceProductCommand 命令对象呢？它是： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100// src/modules/production/commands/produce-product/produce-product.service.tsimport &#123; Injectable, Inject &#125; from '@nestjs/common';import &#123; CommandBus, CommandHandler &#125; from '@nestjs/cqrs';import &#123; ConfigService &#125; from 'nestjs-config';import &#123; Result &#125; from '@libs/ddd/domain/utils/result.util';import &#123; Logger &#125; from '@libs/ddd/domain/ports/logger.port';import &#123; CommandHandlerBase &#125; from '@src/libs/ddd/domain/base-classes/command-handler.base';import &#123; ProduceProductCommand &#125; from './produce-product.command';import &#123; PipelineEntity &#125; from '@modules/production/domain/entities/pipeline.entity';import &#123; Production &#125; from '@modules/production/domain/value-objects/production.value-object';import &#123; Spec &#125; from '@modules/production/domain/value-objects/spec.value-object';import &#123; Concurrency &#125; from '@modules/production/domain/value-objects/concurrency.value-object';import &#123; Summary &#125; from '@modules/production/domain/value-objects/summary.value-object';import &#123; produceProductServiceLoggerSymbol &#125; from '@modules/production/providers/production.providers';@Injectable()@CommandHandler(ProduceProductCommand)export class ProduceProductService extends CommandHandlerBase &#123; private pipelineEntity: PipelineEntity; private isRunning = false; constructor( private readonly commandBus: CommandBus, @Inject(produceProductServiceLoggerSymbol) private readonly logger: Logger, private readonly config: ConfigService, ) &#123; super(); &#125; async handle( command: ProduceProductCommand, ): Promise&lt;Result&lt;boolean, Error&gt;&gt; &#123; const specServer = this.config.get('app.specServer'); const production = new Production(&#123; specs: command.specs.map((spec) =&gt; &#123; const [name, count] = spec.split(':'); return new Spec(&#123; name, count: isNaN(parseInt(count)) ? 1 : parseInt(count), &#125;); &#125;), concurrency: new Concurrency(&#123; n: command.concurrency, &#125;), &#125;); this.logger.log(`Spec server: $&#123;specServer&#125;`); this.logger.log( `Produces products: $&#123;JSON.stringify(production.getRawProps(), null, 2)&#125;`, ); const result = PipelineEntity.create(&#123; production: production &#125;); return result.unwrap( async (pipeline) =&gt; &#123; this.isRunning = true; this.pipelineEntity = pipeline; await this.pipelineEntity.run(); return Result.ok(true); &#125;, async (error) =&gt; &#123; return Result.err(error); &#125;, ); &#125; isAvailable(): boolean &#123; return this.isRunning; &#125; getConcurrency(): number &#123; return this.pipelineEntity.getConcurrency(); &#125; setConcurrency(value: number) &#123; this.logger.log(`Sets concurrency to: $&#123;value&#125;`); const newConcurrency = new Concurrency(&#123; n: value, &#125;); this.pipelineEntity.setConcurrency(newConcurrency); &#125; addSpecs(specs: string[]) &#123; this.logger.log(`Adds specs: $&#123;specs&#125;`); const additionalSpecs = specs.map((spec) =&gt; &#123; const [name, count] = spec.split(':'); return new Spec(&#123; name, count: isNaN(parseInt(count)) ? 1 : parseInt(count), &#125;); &#125;); this.pipelineEntity.addSpecs(additionalSpecs); &#125; getSummary(): Summary &#123; return this.pipelineEntity.getSummary(); &#125; getSpecs(): Spec[] &#123; return this.pipelineEntity.getSpecs(); &#125;&#125; 又是一长串代码，我们只看要点。下面的装饰器声明了这个类会负责处理 ProduceProductCommand 命令对象： 1@CommandHandler(ProduceProductCommand) 第二个重点是 handle 方法。如果你仔细看会发现这个方法已经碰到一点点业务的边了，但这还不是真正的业务逻辑，只是业务逻辑的组装。这个位置对应到领域驱动设计中是应用层(Appplicaiton Layer)。在这个方法中，我们先实例化了一个 Production 值对象(Value Object)，值对象不包含业务逻辑，它用来表示领域概念。比如在这里表示生产这个概念。需要注意的是，在这个建模过程中，我们始终紧密围绕生产这个核心上下文进行建模，以更好地表达领域语言。再往后看，出现了一个 PipelineEntity，Production 值对象作为 PipelineEntity 的 create 工厂方法的参数。PipelineEntity 是整个生产过程的核心，几乎所有业务逻辑都封装在这里面。pipelineEntity.run() 开始执行生产任务。 其他的一些方法，比如 isAvailable、getConcurrency、setConcurrency 等方法要么是维护一些简单的状态，要么只是 pipelineEntity 的代理，用来向外部提供一些方法以在可控的范围内获取/修改 pipelineEntity 的内部信息。 关于 pipeline.entity.ts 里的 PipelineEntity 只需要知道两点： PipelineEntity 是具体处理业务逻辑的地方。 PipelineEntity 会以一个给定的并发数从任务队列中取出产品规格进行生产，直到生产完全部产品。 我们可以通过 PipelineEntity 的 setConcurrency 方法设置并发数，通过 addSpecs 方法添加需要生产的产品规格和相应数量。 监控模块monitor.module.ts 的代码如下： 12345678910111213141516// src/modules/monitor/monitor.module.tsimport &#123; Module &#125; from '@nestjs/common';import &#123; CqrsModule &#125; from '@nestjs/cqrs';import &#123; MonitorHttpController &#125; from './queries/monitor.http.controller';import &#123; MonitorQueryHandler &#125; from './queries/monitor.query-handler';import &#123; UpdatePipelineHttpController &#125; from '@modules/monitor/commands/update-pipeline/update-pipeline.http.controller';import &#123; ProductionModule &#125; from '@modules/production/production.module';const httpControllers = [MonitorHttpController, UpdatePipelineHttpController];@Module(&#123; imports: [CqrsModule, ProductionModule], controllers: [...httpControllers], providers: [MonitorQueryHandler],&#125;)export class MonitorModule &#123;&#125; 我们将 ProductionModule 导入 MonitorModule，因为后者依赖前者。controllers 里是 MonitorModule 开放给外部的 HTTP API。httpControllers 中 MonitorHttpController 里放的是查询 controller，UpdatePipelineHttpController 放的是命令 controller。这里专门做了区分。providers 字段里是一个 MonitorQueryHandler，CQRS 将命令和查询分离，因此命令处理器和查询处理器也要分离。 先看 monitor.http.controller.ts: 123456789101112131415161718192021// src/modules/monitor/queries/monitor.http.controller.tsimport &#123; Controller, HttpStatus, Get &#125; from '@nestjs/common';import &#123; ApiTags, ApiOperation, ApiResponse &#125; from '@nestjs/swagger';import &#123; MonitorQueryHandler &#125; from './monitor.query-handler';import &#123; StatusHttpResponse &#125; from '@modules/monitor/dtos/status.response.dto';@ApiTags('Monitor')@Controller()export class MonitorHttpController &#123; constructor(private readonly monitorQueryHandler: MonitorQueryHandler) &#123;&#125; @Get('/pipeline/status') @ApiOperation(&#123; summary: 'Get status of pipeline' &#125;) @ApiResponse(&#123; status: HttpStatus.OK, type: StatusHttpResponse, &#125;) getStatus(): StatusHttpResponse &#123; return new StatusHttpResponse(this.monitorQueryHandler.getPipelineStatus()); &#125;&#125; MonitorHttpController 很简单，暴露一个 endpoint GET /pipeline/status，内部直接调用 MonitorQueryHandler 的 getPipelineStatus 方法。 12345678910111213141516171819202122// src/modules/monitor/queries/monitor.query-handler.tsimport &#123; Injectable &#125; from '@nestjs/common';import &#123; ProduceProductService &#125; from '@modules/production/commands/produce-product/produce-product.service';import &#123; ProductionStatus &#125; from '@src/interface-adapters/interfaces/production/production-status.interface';@Injectable()export class MonitorQueryHandler &#123; constructor(private readonly produceProductService: ProduceProductService) &#123;&#125; getPipelineStatus(): ProductionStatus &#123; if (!this.produceProductService.isAvailable()) &#123; return &#123;&#125; as ProductionStatus; &#125; return &#123; summary: this.produceProductService.getSummary().getRawProps(), concurrency: this.produceProductService.getConcurrency(), specs: this.produceProductService .getSpecs() .map((spec) =&gt; spec.getRawProps()), &#125;; &#125;&#125; MonitorQueryHandler 里通过依赖注入的 ProduceProductService 获取 pipeline 状态。 接着再看 update-pipeline.http.controller.ts: 12345678910111213141516171819// src/modules/monitor/commands/update-pipeline/update-pipeline.http.controller.tsimport &#123; ApiTags &#125; from '@nestjs/swagger';import &#123; Controller, Patch, Body &#125; from '@nestjs/common';import &#123; CommandBus &#125; from '@nestjs/cqrs';import &#123; UpdatePipelineHttpRequest &#125; from './update-pipeline.request.dto';import &#123; UpdatePipelineCommand &#125; from '@modules/monitor/commands/update-pipeline/update-pipeline.command';@ApiTags('Monitor')@Controller()export class UpdatePipelineHttpController &#123; constructor(private readonly commandBus: CommandBus) &#123;&#125; @Patch('/pipeline') async update(@Body() body: UpdatePipelineHttpRequest) &#123; const command = new UpdatePipelineCommand(body); await this.commandBus.execute(command); return &#123;&#125;; &#125;&#125; UpdatePipelineHttpController 通过暴露一个 endpoint PATCH /pipeline 来动态更新命令行程序，它创建一个 UpdatePipelineCommand 实例，然后通过 this.commandBus 执行。 同样地，UpdatePipelineCommand 也是一个命令的信息载体： 1234567891011121314151617// src/modules/monitor/commands/update-pipeline/update-pipeline.command.tsimport &#123; Command, CommandProps,&#125; from '@libs/ddd/domain/base-classes/command.base';export class UpdatePipelineCommand extends Command &#123; constructor(props: CommandProps&lt;UpdatePipelineCommand&gt;) &#123; super(props); this.concurrency = props.concurrency; this.specs = props.specs; &#125; readonly concurrency: number; readonly specs: string[];&#125; 接着会触发 UpdatePipelineService 的 handle 方法执行，这里也通过依赖注入的方式注入了一个 ProduceProductService实例，最后直接执行上面的 setConcurrency 和 addSpecs 方法来动态修改生产上下文中的参数： 1234567891011121314151617181920212223242526272829303132333435// src/modules/monitor/commands/update-pipeline/update-pipeline.service.tsimport &#123; Injectable, Scope, ConsoleLogger &#125; from '@nestjs/common';import &#123; CommandBus, CommandHandler &#125; from '@nestjs/cqrs';import &#123; Result &#125; from '@libs/ddd/domain/utils/result.util';import &#123; CommandHandlerBase &#125; from '@libs/ddd/domain/base-classes/command-handler.base';import &#123; ProduceProductService &#125; from '@modules/production/commands/produce-product/produce-product.service';import &#123; UpdatePipelineCommand &#125; from '@modules/monitor/commands/update-pipeline/update-pipeline.command';@Injectable(&#123; scope: Scope.DEFAULT,&#125;)@CommandHandler(UpdatePipelineCommand)export class UpdatePipelineService extends CommandHandlerBase &#123; private logger = new ConsoleLogger(UpdatePipelineService.name); constructor( private readonly commandBus: CommandBus, private readonly produceProductService: ProduceProductService, ) &#123; super(); &#125; async handle( command: UpdatePipelineCommand, ): Promise&lt;Result&lt;boolean, Error&gt;&gt; &#123; if (!this.produceProductService.isAvailable()) &#123; return Result.ok(true); &#125; this.produceProductService.setConcurrency(command.concurrency); if (command.specs) &#123; this.produceProductService.addSpecs(command.specs); &#125; return Result.ok(true); &#125;&#125; 让我们运行一下程序看看： 12npm i -g @nullcc/any-factoryany-factory produce-products --specs=\"a:10;b:20;c:30\" --concurrency=1 --with-server 命令行中的日志： HTTP Server swagger API： 请求 pipeline 状态： 修改 pipeline 参数： 请求更新后的 pipeline 状态： 现在可以更新一下可观察的动态命令行程序的基本结构图以给出一种通用结构： 总结在上面的实现中，我们在本地运行命令行程序时同时启动了一个 HTTP Server，对用户开放了一些 API，同时传统的日志记录形式还是可以继续使用（在文中和示例项目中没有展示出来）。在实现过程中，我们还通过模块化将不同上下文隔离开，它们之间可能会产生耦合，但都在我们的控制之中。例如虽然 MonitorModule 依赖于 ProductionModule，但后者的逻辑不会泄露到前者中。 上面除了讨论了如何构建可观察可修改的命令行程序以外，还应用了领域驱动设计的方法指导实现。当然，由于领域驱动设计是个相当大的主题，本文中无法详细描述。 参考资料CQRS(命令查询职责分离)Domain Driven Design(领域驱动设计)DDD六边形架构(翻译)","categories":[{"name":"领域驱动设计","slug":"领域驱动设计","permalink":"https://nullcc.github.io/categories/领域驱动设计/"}],"tags":[{"name":"领域驱动设计","slug":"领域驱动设计","permalink":"https://nullcc.github.io/tags/领域驱动设计/"}]},{"title":"基于 OpenResty 的插件化网关平台架构设计","slug":"基于OpenResty的插件化网关平台架构设计","date":"2021-07-08T16:00:00.000Z","updated":"2022-04-15T03:41:13.033Z","comments":true,"path":"2021/07/09/基于OpenResty的插件化网关平台架构设计/","link":"","permalink":"https://nullcc.github.io/2021/07/09/基于OpenResty的插件化网关平台架构设计/","excerpt":"本文详述了如何基于 OpenResty 设计一个插件化的架构。","text":"本文详述了如何基于 OpenResty 设计一个插件化的架构。 概述OpenResty是一个基于 Nginx 与 Lua 的高性能 Web 平台，它在请求和响应的生命周期设计方面给我们提供了一种基于现有开源方案开发自己的插件化可扩展网关平台的可能性。\b 需求我们不仅需要请求被正确地路由到相应的 upstream （这也是nginx作为反向代理最常用的一种方式），有时还关心请求和响应的一些细节。例如拒绝处理来自IP黑名单的请求、请求限流、鉴权、安全、动态加载SSL证书、日志收集、给请求的 header 打上 request id 以便追踪、请求/响应变换、mock、动态代理等等。甚至在一些测试场景中还会产生定制性很高的需求。 这些需求五花八门，如果能找到一种相对统一且优雅的处理方案，将提高开发和测试的效率。 ngx-lua-module 中的请求生命周期在真正描述我们的设计之前，需要了解 OpenResty 对请求生命周期的基本设计。ngx-lua-module 是 OpenResty 的一个核心 module 。它对一个 HTTP 请求的生命周期做了划分： 图中的星星和右边的一些图示是我自己加的，它们涉及到插件化的设计，之后会详细介绍。在划分出请求的生命周期后，我们获得了面向切编程(AOP)的能力，这是插件化的基础。 我们来具体看看图中请求生命周期的各个阶段（指令）。 init_by_lua*当 Nginx master 进程（如果有）加载 Nginx 配置文件时，在全局的 Lua 虚拟机上执行我们指定的代码。一般\b可以在这个阶段做一些全局性的工作。在我们等一下要讨论的插件化架构设计中，会在这个阶段读取应用程序配置文件以及注册插件。 init_worker_by_lua*当 Nginx 开启 master 进程模式时， Nginx worker 进程启动时会执行指定的代码。如果关闭 master 模式，将在 init_by_lua_* 后直接运行。在这个阶段中可以创建一些定时器任务。 ssl_certificate_by_lua*这个阶段用来动态地加载SSL证书。这允许我们可以在建立连接前才设置证书和私钥，因此我们可以结合 SNI，针对不同的请求域名动态设置不同的证书和私钥。 set_by_lua*这个阶段允许我们使用 Lua 定义一些变量供后面的阶段使用。 rewrite_by_lua*在重写阶段，我们可以修改对请求数据，比如修改URI、请求体和请求头等等。 access_by_lua*这个阶段一般用来检查请求的准入性，比如通过查询一些黑白名单来对请求进行拒绝和放行。 content_by_lua*这个阶段负责响应内容的生成，这个阶段可以说是灵活性最大的阶段。 balancer_by_lua*这个阶段负责处理负载均衡相关事宜。 header_filter_by_lua*我们可以在这个阶段过滤响应头，比如动态地加入 Request ID。 body_filter_by_lua*我们可以在这个阶段过滤响应体。需要注意的是，在进入这个阶段时，响应头已经全部发送给客户端，因此无法在该阶段修改响应头。 log_by_lua*日志阶段，可用供我们记录一些必要的信息。 关于这些指令的详细描述可以参考lua-nginx-module，此处不再赘述。 基本设计从一个比较高的视角来看，有了请求生命周期的阶段划分，我们可以将刚才描述的每个需求对应到一个或多个处理阶段中去。在实现时，针对每种需求（通用需求或特定需求都可以）设计一个插件，该插件只关心它需要关心的阶段。在网关收到针对某个 hostname 的请求时，框架应该先检查是否有针对该 host 的插件被注册，接着在请求的每个阶段一一调用这些插件中对应的指令方法（如果有的话）。 现在思路就比较明确了，我们来按顺序理一下整个过程： [init_by_lua*] \b\bNginx master 进程加载 nginx.conf 启动并初始化，在 init 阶段读取插件配置，注册插件。 [init_worker_by_lua*] Nginx worker 进程启动并初始化，有些插件会在这个阶段注册定时器，定期执行某些操作。 [ssl_certificate_by_lua*] 如果是 HTTPS 请求，框架会调用与当前 hostname 关联的插件的 ssl_certificate 方法动态设置证书。 [set_by_lua*] 框架会调用与当前 hostname 关联的插件的 set 方法设置一些变量。 [rewrite_by_lua*] 框架会调用与当前 hostname 关联的插件的 rewrite 方法重写请求的数据。 [access_by_lua*] 框架会调用与当前 hostname 关联的插件的 access 方法判断请求准入性。 [content_by_lua*] 框架会调用与当前 hostname 关联的插件的 content 方法生成响应内容。 [balancer_by_lua*] 框架会调用与当前 hostname 关联的插件的 balancer 方法将请求转发到 upstream 。 [header_filter_by_lua*] 框架会调用与当前 hostname 关联的插件的 header_filter 方法过滤 response headers 。 [body_filter_by_lua*] 框架会调用与当前 hostname 关联的插件的 body_filter 方法过滤 response body 。至此，响应已经完全发送给客户端。 [log_by_lua*] 框架会调用与当前 hostname 关联的插件的 log 方法做一些日志相关的操作。 实现指令需要让这套逻辑生效，首先需要处理的是 nginx.conf 文件： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# user openresty;worker_processes auto;worker_cpu_affinity auto;error_log logs/error.log;pid logs/nginx.pid;worker_rlimit_nofile 65535;events &#123; use epoll; worker_connections 65535; multi_accept on;&#125;# environment variablesenv MYSQL_HOST;env MYSQL_PORT;env MYSQL_DB;env MYSQL_USER;env MYSQL_PASSWORD;http &#123; include mime.types; include proxy.conf; include ../sites-enabled/*.conf; include ../sites-enabled/*/*.conf; include ../sites-enabled/*/*/*.conf; resolver 8.8.8.8; log_format main &apos;$remote_addr - $remote_user [$time_local] $status &apos; &apos;&quot;$request&quot; $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log logs/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; keepalive_disable none; gzip on; gzip_min_length 1k; gzip_buffers 4 16k; gzip_http_version 1.0; gzip_comp_level 4; gzip_types text/plain application/x-javascript text/css application/xml application/json; gzip_vary on; lua_socket_log_errors off; # lua packages lua_package_path &quot;lualib/?.lua;/usr/local/openresty/nginx/lua/?.lua;/usr/local/openresty/nginx/plugins/?.lua;/usr/local/openresty/nginx/lualib/?.lua;;&quot;; lua_package_cpath &quot;lualib/?.so;/usr/local/openresty/nginx/lualib/?.so;;&quot;; lua_shared_dict ngx_cache 128m; lua_shared_dict plugin_registry 4m; lua_ssl_trusted_certificate &quot;/etc/ssl/certs/ca-bundle.crt&quot;; init_by_lua_file &quot;lua/directives/init.lua&quot;; init_worker_by_lua_file &quot;lua/directives/init_worker.lua&quot;;&#125; 这里只解释几个关键的地方： 几个 MySQL 相关的环境变量用来连接数据库加载插件配置。 几个 include ../sites-enabled/*.conf 声明了 Nginx 启动时需要加载的具体站点的 conf 文件。 lua_package_path 声明了搜索 lua 包的路径。 lua_package_cpath 声明了搜索 so 库的路径。 lua_shared_dict plugin_registry 4m; 声明了一个4M大小的共享内存，用来存放插件数据。 init_by_lua_file 指定了在 init 阶段要调用的 Lua 脚本。 init_worker_by_lua_file 指定了在 init_worker 阶段要调用的 Lua 脚本。 lua/directives/init.lua 负责加载插件配置，这里是从 MySQL 读取。 123456-- lua/directives/init.lualocal runtime = require \"core.runtime\"local constants = require \"core.constants\"local app_plugins = runtime.load_app_plugin_conf_from_db()runtime.register_plugins(constants.PLUGIN_DIR, app_plugins) lua/directives/init_worker.lua 中 plugin_dispatch 方法的第一个参数为 nil，意思是调用所有插件的 init_worker 方法。这里需要说明一下，除了 init 做初始化不涉及插件调用以及 init_worker 调用所有插件的 init_worker 方法以外，其他所有阶段都会调用 runtime.plugin_dispatch 方法来调用插件的相关方法，第一个参数为当前请求的 server name，第二个参数为当前的请求阶段。 12345678-- lua/directives/init_worker.lualocal phases = require \"core.phases\"local runtime = require \"core.runtime\"local uuid = require \"resty.jit-uuid\"uuid.seed()runtime.plugin_dispatch(nil, phases.INIT_WORKER) 共用的 nginx.conf 文件只需要描述这么多，在继续往下之前，我们先来看一个具体站点的 conf 文件： 12345678910111213141516171819202122server &#123; listen 80; listen 443 ssl; server_name api.foo.com; ssl_certificate certs/test.crt; ssl_certificate_key certs/test.key; ssl_certificate_by_lua_file &quot;lua/directives/ssl_certificate.lua&quot;; location / &#123; lua_code_cache on; resolver 127.0.0.11; rewrite_by_lua_file &quot;lua/directives/rewrite.lua&quot;; access_by_lua_file &quot;lua/directives/access.lua&quot;; content_by_lua_file &quot;lua/directives/content.lua&quot;; header_filter_by_lua_file &quot;lua/directives/header_filter.lua&quot;; body_filter_by_lua_file &quot;lua/directives/body_filter.lua&quot;; log_by_lua_file &quot;lua/directives/log.lua&quot;; &#125;&#125; 上面这个 server block 声明了一些信息： 因为我们用到了 ssl_certificate_by_lua_file 来动态加载SSL证书，所以ssl_certificate 和 ssl_certificate_key 被设置成了一个基本的 crt 和 key 文件，如果不设置会报错。 在 location / block 中声明了各种 *_by_lua_file 对应的脚本。 现在可以继续看其他指令的脚本文件了。 lua/directives/ssl_certificate.lua: 123456local ssl = require \"ngx.ssl\"local phases = require \"core.phases\"local runtime = require \"core.runtime\"local server_name, err = ssl.server_name()runtime.plugin_dispatch(server_name, phases.SSL_CERTIFICATE) lua/directives/rewrite.lua: 12345-- lua/directives/rewrite.lualocal phases = require \"core.phases\"local runtime = require \"core.runtime\"runtime.plugin_dispatch(ngx.var.server_name, phases.REWRITE) lua/directives/set.lua: 12345-- lua/directives/set.lualocal phases = require \"core.phases\"local runtime = require \"core.runtime\"return runtime.plugin_dispatch(ngx.var.server_name, phases.SET) lua/directives/access.lua: 12345-- lua/directives/access.lualocal phases = require \"core.phases\"local runtime = require \"core.runtime\"runtime.plugin_dispatch(ngx.var.server_name, phases.ACCESS) lua/directives/content.lua: 12345-- lua/directives/content.lualocal phases = require \"core.phases\"local runtime = require \"core.runtime\"runtime.plugin_dispatch(ngx.var.server_name, phases.CONTENT) lua/directives/balancer: 12345-- lua/directives/balancerlocal phases = require \"core.phases\"local runtime = require \"core.runtime\"runtime.plugin_dispatch(ngx.var.server_name, phases.BALANCER) lua/directives/header_filter: 12345-- lua/directives/header_filterlocal phases = require \"core.phases\"local runtime = require \"core.runtime\"runtime.plugin_dispatch(ngx.var.server_name, phases.HEADER_FILTER) lua/directives/body_filter: 1234567891011121314151617181920-- lua/directives/body_filterlocal phases = require \"core.phases\"local runtime = require \"core.runtime\"-- Should concat response body chunk if it's not the end of response streamlocal chunk = ngx.arg[1]local end_of_resp_stream = ngx.arg[2]if end_of_resp_stream == false then if ngx.ctx.response_body == nil then ngx.ctx.response_body = chunk else ngx.ctx.response_body = ngx.ctx.response_body .. chunk endelse ngx.ctx.end_of_resp_stream = trueendruntime.plugin_dispatch(ngx.var.server_name, phases.BODY_FILTER) 关于 body_filter 阶段需要特别说明一下，由于 Nginx 从 updtream 获取 response body 并不是一次获取所有，而是分块 (chunked) 获取。并且每取得一个分块，Nginx 都会触发一次 body_filter 。为了灵活性最大化，这里设置了一个 ngx.ctx.end_of_resp_stream 上下文变量来让插件判断当前的响应数据传输是否已经结束。这样不管是想处理每个分块还是按照整体来处理整个响应的插件都可以工作。 lua/directives/log: 12345-- lua/directives/loglocal phases = require \"core.phases\"local runtime = require \"core.runtime\"runtime.plugin_dispatch(ngx.var.server_name, phases.LOG) 运行时之前提到 init 指令会调用 runtime.register_plugins 注册插件，其他指令会调用 runtime.plugin_dispatch 触发插件相应阶段的指令方法。 先来看注册、加载和卸载插件的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344-- lua/core/runtime.lualocal _M = &#123; _VERSION = '0.0.1', _plugins = &#123;&#125;&#125;function _M.register_plugins(plugin_dir, app_plugins) for _, app_plugin in ipairs(app_plugins) do local ok, err = _M.load_plugin(plugin_dir, app_plugin.server_name, app_plugin.plugin_name, app_plugin.args or &#123;&#125;) if not ok then ngx.log(ngx.ERR, string.format(\"[rumtime] failed to load plugin: %s, err: %s\", app_plugin.plugin_name, err)) end endendfunction _M.load_plugin(plugin_dir, server_name, plugin_name, args) local plugin_path = string.format(\"%s/%s.lua\", plugin_dir, plugin_name) local plugin_module, err = loadfile(plugin_path) if plugin_module == nil then return false, error(string.format(\"[runtime] plugin not found, plugin: %s, error: %s\", plugin_name, err)) end local plugin = plugin_module() plugin.init_plugin(args) local key = string.lower(server_name) if _M._plugins[key] == nil then _M._plugins[key] = &#123;&#125; end table.insert(_M._plugins[key], plugin) ngx.log(ngx.ERR, string.format(\"[runtime] plugin \\\"%s\\\" for server name \\\"%s\\\", args: %s loaded\", plugin_name, key, cjson.encode(args))) return true, nilendfunction _M.unload_plugin(server_name, plugin_name) local plugins = _M._plugins[server_name] if plugins ~= nil then for i, v in ipairs(plugins) do if plugin_name == v._NAME then table.remove(plugins, i) ngx.log(ngx.ERR, string.format(\"[runtime] plugin \\\"%s\\\" for server name \\\"%s\\\" unloaded\", plugin_name, server_name)) break end end endend register_plugins 注册插件，它在内部调用 load_plugin 执行具体的注册操作。load_plugin 调用 loadfile 从指定的目录加载并用参数初始化插件，然后插入到 _plugins 这个 table 中，注意这里把 _plugins 当成一个字典来用，键是 server name，值是插件数组。unload_plugin 用来卸载指定 hostname 下的指定插件。load_plugin 和 unload_plugin 可以结合一套插件管理 API 来动态加载/卸载 API 。 接下来是指令调用插件的代码： 1234567891011121314151617181920212223242526272829-- lua/core/runtime.luafunction _M.plugin_dispatch(server_name, phase) ngx.log(ngx.ERR, server_name, \", \", phase) local plugins = _M.get_plugins(server_name) if plugins ~= nil then for _, plugin in ipairs(plugins) do if plugin[phase] ~= nil then local ok, ret = pcall(plugin[phase]) if not ok then ngx.log(ngx.ERR, string.format(\"[rumtime] dispatch error, plugin name: %s, phase: %s, err: %s\", plugin._NAME, phase, ret)) end end end endendfunction _M.get_plugins(server_name) if server_name == nil then local plugins = &#123;&#125; for _, server_plugins in pairs(_M._plugins) do for _, plugin in ipairs(server_plugins) do table.insert(plugins, plugin) end end return plugins end local key = string.lower(server_name) return _M._plugins[key]end plugin_dispatch 首先使用 server name 调用 get_plugins 来获取和 server name 相关联的所有插件。这里如果 server name 为 nil 则会将返回所有插件，之前在 init_worker 指令中见到了这种用法。在获得插件列表后，plugin_dispatch 会遍历插件列表，使用 pcall 调用 plugin[phase]。pcall 的意思是 protected call，也就是一种保护模式调用，这个调用中如果出现错误，会以返回码的方式告诉调用方而不会导致请求处理直接挂掉。 插件刚才我们一直围绕插件展开讨论，但插件究竟是个什么东西我们一直没提。插件其实就是一个 Lua table，里面有一些数据和指令相关的方法。下面是插件的基本定义： 123456789101112131415161718192021222324-- lua/core/plugin.lualocal _M = &#123;&#125;function _M.init_plugin(...) local plugin, args = ... for key, value in pairs(args) do plugin[key] = value end if type(plugin._init_plugin) == \"function\" then plugin._init_plugin() endendreturn &#123; __index = function(table, key) local value = _M[key] if type(value) == \"function\" then return function(...) return value(table, ...) end end return value end&#125; 我们已经知道 runtime 在调用插件方法时的调用链是这样的： runtime.plugin_dispatch(server_name, phase) runtime.get_plugins(server_name) pcall(plugin[phase]) 在第三步 pcall(plugin[phase]) 中，runtime 会先获取 plugin[phase]。这里要注意，pcall 的参数是一个函数（或者说闭包），但我们之前说过，每个插件只会实现它关心的指令，因此这里 plugin[phase] 有可能是 nil，即没有实现。所以我们在 pcall(plugin[phase]) 之前需要先判断 plugin[phase] 是否存在。 plugin.lua 中还用到了 metatable.__index，\b这里需要解释一下为什么这么写。大部分插件会有一些初始化参数需要在加载时赋值给插件，另外插件可能自己也有一些初始化逻辑，如果每个插件都写一遍参数初始化的代码很麻烦。Lua 中 metatable的工作机制是，当通过键来访问 table 的时候，如果这个键没有值，那么 Lua 就会查找该 table 的 metatable（如果有的话）中的 index 。如果index 是一个 table，Lua会在表格中查找相应的键。如果 __index 是一个函数，Lua 就会用 table 和键调用那个函数。这个机制有点类似 JavaScript 中的 prototype 。 由于 plugin.lua 返回的 table 包含一个 __index，且 __index 是一个函数。那么只要具体的插件把这个 table 设置为自己的 metatable，当在这个插件上调用一个不存在的方法时，Lua 就会去它的 metatable 中调用 __index 键对应的函数。于是像初始化插件这样的通用操作就可以放在 plugin.lua 中实现了。 一个具体的插件上面说了这么多，我们还没有见过真正可以用的插件长什么样。这里展示一个叫做 request-id 的插件，它可以给所有 response headers 加上一个指定的header，其中键可以由插件参数指定，值是一个UUID。 12345678910111213141516171819202122232425262728293031-- plugins/request-id.lua--- request-id-- Add request-id in HTTP response headers-- @module request-id-- @license MIT-- @release 0.0.1-- @phases init_worker, header_filterlocal uuid = require \"resty.jit-uuid\"local meta_plugin = require \"core.plugin\"local _M = &#123; _VERSION = \"0.0.1\", _NAME = \"request-id\", -- args key = \"\"&#125;setmetatable(_M, meta_plugin)function _M.init_worker() uuid.seed()endfunction _M.header_filter() ngx.header[_M.key] = uuid()endreturn _M request-id 插件只有一个参数 key，每个插件都必须调用 setmetatable(_M, meta_plugin) 来讲上面说的基础插件设置成自己的 metatable，以便自动获得初始化函数（如果有其他需要在插件之间共享的函数也可以加进去）。这个插件涉及的指令只有 init_worker 和 header_filter，在 init_worker 中为 uuid 库初始化了随机数种子，在 header_filter 中将一个 key 为 ）_M.key，值为 uuid 的 header 设置到 response headers 中，就这么简单。 还有一些更复杂的插件，由于篇幅所限无法全部展示出来。不过一旦理解了基本原理和设计，要创建新的插件也不是什么难事。 实现原则有一个实现原则是，不同插件之间不应该产生关联，比如共享状态或者对调用顺序有任何假设。这是因为如果插件之间存在关联，则运行时的效果可能是不可预知的。举个比较刻意的例子，假设对 www.foo.com 我们注册有 A 和 B 两个插件，A 在设置了 ngx.ctx.foo = “a” 并在之后读取了 ngx.ctx.foo，B 设置了 ngx.ctx.foo = “b” 并在之后读取了 ngx.ctx.foo。由于框架对插件的调用顺序和他们被注册的顺序一致（一般来说这样设计没问题，因为框架本身并不关心插件的被调用顺序，只是按序遍历插件列表逐个调用罢了。）且 A 在 B 之前被注册，则实际结果是 A 在读取 ngx.ctx.foo 时期望读到的是 “a”，但读到的却是 “b”，因为 ngx.ctx.foo 这个变量在 A 和 B 之间是共享的且 B 比 A 晚执行覆盖了这个变量。 插件间不应产生关联虽说是原则，但从技术上我们无法阻止有人编写存在关联的插件，只能尽量避免。一个比较好的实践是：在插件内如果有需要用到 ngx.ctx 这种请求相关的上下文变量时，给他一个前缀以降低和其他插件冲突的风险。nginx 内部还有一种范围更广共享方式是使用 lua_shared_dict，这种方式是全局的，可以被所有 worker 共享，使用这种共享内存需要特别小心。如果把视角放到 nginx 之外，产生关联方式就更多了：数据库、缓存、文件等等，其实道理都是一样的，插件间的关联会产生不确定性。 更多\b还可以继续完善这套系统，加入插件管理 API，这部分内容以上面的描述为基础，限于篇幅这里就不展开说了。","categories":[{"name":"Web后端","slug":"Web后端","permalink":"https://nullcc.github.io/categories/Web后端/"}],"tags":[{"name":"OpenResty","slug":"OpenResty","permalink":"https://nullcc.github.io/tags/OpenResty/"},{"name":"插件","slug":"插件","permalink":"https://nullcc.github.io/tags/插件/"}]},{"title":"transformer+source map实现TypeScript函数+行列级别错误定位","slug":"transformer+source map实现TypeScript函数+行列级别错误定位","date":"2021-04-01T16:00:00.000Z","updated":"2022-04-15T03:41:13.026Z","comments":true,"path":"2021/04/02/transformer+source map实现TypeScript函数+行列级别错误定位/","link":"","permalink":"https://nullcc.github.io/2021/04/02/transformer+source map实现TypeScript函数+行列级别错误定位/","excerpt":"本文将给出一个在运行时获取TS函数完整原始代码并展示出错点的方式。","text":"本文将给出一个在运行时获取TS函数完整原始代码并展示出错点的方式。 场景和需求在使用TypeScript实现的自动化测试场景中，一般情况下如果测试失败我们都会打印出error message和error stack信息（比如显示在一些dashboard中）来方便我们排查问题。不过这种做法只能获取到出错点的文件和行列号。而且如果运行的是编译成JS后的代码，获取到的文件名和行列号都是JS代码的。当然你可能会说可以使用source map转换成原始的TS代码文件的行列号，或者不主动编译成JS，而是使用ts-node直接运行TS代码，这样报错信息里的代码和行列号就是TS的了。我们确实可以这么做，但也就仅此而已。 如果能取到出错点对应的方法或函数的完整原始TS代码并展示出出错点和出错信息，直接在错误报告中打印出来，岂不是可以省掉再去工程文件中定位到对应文件和行列号这个步骤，在test case数量大且运行频繁的时候，这种做法能节省我们不少排错时间。 分析问题很多人知道在JS中可以使用toString方法打印出函数（非native函数，打印native函数源码只会显示function () { [native code] }）的源码： 12345// a.jsfunction add(a, b) &#123; return a + b;&#125;console.log(add.toString()); 运行node a.js，脚本会打印出add函数的JS源码: 123function add(a, b) &#123; return a + b;&#125; 在TS中也能这么做，但打印出的代码并不是TS源码，而是转换成JS后的代码： 12345// b.tsfunction add(a: number, b: number): number &#123; return a + b;&#125;console.log(add.toString()); 运行ts-node b.ts，脚本打印出的add函数源码还是JS的： 123function add(a, b) &#123; return a + b;&#125; 使用tsc编译结果也是一样的，这里就不列出了。 很显然，不管是用ts-node隐式编译或者是用tsc显式编译TS代码，我们无法简单地使用toString方法来打印函数的完整TS源码。 不过这也难不倒我们，TS中的transformer是一个很强大的工具，它允许我们在编译阶段对TS代码的抽象语法树(AST)做一些操作。实际上，在transformer中我们可以访问AST的所有node，这些node里有一些信息是我们感兴趣的，比如TS文件名、函数/方法声明的start和end信息，这些TS源码级别的信息有助于实现我们的目标。 解决方案这个transformer有几个要实现的目标： 找到所有函数和方法的声明点，并记录它们在TS源码中的起止位置。 将这些记录输出到一个外部文件中。 第一个目标可以通过遍历所有的AST node来实现，使用下面的代码： 1234567891011121314// transformer.tsimport ts from \"typescript\";export default (program: ts.Program, fileFnRangeMap: any): ts.TransformerFactory&lt;ts.SourceFile&gt; =&gt; &#123; return (ctx: ts.TransformationContext) =&gt; &#123; return (sourceFile: ts.SourceFile): ts.SourceFile =&gt; &#123; // 这里定义访问者方法，该方法会在TS遍历每个AST node时被调用 const visitor = (node: ts.Node): ts.Node =&gt; &#123; return ts.visitEachChild(visitNode(node, program, sourceFile.fileName, fileFnRangeMap), visitor, ctx); &#125;; return &lt;ts.SourceFile&gt; ts.visitEachChild(visitNode(sourceFile, program, sourceFile.fileName, fileFnRangeMap), visitor, ctx); &#125;; &#125;;&#125; 上面的代码实际是一个访问者模式的典型用法，我们不用关心TS在编译代码时具体是怎么遍历AST的，我们只需要提供一个方法，告诉TS在访问到每个node时该做什么。visitNode方法需要我们自己实现。另外你可能会好奇fileFnRangeMap是做什么的，可以暂时先忽略这个参数。 再来看visitNode方法： 123456789101112131415161718192021222324252627282930313233343536// transformer.tsconst visitNode = (node: ts.Node, program: ts.Program, fileName: string, fileFnRangeMap: any): ts.Node =&gt; &#123; if (ts.isSourceFile(node)) &#123; fileFnRangeMap[node.fileName] = []; return node; &#125; if (!isFnDeclaration(node)) &#123; return node; &#125; let start, end = 0; const positions = fileFnRangeMap[fileName]; if (isVariableDeclarationWithArrowFunction(node)) &#123; if (ts.isVariableDeclarationList(node.parent) || ts.isVariableDeclaration(node.parent)) &#123; start = node.parent.pos; end = node.parent.end; &#125; &#125; else &#123; start = node.pos; end = node.end; &#125; positions.push(&#123; start, end &#125;); return node;&#125;;const isFnDeclaration = (node: ts.Node): boolean =&gt; &#123; return ts.isFunctionDeclaration(node) || ts.isFunctionExpression(node) || isVariableDeclarationWithArrowFunction(node) || ts.isArrowFunction(node) || ts.isMethodDeclaration(node) || ts.isConstructorDeclaration(node);&#125;;const isVariableDeclarationWithArrowFunction = (node: ts.Node): boolean =&gt; &#123; return ts.isVariableDeclaration(node) &amp;&amp; !!node.initializer &amp;&amp; ts.isArrowFunction(node.initializer);&#125;; 在TS遍历AST node时会对每个node调用该方法，首先判断当前node是否是SourceFile node，如果是就从中提取出这个文件的名称，并设置fileFnRangeMap中以这个文件名为key的value为一个空数组，我们不打算对node做任何操作，因此直接返回它。如果不是SourceFile node，就判断它是否是一个函数声明node，函数声明node有以下几种： 使用function声明的函数 123function add(x: number, y: number): number &#123; return x + y;&#125; 箭头函数 123const results = [1, 2, 3].reduce((x: number, y: number): number =&gt; &#123; return x + y;&#125;, 0); 带赋值语句的箭头函数 123const add = (x: number, y: number): number =&gt; &#123; return x + y;&#125;; 类构造函数和类方法声明 12345678export class Calc &#123; constructor() &#123; &#125; add(x: number, y: number): number &#123; return x + y; &#125;&#125; 我们需要识别出这几种node，我们可以直接使用typescript提供了一些方法来判断，像这样： 1ts.isFunctionDeclaration(node) 如果是上述几种我们关心的函数声明node，需要获取下它们在TS源码里的起止位置，并push到fileFnRangeMap[$sourceFileName]中。这里我们还是不会对node做任何操作，直接返回即可。 回顾这部分的内容，这个transformer帮助我们在TS遍历AST时记录下我们所关心的函数声明node的起止位置，并把这些信息记录到fileFnRangeMap中以相应文件名为key的数组里。 到此，我们已经准备好了TS源码中所有函数声明的信息，之后把它输出到一个外部文件就行了。为了输出到外部文件，有一种做法是在遍历到每个函数声明node时把fileFnRangeMap字符串化并存储到文件，这么做可以但效率太低，因为每遍历到一个函数声明node都要写一次文件。 其实我们可以在外部实现这个操作，这就需要控制整个TS编译过程，使用一个compile.ts文件来控制： 1234567891011121314151617181920212223242526// compile.tsimport ts from \"typescript\";import transformer from \"./transformer\";import * as util from \"./util\";import &#123; OUTPUT_FILE_NAME &#125; from \"./constant\";export default function compile(dir: string, configFilePath: string, writeFileCallback?: ts.WriteFileCallback) &#123; const parsedCommandLine = ts.getParsedCommandLineOfConfigFile(configFilePath, undefined as any, ts.sys as any); if (!parsedCommandLine) &#123; throw new Error(\"Parsing TS config file error!\"); &#125; const filePaths = util.scan(dir); const compilerOptions = parsedCommandLine.options; compilerOptions.sourceMap = true; const program = ts.createProgram(filePaths, compilerOptions); const fileFnRangeMap = &#123;&#125;; const transformers: ts.CustomTransformers = &#123; before: [transformer(program, fileFnRangeMap)], after: [], &#125;; const &#123; emitSkipped, diagnostics &#125; = program.emit(undefined, writeFileCallback, undefined, false, transformers); if (emitSkipped) &#123; throw new Error(diagnostics.map(diagnostic =&gt; diagnostic.messageText).join('\\n')); &#125; util.writeToFile(OUTPUT_FILE_NAME, JSON.stringify(fileFnRangeMap));&#125; 这个compile.ts里的compile方法的用法是这样的： 1compile(sourceCodeDir, tsconfigFile); 有几个地方需要说明，compile中强制开启了source map，因为我们必须借助source map才能通过编译后的JS代码行列号定位到TS源码的行列号。compile方法让我们能控制整个TS编译过程。注意第16行声明了一个fileFnRangeMap对象并将它作为transformer方法的第二个参数。接着在最后将fileFnRangeMap对象字符串化到文件里。 来看一个例子，假设有一个项目目录和文件如下： 12345|---my-app| |---src| | |---inner| | | |---b.ts| | |---a.ts src/a.ts:123export function add(a: number, b: number): number &#123; return a + b;&#125; src/inner/b.ts:123456789export const add = (a: number, b: number): number =&gt; &#123; return a + b;&#125;export class Calc &#123; add(a: number, b: number): number &#123; return a + b; &#125;&#125; 在使用如下代码编译后会在当前目录生成一个_ts-err-hunter-file-fn-range.json文件，里面记录了src目录下所有TS文件里方法声明的起止位置，另外我们还获得了source map。 1compile(\"src\", \"tsconfig.json\"); _ts-err-hunter-file-fn-range.json:12345678910111213141516171819202122&#123; \"src/a.ts\": [ &#123; \"start\": 0, \"end\": 69 &#125; ], \"src/inner/b.ts\": [ &#123; \"start\": 6, \"end\": 72 &#125;, &#123; \"start\": 18, \"end\": 72 &#125;, &#123; \"start\": 93, \"end\": 153 &#125; ]&#125; 有了上面这些信息，当运行时报错时，我们就可以通过error stack获得出错点的JS文件路径和行列号。然后使用source map查找到对应TS文件的路径和行列号。再计算出TS文件的行列号对应的位置，并查询该位置在_ts-err-hunter-file-fn-range.json里的对应文件中落在哪个函数声明区间，这个区间的起止位置就是这个出错点在TS文件中函数的完整区间了。最后直接把这个区间的代码打印出来可以了。具体的查找过程不复杂，就不赘述了。 ts-err-hunter我写了ts-err-hunter这个package来实现整个过程。 为了使用ts-err-hunter, 需要将下面的代码加入到项目的入口文件里： 123import &#123; register &#125; from \"ts-err-hunter\";register(); 我们假设项目的源码目录是src，且tsconfig.json文件在项目根目录。创建一个名为compile.ts的文件，代码如下： 123import &#123; compile &#125; from \"ts-err-hunter\";compile(\"src\", \"tsconfig.json\"); 然后执行这个文件： 1$ ts-node compile.ts 执行上面的操作后，相当于使用项目的tsconfig.json配置文件来编译源代码，可以看到引入ts-err-hunter对源项目的编译过程影响并不大。 为了展示效果，假设我们有这么一个TS文件： 12345678910111213141516171819202122import fs from \"fs\";import &#123; register &#125; from \"ts-err-hunter\";register();const foo = () =&gt; &#123; // comments... fs.readFileSync(\"xxx.json\");&#125;(async () =&gt; &#123; try &#123; foo(); &#125; catch (err) &#123; const sourceCode = await err.getSourceCode(); if (sourceCode) &#123; console.log(`source file: $&#123;sourceCode.fileName&#125;`); console.log(sourceCode.content); &#125; throw err; &#125;&#125;)(); 使用上面的方法编译并运行这些代码，我们将得到出错点的详细信息： 1234567source file: /absolute/path/to/TS/code.ts&gt; 6 const foo = () =&gt; &#123;&gt; 7 // comments...&gt; 8 fs.readFileSync(&quot;xxx.json&quot;); ^ ------------&gt; ENOENT: no such file or directory, open &apos;xxx.json&apos;&gt; 9 &#125; 这就是完整的过程和效果啦！","categories":[{"name":"工具","slug":"工具","permalink":"https://nullcc.github.io/categories/工具/"}],"tags":[{"name":"typescript","slug":"typescript","permalink":"https://nullcc.github.io/tags/typescript/"}]},{"title":"理解Scala类型系统中的型变","slug":"理解Scala类型系统中的型变","date":"2020-09-29T16:00:00.000Z","updated":"2022-04-15T03:41:13.036Z","comments":true,"path":"2020/09/30/理解Scala类型系统中的型变/","link":"","permalink":"https://nullcc.github.io/2020/09/30/理解Scala类型系统中的型变/","excerpt":"本文深入解析了Scala类型系统中的三种型变：协变、逆变和不变。","text":"本文深入解析了Scala类型系统中的三种型变：协变、逆变和不变。 Scala类型系统中的型变有三种形式：协变、逆变和不变。在深入理解型变之前，有必要回顾一下设计模式中的一个关键概念：里氏替换原则。 型变的核心：里氏替换原则里式替换原则有两个关键点： 任何使用父类的地方都可以用它的子类替换而不会产生任何异常，但反过来则不行。 子类重载（注意不是重写）父类的方法的入参的限制要比父类的相应方法更宽松，但返回值要比父类更严格。 第一点比较好理解，即父类知道的，子类都知道；子类知道的，父类未必知道，就不赘述了。 关于第二点，先来看一个例子： 12345class SignalProcessor[+T] &#123; def process(in: T): Unit = &#123; println(in) &#125;&#125; 这段代码无法通过编译： 123covariant type T occurs in contravariant position in type T of value in def process(in: T): Unit = &#123; ^ 编译器给出的异常信息表明，SignalProcessor类的process方法的入参in是泛型类型T的逆变点，但我们给了一个协变的定义。初看这种报错基本上是一头雾水，要搞清楚根本原因我们需要具体分析一下这个SignalProcessor类。 有如下代码： 12val anySignalProcessor = new SignalProcessor[AnyRef]val stringSignalProcessor = new SignalProcessor[String] 考察SignalProcessor[AnyRef]和ignalProcessor[String]的类型关系，无非两种情况： SignalProcessor[String]是SignalProcessor[AnyRef]的子类型。 SignalProcessor[AnyRef]是SignalProcessor[String]的子类型。 第一种假设，根据里式替换原则，出现SignalProcessor[AnyRef]的地方都要可以用SignalProcessor[String]来替换： 123val foo: Integer = 1anySignalProcessor.process(foo) // OKstringSignalProcessor.process(foo) // Oops! Type mismatch, expected: String, actual: Integer 由于SignalProcessor[String]不能处理非String的入参，也就不能替换SignalProcessor[AnyRef]，假设一不成立。 第二种假设，根据里式替换原则，出现SignalProcessor[String]的地方都要可以用SignalProcessor[AnyRef]来替换： 123val bar: String = \"abc\"stringSignalProcessor.process(bar) // OKanySignalProcessor.process(bar) // OK 因此假设二是成立的。 所以，SignalProcessor[AnyRef]是SignalProcessor[String]的子类型，process方法在子类的入参AnyRef也确实比父类的入参String更宽松。此时刚才提到的式替换原则的第二个关键点的上半句：“子类重载（注意不是重写）父类的方法的入参的限制要比父类的相应方法更宽松”就很好理解了。 回到刚才那段编译报错的代码，应该改为： 12345class SignalProcessor[-T] &#123; def process(in: T): Unit = &#123; println(in) &#125;&#125; 由上可见，泛型类的方法参数导致了泛型类型在子类和父类关系上发生了逆转。因此，方法参数的位置被称为逆变点(contravariant position)。也可以说泛型类在方法参数上是逆变的。 再来看下半句“子类方法的返回值要比父类更严格”，如果process方法返回一个值，这个值一般会被process方法的调用方消费，比如作为另一个方法的参数： 12345class SignalProcessor[-T] &#123; def process(): T = &#123; null.asInstanceOf[T] &#125;&#125; 这段代码也会报错，但报错信息和之前稍有不同： 123error: contravariant type T occurs in covariant position in type (): T of method process def process(): T = &#123; ^ 意思是SignalProcessor类的process方法的出参是泛型类型T的协变点，但我们给了一个逆变的定义。 在这个例子中，对于泛型类在方法的泛型出参上的父子类型关系也无非两种情况： SignalProcessor[AnyRef]的process方法的返回值类型是SignalProcessor[String]的process方法的返回值类型的子类型。 SignalProcessor[String]的process方法的返回值类型是SignalProcessor[AnyRef]的process方法的返回值类型的子类型。 为了使得之后使用process方法的返回值的调用符合里式替换原则，很显然SignalProcessor[String]的process方法的返回值必须得是SignalProcessor[AnyRef]的process方法的返回值的子类型。 SignalProcessor[String]的process的返回值在类型上确实需要比SignalProcessor[AnyRef]更严格。 由上可见，泛型类的方法返回值需要符合泛型类型的子类和父类的关系。因此，方法返回值的位置被称为协变点(covariant position)。也可以说，泛型类在方法返回值上是协变的。 因此上面这段代码应该改成： 12345class SignalProcessor[+T] &#123; def process(): T = &#123; null.asInstanceOf[T] &#125;&#125; 论证完里式替换原则的两个关键点，Scala的型变就非常好理解了。 协变假设A是B的子类型，另有泛型类Foo[+T]，则Foo[A]是Foo[B]的子类型，这被称为协变。例子： 12345678910class Material &#123;&#125;class Liquid extends Material &#123;&#125;class Container[+T] (private val item: T) &#123; def get(): T = item&#125;val liquidContainer = new Container[Liquid](new Liquid)val materialContainer: Container[Material] = liquidContainer // OKprintln(materialContainer.get()) // Main$$anon$1$Liquid@6035b93b 在需要Container[Material]的地方可以用Container[Liquid]替换，反之则不行： 123456789class Material &#123;&#125;class Liquid extends Material &#123;&#125;class Container[+T] (private val item: T) &#123; def get(): T = item&#125;val materialContainer = new Container[Material](new Material)val liquidContainer: Container[Liquid] = materialContainer // Oops! 这段代码会报错： 1234error: type mismatch;found : this.Container[this.Material]required: this.Container[this.Liquid]val liquidContainer: Container[Liquid] = materialContainer 协变很好理解，液体(Liquid)是一种物质(Material)，因此液体容器(Container[Liquid])是一种物质容器(Container[Material])。根据里式替换原则，这里我可以把使用Container[Material]类型对象的地方替换成使用Container[Liquid]类型的对象，获取到的是Liquid，这是可以的，因为Liquid是一种Material。 逆变假设A是B的子类型，另有泛型类Foo[-T]，则Foo[B]是Foo[A]的子类型。例子： 123456789101112class Animal &#123;&#125;class Bear extends Animal &#123;&#125;class Hunter[-T] &#123; def hunt(t: T): Unit = &#123; println(\"Caught \" + t) &#125;&#125;val animalHunter = new Hunter[Animal]val bearHunter: Hunter[Bear] = animalHunterbearHunter.hunt(new Bear) // Caught Main$$anon$1$Bear@aa549e5 在需要Hunter[Bear]的地方可以用Hunter[Animal]替换，反之则不行： 1234567891011class Animal &#123;&#125;class Bear extends Animal &#123;&#125;class Hunter[-T] &#123; def hunt(t: T): Unit = &#123; println(\"Caught \" + t) &#125;&#125;val bearHunter = new Hunter[Bear]val animalHunter: Hunter[Animal] = bearHunter // Oops! 这段代码会报错： 1234error: type mismatch; found : this.Hunter[this.Bear] required: this.Hunter[this.Animal]val animalHunter: Hunter[Animal] = bearHunter 逆变理解起来不如协变直观。根据逆变的逻辑，在这段代码中，熊(Bear)是一种动物(Animal)，那么动物猎人(Hunter[Animal])是一种猎熊者(Hunter[Bear])。这个逻辑好像有点违反常识，不是应该说猎熊者(Hunter[Bear])是一种动物猎人(Hunter[Animal])吗？但是如果套用里氏替换原则中的概念：在需要基类的地方，都可以用子类替换，但反过来则不行。 逆变的道理也是一样的，在需要Hunter[Bear]的地方，我们用一个Hunter[Animal]去替代是可以的，因为动物猎人掌握狩猎一切动物的技能，这当然也包括猎熊。但是反过来，在需要Hunter[Animal]的地方，我们无法用Hunter[Bear]去替代，因为动物猎人是全能的，猎熊者只知道如何狩猎熊，如果换成麋鹿，Hunter[Bear]就不灵了。 根据这个逻辑，不难得出Hunter[Animal]应该是Hunter[Bear]的子类型。 不变如果一个泛型类在类型参数上不加任何修饰，那这个泛型类在这个类型参数上就是不变的，比如Foo[T]。不变也是很有用的，这里引用Scala官方文档中的一个例子： 123456789101112131415161718abstract class Animal &#123; def name: String&#125;case class Cat(name: String) extends Animalcase class Dog(name: String) extends Animalclass Container[A](value: A) &#123; private var _value: A = value def getValue: A = _value def setValue(value: A): Unit = &#123; _value = value &#125;&#125;val catContainer: Container[Cat] = new Container(Cat(\"Felix\"))val animalContainer: Container[Animal] = catContaineranimalContainer.setValue(Dog(\"Spot\"))val cat: Cat = catContainer.getValue // 糟糕，这里会将一只狗赋值给一只猫！ 还好这段代码无法通过编译，因为编译器会阻止我们这么做。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://nullcc.github.io/tags/Scala/"},{"name":"类型系统","slug":"类型系统","permalink":"https://nullcc.github.io/tags/类型系统/"},{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"LL(1)语法解析器的一次简单实践","slug":"LL(1)语法解析器的一次简单实践","date":"2020-04-18T16:00:00.000Z","updated":"2022-04-15T03:41:13.014Z","comments":true,"path":"2020/04/19/LL(1)语法解析器的一次简单实践/","link":"","permalink":"https://nullcc.github.io/2020/04/19/LL(1)语法解析器的一次简单实践/","excerpt":"本文记录LL(1)语法解析器的一次简单实践。","text":"本文记录LL(1)语法解析器的一次简单实践。 \b需求概述前段时间和Team内部的同学进行了一次Coding Dojo，遇到一道题目，我个人觉得有点意思，就记录了一下自己的解题方式和心得。题目是StringCalculator。 这道题目一共有7个小需求，题目需求这里就不详细说明了，可以参考上面的链接。本文主要给出实现。 实现这个题目实际上可以用LL(1)语法解析器来处理。这里先给出完整的语法解析器实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142// lexer.tsexport interface LexerOptions &#123; collectAllErrors?: boolean;&#125;export class Lexer &#123; public static EOF = '&lt;EOF&gt;'; public static TYPE_EOF = 0; public static TYPE_NUMBER: number = 1; public static TYPE_DELIMITER: number = 2; public static TYPE_NEW_LINE: number = 3; public static tokenNames: string[] = [ '&lt;EOF&gt;', 'NUMBER', 'TYPE_DELIMITER', 'NEW_LINE']; private options: LexerOptions; private input: string; private c: string; private i: number; private delimiter = ','; private _errors: string[] = []; public constructor(input: string, options?: LexerOptions) &#123; this.options = options || &#123;&#125;; this.input = input; this.i = 0; this.checkDelimiter(); this.c = this.input[0]; &#125; public nextToken(): Token &#123; while (this.c != Lexer.EOF) &#123; switch (this.c) &#123; case this.delimiter[0]: &#123; for (let i = 1; i &lt; this.delimiter.length; i += 1) &#123; this.expect(this.delimiter[i]); &#125; this.consume(); if (this.expectNumber()) &#123; return new Token(Lexer.TYPE_DELIMITER, this.delimiter); &#125; this.collectError(`Number expected but '$&#123;this.getC()&#125;' found at position $&#123;this.i&#125;.`); break; &#125; case '\\n': &#123; this.consume(); if (this.expectNumber()) &#123; return new Token(Lexer.TYPE_DELIMITER, '\\n'); &#125; this.collectError(`Number expected but '$&#123;this.getC()&#125;' found at position $&#123;this.i&#125;.`); break; &#125; default: if (this.isNumber()) &#123; return this.readNumber(); &#125; if (this.i &lt; this.input.length - 1 &amp;&amp; this.c !== this.delimiter[0]) &#123; this.collectError(`'$&#123;this.delimiter[0]&#125;' expected but '$&#123;this.input[this.i]&#125;' found at position $&#123;this.i&#125;.`); break; &#125; this.consume(); &#125; &#125; return new Token(Lexer.TYPE_EOF, Lexer.EOF); &#125; get errors(): string[] &#123; return this._errors; &#125; private checkDelimiter(): void &#123; const regex = /\\/\\/(.+?)\\n(.+)?/; const res = regex.exec(this.input); if (res) &#123; this.delimiter = res[1]; this.input = res[2]; &#125; &#125; private isNumber(): boolean &#123; if (this.c === '-' || this.c === '.') &#123; return true; &#125; return !isNaN(parseFloat(this.c)); &#125; private consume(): void &#123; this.i += 1; if (this.i &gt; this.input.length) &#123; this.c = Lexer.EOF; &#125; else &#123; this.c = this.input[this.i]; &#125; &#125; private readNumber() &#123; let numberStr = ''; do &#123; numberStr += this.c; this.consume(); &#125; while (this.isNumber()); return new Token(Lexer.TYPE_NUMBER, parseFloat(numberStr)); &#125; private expectNumber(): boolean &#123; return /\\d/.test(this.c) || this.c === '-'; &#125; private expect(c: string): void &#123; this.consume(); if (this.c !== c) &#123; this.collectError(`$&#123;c&#125; expected but '$&#123;this.getC()&#125;' found at position $&#123;this.i&#125;.`); &#125; &#125; private getC(): string &#123; if (this.i &lt; this.input.length) &#123; return this.c; &#125; return '&lt;EOF&gt;'; &#125; private collectError(errMsg: string) &#123; if (this.options.collectAllErrors) &#123; this._errors.push(errMsg); &#125; else &#123; throw new Error(errMsg); &#125; &#125;&#125;export class Token &#123; public type: number; public value: string | number; public constructor(type: number, value: string | number) &#123; this.type = type; this.value = value; &#125; public getTokenName(x: number): string &#123; return Lexer.tokenNames[x]; &#125;&#125; 有了语法解析器以后，剩下的事情就很简单了。这里也不需要parser，只需要实现一个StringCalculator类即可： 123456789101112131415161718192021222324252627282930313233343536import &#123; Lexer, LexerOptions &#125; from './lexer';export class StringCalculator &#123; private options: LexerOptions; public constructor (options?: LexerOptions) &#123; this.options = options || &#123;&#125;; &#125; public add(input: string): number &#123; const tokens = []; const lexer = new Lexer(input, this.options); let token = lexer.nextToken(); while (token.type !== Lexer.TYPE_EOF) &#123; tokens.push(token); token = lexer.nextToken(); &#125; let res = 0; let errors: string[] = []; errors = errors.concat(lexer.errors); const negativeNumberTokens = tokens.filter(token =&gt; token.value &lt; 0); if (negativeNumberTokens.length &gt; 0) &#123; const negativeNumbers = negativeNumberTokens.map(token =&gt; token.value); errors = errors.concat(`Negative not allowed: $&#123;negativeNumbers.join(', ')&#125;`); &#125; if (errors.length &gt; 0) &#123; throw new Error(errors.join('\\n')); &#125; for (const token of tokens) &#123; if (token.type === Lexer.TYPE_NUMBER) &#123; res += &lt;number&gt; token.value; &#125; &#125; return res; &#125;&#125; 测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129// string-calculator.test.tsimport &#123; StringCalculator &#125; from '../src/calculator';describe('Test string calculator.', () =&gt; &#123; test('Test case 0.', async () =&gt; &#123; const input = ''; const stringCalculator = new StringCalculator(); const res = stringCalculator.add(input); expect(res).toEqual(0); &#125;); test('Test case 1.', async () =&gt; &#123; const input = '0'; const stringCalculator = new StringCalculator(); const res = stringCalculator.add(input); expect(res).toEqual(0); &#125;); test('Test case 2.', async () =&gt; &#123; const input = '1,2,3,10'; const stringCalculator = new StringCalculator(); const res = stringCalculator.add(input); expect(res).toEqual(16); &#125;); test('Test case 3.', async () =&gt; &#123; const input = '1,2.5,3'; const stringCalculator = new StringCalculator(); const res = stringCalculator.add(input); expect(res).toEqual(6.5); &#125;); test('Test case 4.', async () =&gt; &#123; const input = '1\\n2,3,10'; const stringCalculator = new StringCalculator(); const res = stringCalculator.add(input); expect(res).toEqual(16); &#125;); test('Test case 5.', async () =&gt; &#123; const input = '1\\n2,3,10'; const stringCalculator = new StringCalculator(); const res = stringCalculator.add(input); expect(res).toEqual(16); &#125;); test('Test case 6.', async () =&gt; &#123; try &#123; const input = '1\\n,2,3,10'; const stringCalculator = new StringCalculator(); const res = stringCalculator.add(input); expect(true).toBe(false); &#125; catch (e) &#123; expect(e.message).toBe(\"Number expected but ',' found at position 2.\"); &#125; &#125;); test('Test case 7.', async () =&gt; &#123; try &#123; const input = '1,2,3,10,'; const stringCalculator = new StringCalculator(); const res = stringCalculator.add(input); expect(true).toBe(false); &#125; catch (e) &#123; expect(e.message).toBe(\"Number expected but '&lt;EOF&gt;' found at position 9.\"); &#125; &#125;); test('Test case 8.', async () =&gt; &#123; const input = '//;\\n1;2'; const stringCalculator = new StringCalculator(); const res = stringCalculator.add(input); expect(res).toEqual(3); &#125;); test('Test case 9.', async () =&gt; &#123; const input = '//|\\n1|2|3'; const stringCalculator = new StringCalculator(); const res = stringCalculator.add(input); expect(res).toEqual(6); &#125;); test('Test case 10.', async () =&gt; &#123; const input = '//sep\\n2sep3'; const stringCalculator = new StringCalculator(); const res = stringCalculator.add(input); expect(res).toEqual(5); &#125;); test('Test case 11.', async () =&gt; &#123; try &#123; const input = '//|\\n1|2,3'; const stringCalculator = new StringCalculator(); const res = stringCalculator.add(input); &#125; catch (e) &#123; expect(e.message).toBe(\"'|' expected but ',' found at position 3.\"); &#125; &#125;); test('Test case 12.', async () =&gt; &#123; try &#123; const input = '-1,2'; const stringCalculator = new StringCalculator(); const res = stringCalculator.add(input); &#125; catch (e) &#123; expect(e.message).toBe(\"Negative not allowed: -1\"); &#125; &#125;); test('Test case 13.', async () =&gt; &#123; try &#123; const input = '2,-4,-5'; const stringCalculator = new StringCalculator(); const res = stringCalculator.add(input); &#125; catch (e) &#123; expect(e.message).toBe(\"Negative not allowed: -4, -5\"); &#125; &#125;); test('Test case 14.', async () =&gt; &#123; try &#123; const input = '-1,,2'; const stringCalculator = new StringCalculator(&#123; collectAllErrors: true &#125;); const res = stringCalculator.add(input); &#125; catch (e) &#123; expect(e.message).toBe(\"Number expected but ',' found at position 3.\\nNegative not allowed: -1\"); &#125; &#125;);&#125;); 其他Coding Dojo过程中我发现这题的很多的实现都是先用split之类的方法去解开整个输入字符串，然后收集到一个数字数组后再去求和。这样做不能说不行，但是随着本题的需求越来越多，这种实现方式的弊端会慢慢显现出来，代码越来越复杂，而且很难修改。如果利用LL(1)语法解析器，其解析精度会比较高，而且一系列需求做下来实际上是对它的渐进式增强。在实际解题的过程中我发现对于每个新需求，使用LL(1)语法解析器的方案需要做的修改和重构量都很小，而且比较不容易出错。","categories":[{"name":"语言应用","slug":"语言应用","permalink":"https://nullcc.github.io/categories/语言应用/"}],"tags":[{"name":"语法解析器","slug":"语法解析器","permalink":"https://nullcc.github.io/tags/语法解析器/"}]},{"title":"利用OpenResty做代理实现到STF的请求转发和自定义权限校验","slug":"利用OpenResty做代理实现到STF的请求转发和自定义权限校验","date":"2020-02-27T16:00:00.000Z","updated":"2022-04-15T03:41:13.030Z","comments":true,"path":"2020/02/28/利用OpenResty做代理实现到STF的请求转发和自定义权限校验/","link":"","permalink":"https://nullcc.github.io/2020/02/28/利用OpenResty做代理实现到STF的请求转发和自定义权限校验/","excerpt":"本文将介绍一种在两个独立系统之间搭建中间层来做请求转发和权限校验的方案。","text":"本文将介绍一种在两个独立系统之间搭建中间层来做请求转发和权限校验的方案。 需求在详细介绍架构设计之前必须先陈述一下需求。这是在我现实工作场景中遇到的事情，在我们团队中，需要对app做自动化测试。我们有大约50台真机(iOS + Android)，这些设备分布在很多台主机上。每天我们都需要在这些真机（还有模拟器，但模拟器在这里不是重点）上执行很多测试用例。于是之前我们设计了一个系统（代号device-spy）来主动扫描分布在这些主机上的所有设备的信息，并以一定的策略对外提供服务。客户端通过调用API来向device-spy申请符合某种条件的设备并锁定它，然后直接使用这个设备执行测试用例。当测试用例执行完毕，客户端调用device-spy的API将设备解除占用。当测试运行时，appium会对每一个会话生成session，device-spy系统采用定时轮询的方式检测每个设备对应的session的存在性。如果session消失，device-spy将主动将设备解除占用。另外，客户端崩溃也会导致appium session消失，进而device-spy就会检测到并将相应设备解除占用。我们就是用这种方式来管理这些设备，关于device-spy的情况就先介绍这么多。 最近，我们团队希望使用STF来实现Android手机设备的远程控制，STF是针对Android设备远程控制的一套比较成熟的开源方案。我们想对device-spy和STF做一个集成，只有在device-spy上申请对某个Android的远程控制后（这会锁住设备使它无法被其他人或者测试脚本使用），才能在STF上真正地去远程控制。任何试图直接在STF上实行远程控制的操作都将被禁止。这个需求的主要目的是不想让用户在STF上对Android设备的操作影响到自动化测试脚本的执行。 设计思路在了解了需求后，可以发现这里有两个完全独立的系统：device-spy和STF，我们的目标实际上是要针对这两个系统设计一套鉴权和请求转发方案。从设计方案的最开始，我们需要明确几条原则： 不对STF的代码做任何改动。 对device-spy只能做尽量少的改动，最好是只增加新功能不修改现有功能。 设计要尽量简单可靠。 计算机科学领域有一句名言：“计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决”。 这个需求也完全可以靠增加一个中间层来解决。既然要求不修改STF，还能达到控制对它API的访问的要求。很直接地可以想到在STF前面加一层反向代理来负责转发所有到STF的请求（HTTP和Websocket）。还可以在这个中间层上做一些鉴权的逻辑用来控制用户浏览器对STF的访问。 思路很明确，理论上完全可行。剩下的就是技术选型和架构设计。 技术选型方面，一般来说Web后端做反向代理的组件最常用的就是nginx了。nginx性能强悍且有很好的可扩展性，比如可以使用ngx_lua_module做扩展，允许用户在请求的生命周期的不同阶段实现自己的逻辑。\bOpenResty又是这方面很好的一个技术方案，于是我把目光投向OpenResty。 OpenResty是基于nginx和Lua的高性能Web平台，开发者可以在OpenResty的请求处理流程的各个阶段中设置自己的逻辑。下面是OpenResty处理请求流程图： 可以不必深究这张图的所有细节，目前重点只需要关注”access_by_lua”和”content_by_lua”两个阶段。在access_by_lua阶段，我们还没有开始处理任何实际的业务逻辑，一般在这个阶段都是处理一些和权限有关的事情，因此这里可以设置鉴权逻辑，当鉴权成功就转发请求到upstream的STF服务，失败则返回一个403 Forbidden。content_by_lua阶段一般用来放置一些实际的操作，我们可以设计一些API用来实现设备注册、用户浏览器授权和设备注销。device-spy将会直接和这些API做集成来为鉴权逻辑做准备。 至此，思路我们已经准备好了。 架构设计思路有了，还需要设计详细架构才能开始编码。这个中间层系统我把他命名为midway。下图有助于清晰地理解整个流程： 详细解释一下各个步骤的行为： 用户在device-spy上申请对某台安卓机器的控制，并指定timeout。 device-spy锁定这台设备，并生成一个code（为了安全性这个code是有需要加密的，在midway端会解密，防止伪造，但简单起见可以先不实现）。 device-spy向midway注册设备（携带参数code, timeout，device_ip和adb_port）。 midway用uuid生成一个session key，并在redis里创建(string)code:session，(string)device:session和(hash)session:{ permission, device }，这三个key-value pairs都设置timeout过期时间。 device-spy重定向浏览器到midway的/auth?code=xxx上。 midway验证这个code，如果这个code已经注册，就把对应的session key作为cookie设置在浏览器端。 (7-10) midway会反向代理所有到STF的HTTP和Websocket请求。特别地，只有当浏览器访问STF的/api/v1/devices/{device_ip}:{adb_port}这个API时，midway才会做权限校验，看看用户session是否有权限操作这台设备，如果不可以就返回403，验证通过则直接proxy_pass到STF，最后返回响应。只对这个API做权限校验就可以了，因为这个远程操控制某台设备之前必须请求该API获取相关信息后建立websocket连接。 (11) 用户在device-spy上主动注销设备，device-spy会请求midway的注销设备API。 实现在实现阶段，我们需要依照上面的架构设计打通所有节点之间的连接，形成功能闭环。由于device-spy上的实现无非就是增加几个API调用不难理解，所以这里我将只讨论midway的详细实现。 OpenResty是基于nginx的，所以一个nginx.conf是少不了的。nginx.conf这个文件非常重要，它描述了我们的所有请求将如何被nginx处理： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242worker_processes 1;error_log logs/error.log;events &#123; worker_connections 1024;&#125;# declare environment variablesenv STF_IP;env STF_SSH_USER;env STF_SSH_PASSWORD;http &#123; map $http_upgrade $connection_upgrade &#123; default upgrade; &apos;&apos; close; &#125; # lua packages lua_package_path &quot;lualib/?.lua;/usr/local/openresty/nginx/lualib/?.lua;;&quot;; lua_package_cpath &quot;lualib/?.so;/usr/local/openresty/nginx/lualib/?.so;;&quot;; init_by_lua_file &quot;lua/init.lua&quot;; upstream stf_app &#123; server $&#123;STF_IP&#125;:21000; &#125; upstream stf_websocket &#123; server $&#123;STF_IP&#125;:7110; &#125; init_worker_by_lua_block &#123; local uuid = require &quot;resty.jit-uuid&quot; uuid.seed() &#125; # STF app server &#123; listen 21000; server_name $&#123;SERVER_NAME&#125;; resolver 127.0.0.11; # device-spy requests this API to register device with: # 1. code # 2. timeout (unit: second) # 3. device (ip:adb_port) # # Sample: # method: POST # content-type: application/json # data: &#123; # &quot;code&quot;: &quot;xyz&quot;, # &quot;timeout&quot;: 3600, # &quot;device&quot;: &quot;$&#123;DEVICE_IP&#125;:7500&quot; # &#125; location /register-device &#123; proxy_ssl_name $host; lua_code_cache on; content_by_lua_file &quot;lua/register.lua&quot;; &#125; # device-spy requests this API to deregister device: # Sample: # method: POST # content-type: application/json # data: &#123; # &quot;device&quot;: &quot;$&#123;DEVICE_IP&#125;:7500&quot; # &#125; location /deregister-device &#123; proxy_ssl_name $host; lua_code_cache on; content_by_lua_file &quot;lua/deregister.lua&quot;; &#125; # Client requests this API to authenticate with code in query string, e.g. /auth?code=xxx # OpenResty will set cookie in client if success location ~ ^/auth$ &#123; proxy_ssl_name $host; lua_code_cache on; content_by_lua_file &quot;lua/auth.lua&quot;; &#125; # OpenResty verifies permission before proxying this request to STF app location ~ ^/api/v1/devices/(.+)$ &#123; access_by_lua_file &quot;lua/access.lua&quot;; # verify permission by lua script proxy_pass http://stf_app; &#125; # OpenResty injects JS code to STF index page to redirect browser to &apos;/&apos; after timeout location ~ ^/$ &#123; proxy_ssl_name $host; lua_code_cache on; content_by_lua_file &quot;lua/index.lua&quot;; &#125; # Other http requests will be proxied to STF directly location / &#123; proxy_pass http://stf_app; &#125; &#125; # STF websocket server &#123; listen 7110; server_name $&#123;SERVER_NAME&#125;; resolver 127.0.0.11; # STF websocket requests will be proxied to STF websocket service directly location /socket.io/ &#123; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $host; proxy_pass http://stf_websocket; # enable WebSockets proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &quot;upgrade&quot;; &#125; &#125; # STF websocket providers (ports 7400-7499) server &#123; # can use &quot;listen 7400-7499;&quot; when OpenResty be upgraded to nginx 1.15.10 later listen 7400; listen 7401; listen 7402; listen 7403; listen 7404; listen 7405; listen 7406; listen 7407; listen 7408; listen 7409; listen 7410; listen 7411; listen 7412; listen 7413; listen 7414; listen 7415; listen 7416; listen 7417; listen 7418; listen 7419; listen 7420; listen 7421; listen 7422; listen 7423; listen 7424; listen 7425; listen 7426; listen 7427; listen 7428; listen 7429; listen 7430; listen 7431; listen 7432; listen 7433; listen 7434; listen 7435; listen 7436; listen 7437; listen 7438; listen 7439; listen 7440; listen 7441; listen 7442; listen 7443; listen 7444; listen 7445; listen 7446; listen 7447; listen 7448; listen 7449; listen 7450; listen 7451; listen 7452; listen 7453; listen 7454; listen 7455; listen 7456; listen 7457; listen 7458; listen 7459; listen 7460; listen 7461; listen 7462; listen 7463; listen 7464; listen 7465; listen 7466; listen 7467; listen 7468; listen 7469; listen 7470; listen 7471; listen 7472; listen 7473; listen 7474; listen 7475; listen 7476; listen 7477; listen 7478; listen 7479; listen 7480; listen 7481; listen 7482; listen 7483; listen 7484; listen 7485; listen 7486; listen 7487; listen 7488; listen 7489; listen 7490; listen 7491; listen 7492; listen 7493; listen 7494; listen 7495; listen 7496; listen 7497; listen 7498; listen 7499; server_name $&#123;SERVER_NAME&#125;; resolver 127.0.0.11; # Requests to STF providers will be proxied to STF providers directly location / &#123; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $host; proxy_pass http://$&#123;STF_IP&#125;:$server_port; # enable WebSockets proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &quot;upgrade&quot;; &#125; &#125;&#125; 注意，需要把这个文件中的${STF_IP}和${SERVER_NAME}换成实际的STF的IP和实际server的IP，这里做展示用因此隐藏了这个IP。 这个nginx.conf内容比较多，我们从上往下看。 10-12行声明了三个环境变量，环境变量需要在docker-compose.yml中事先声明好。需要说明的是，我在STF的启动命令里使用了--allow-remote，以允许用Wi-Fi远程控制设备。所以每次控制设备之前需要手动在STF所在服务器上调用adb connect {device_ip}:{adb_port}来确保STF服务器和设备处于已连接状态。这三个环境变量将在Lua脚本中被使用，之后会提到。 15-18行是用来将HTTP 1.1升级成Websocket协议的。 21-23行声明了Lua package的查找路径。*.lua表示文本格式的Lua脚本文件，*.so表示的是C动态链接库。 25-32行声明了STF的一些服务作为upstream，stf_app是给HTTP API用的，stf_websocket是给Websocket用的。 33-36行是Lua的一个第三方库jit-uuid要求的声明，初始化uuid seed。 39行开始是STF App部分的声明块，声明了/register-device, /deregister-device, /auth, /api/v1/devices/(.+)和所有其他HTTP API的处理方式。在/register-device, /deregister-device和/auth的声明中，content_by_lua_file后的Lua脚本会负责处理这个API的行为。对/api/v1/devices/(.+)，会先用access_by_lua_file指定的Lua脚本检查用户是否有权限请求这个STF API，然后根据检查结果，若是验证通过就转发至STF，否则返回权限不足。其余所有HTTP请求都直接转发至STF。 104-121行是STF Websocket部分的声明块，指定了当API为/socket.io/时，应该将HTTP 1.1升级到Websocket，并直接转发Websocket请求到STF Websocket service。 124-241行中有一长串的listen声明，这主要是因为在远程控制Android设备时，STF会启动provider来提供Websocket服务（注意和刚才题到的STF Websocket service相区别），可以在STF启动命令中使用--provider-min-port {port}和--provider-max-port {port}来指定，这里我的参数是--provider-min-port 7400 --provider-max-port 7499，所以就需要有100个listen声明。值得一提的是，nginx在1.15.10中可以在listen中使用一个端口范围，详情参见nginx listen。不过当前最新的OpenResty基于的Nginx版本是1.15.8，所以不支持。之后如果OpenResty更新后，可以直接使用listen 7400-7499;这种更简洁的方式。对于provider，所有请求都应该被直接转发。 完成nginx.conf后，就轮到设计各个Lua脚本文件了。这里不打算给出所有Lua脚本的详细代码，只说大致做法。 设备注册midway的/register-deviceAPI的行为（参数code、 timeout和device）： 在Redis中创建一个string，key: code, value: session，并设置过期时间为timeout。 在Redis中创建一个string，key: device, value: session，并设置过期时间为timeout。 在Redis中创建一个hash，key: session, value: { permission, device }，并设置过期时间为timeout。 SSH到STF的服务器上（会使用nginx.conf文件中声明的那三个环境变量），然后执行adb connect {device_ip}:{adb_port}。 用户浏览器授权midway的/auth?code=xxxAPI的行为： 在Redis中以code为key获取session。 如果能获取到，就移除key: code。然后设置浏览器的Cookie：”midwaysid={session}; Max-Age={timeout}; path=/“。最后重定向浏览器到midway的根路径下（对应STF的首页）。 如果获取不到session，返回400 Invalid code。 设备注销midway的/deregister-deviceAPI的行为（参数device）： 在Redis中移除key: device。 设备信息获取midway的/api/v1/devices/(.+)API的行为： 使用device做为key在Redis中查询它所对用的session。 将浏览器Cookie中的key为midwaysid的value和第一步查到的session对比，二者相等则认为验证通过，并直接转发请求到STF。 第二步中验证不通过将会导致返回403 Permission denied。 下面这个块是针对一个特殊的行为的，下面会详细说明： 123456# OpenResty injects JS code to STF index page to redirect browser to &apos;/&apos; after timeoutlocation ~ ^/$ &#123; proxy_ssl_name $host; lua_code_cache on; content_by_lua_file &quot;lua/index.lua&quot;;&#125; 确保用户在session timeout后被强制退出控制页面在实现了上述的所有细节后，实际使用时我发现一个问题。当用户进入到STF的设备远程控制页面后，之后的大部分操作都不再需要请求设备信息获取接口了，只需要Websocket连接就够了。由于目前我们只针对设备信息获取的API做了鉴权，假如用户一直停留在某个设备的远程控制页面不出来，就算timeout过了，还是可以继续使用的。 由于STF是一个单页Web应用，针对这个问题，我想到的最简单的方案是在用户浏览器初次请求STF首页的时候，在其中注入一段特定的JS代码。这段代码是由midway生成并插入到STF的首页响应文本中的。当然，如果session不存在或者为空，则不需要注入JS代码。这段JS代码大致是这样的： 1&lt;script&gt;setTimeout(function()&#123;alert('Remote control timeout!');location.href = '/';&#125;, $&#123;session-ttl&#125;)&lt;/script&gt; ${session-ttl}是session在Redis中的TTL，midway会负责填入这个值，这个TTL将随着用户刷新页面的时间的不同而不同。这段代码的主要目的是在TTL时间后，弹框提示用户之前为申请远程设备控制的timeout已经到了，并强制跳转到/。这样就把用户带离了设备控制页面，由于此时session已过期，用户就无法再次直接进入到STF的设备控制页面了。如果想要继续控制设备，就必须去device-spy再做一次申请。 我把对这个行为的控制放到lua/index.lua中。 还有一个地方值得说一下，STF默认缓存了首页文档，在初次请求后直到页面缓存失效，当请求首页的HTML页面时都会返回304 Not Modified。这个缓存效果是我们不想要的。这是因为注入到STF首页的JS代码的setTimeout的超时时间和session的TTL有关，因此我们肯定希望每次都返回由midway生成的代码。一个简单的做法是在index.lua中加入： 1ngx.header[\"Cache-Control\"] = \"no-store\" 另外，我们需要让response headers中的”Content-Length”符合我们生成的文档的长度，所以别忘了加入： 1ngx.header[\"Content-Length\"] = string.len(body) 部署既可以将STF和midway分别部署在两台独立的服务器上，也可以将它们部署在同一台服务器上。在初次访问STF时会有一个验证步骤，浏览器会被重定向到启动STF的命令中--public-ip所指向的地址。如果是分开部署，由于我们的目的是让midway反向代理STF的所有流量，因此--public-ip的值应该是midway的地址。","categories":[{"name":"web后端","slug":"web后端","permalink":"https://nullcc.github.io/categories/web后端/"}],"tags":[{"name":"OpenResty","slug":"OpenResty","permalink":"https://nullcc.github.io/tags/OpenResty/"}]},{"title":"聊一聊0.1+0.2=0.30000000000000004这件事","slug":"聊一聊0.1+0.2=0.30000000000000004这件事","date":"2019-12-14T16:00:00.000Z","updated":"2022-04-15T03:41:13.037Z","comments":true,"path":"2019/12/15/聊一聊0.1+0.2=0.30000000000000004这件事/","link":"","permalink":"https://nullcc.github.io/2019/12/15/聊一聊0.1+0.2=0.30000000000000004这件事/","excerpt":"本文专为对于0.1+0.2=0.30000000000000004这一结果很懵逼的人打造。","text":"本文专为对于0.1+0.2=0.30000000000000004这一结果很懵逼的人打造。 人们都知道在十进制下0.1+0.2的结果等于0.3，这个是答案毋庸置疑的。但是如果有一天你在某个编程语言环境下输入0.1+0.2，但计算机给出的答案是0.30000000000000004，相信不少人一开始一定会大吃一惊，难道计算机连这么简单的加法运算都能算错吗？甚至有人专门建了一个叫做0.30000000000000004.com的网站来记录各种编程语言对于0.1+0.2的计算结果。 0.1和0.2的二进制表示我们先来看一下十进制数0.1和0.2如何用二进制表示。0.1和0.2都是小数，对于小数需要采用乘2法来计算它们的二进制表示，也就从一个给定的小数开始，不断乘以2，对于每一轮计算的结果，如果个位是0，就提取0，然后继续计算，如果个位是1，就提取1，然后将个位置0并继续计算，如下： 1234567891011120.1 * 2 = 0.2 -&gt; 00.2 * 2 = 0.4 -&gt; 00.4 * 2 = 0.8 -&gt; 00.8 * 2 = 1.6 -&gt; 10.6 * 2 = 1.2 -&gt; 10.2 * 2 = 0.4 -&gt; 00.4 * 2 = 0.8 -&gt; 00.8 * 2 = 1.6 -&gt; 10.6 * 2 = 1.2 -&gt; 10.2 * 2 = 0.4 -&gt; 00.4 * 2 = 0.8 -&gt; 0... 0.1的二进制表示中出现了无限循环的情况，也就是(0.1)10 = (0.00110011001100…)2 再来看0.2的二进制表示： 1234567891011120.2 * 2 = 0.4 -&gt; 00.4 * 2 = 0.8 -&gt; 00.8 * 2 = 1.6 -&gt; 10.6 * 2 = 1.2 -&gt; 10.2 * 2 = 0.4 -&gt; 00.4 * 2 = 0.8 -&gt; 00.8 * 2 = 1.6 -&gt; 10.6 * 2 = 1.2 -&gt; 10.2 * 2 = 0.4 -&gt; 00.4 * 2 = 0.8 -&gt; 00.8 * 2 = 1.6 -&gt; 1... 0.2的二进制表示中也有无限循环的情况，也就是(0.2)10 = (0.01100110011001…)2 通过上述转换，我们可以发现计算机无法用二进制精确地表示某些十进制小数。 浮点数运算标准IEEE-754要理解浮点数运算，就必须提到IEEE-754浮点数运算标准。该标准被运用在大部分CPU和浮点运算器上，可以说是计算机浮点数运算的事实标准，它主要由加州大学伯克利分校的William Morton Kahan教授研究和制定。本文并不打算完整讲述IEEE-754，如有需要可以移步wikipedia-IEEE_754，下面将只介绍IEEE-754知识的一个子集。 IEEE-754规定一个浮点数由三个域组成，如下图： sign 符号位 exponent 指数偏移值 fraction 分数值 计算一个IEEE-754浮点数的真值公式如下： 1value = sign * exponent * fraction IEEE-754规定单精度浮点数(32 bits)和双精度浮点数(64 bits)的结构如下： 32位IEEE-754浮点数的真值计算公式： value = (-1)S 2E-127 (1.M) 64位IEEE-754浮点数的真值计算公式： value = (-1)S 2E-1023 (1.M) 上述两个公式中，对于32位浮点数，E = e + 127，对于64位浮点数，E = e + 1023。 可以用一种简单的方法将一个浮点数的二进制形式转换成IEEE-754格式，步骤如下： 将浮点数的二进制表示为科学计数法形式。 将该浮点数的科学计数法表示的符号位提取出来，正数为0，负数为1，作为S。 将该浮点数的科学计数法表示的2的次方提取出来，作为e。 将该浮点数的科学计数法表示的尾数提取出来，作为M。 计算(-1)S 2E-127 (1.M)的值，即为该IEEE-754格式浮点数的真值。 按照上面的步骤我们以双精度浮点数来计算0.1+0.2。将下面两个序列以二进制相加： (0.1)10 = (0.00110011001100…)2(0.2)10 = (0.01100110011001…)2 我们得到： (0.3)10 = (0.1001100110011001…)2 同样可以发现，计算机也无法精确地表示十进制数的0.3。 将0.3的二进制形式转换为科学计数法形式： (0.3)10 = (1.001100110011001…)2 * 2-1 于是得到 S = 0， E = -1 + 1023 = 1022，M = (0.001100110011001…)2 所以在IEEE-754的规定下，由上述S，E和M所表示的64位双精度浮点数的真值就是： value = (-1)0 21022-1023 (1.001100110011001…)2 64位双精度浮点数的M长度是52位，因此(0.001100110011001…)2实际上被存储为(0001100110011001100110011001100110011001100110011010)2，请注意最后两位，按照之前的规律应该是01（很容易发现0.3的二进制形式中包含循环模式1001），怎么变成10了呢？实际上这是舍入操作导致的。IEEE-754规定了四种舍入方式： 舍入到最接近，这是默认的舍入方式，会将结果舍入为最接近且可以表示的值，但是当存在两个数一样接近的时候，则取其中的偶数（在二进制中是以0结尾的） 朝+∞方向舍入 朝-∞方向舍入 朝0方向舍入 如过要对10011001舍入左起第3位（从0开始）之后的值，则有两种选择： 1001 (10011001-00001001） 1010 (10011001+00000111) 由于舍入到1010(+7)比舍入到1001(-9)更接近10011001，因此上面的M的最后两位就是10而不是01了。 验证说了这么多，还需要验证我们的理论。计算验证很简单，可以写一段Python代码来验证这个结果： 12345678910111213def bin2dec(b): bit_seq = [int(bit) for bit in b if bit != '.'] value = 0 for i, bit in enumerate(bit_seq): value += 2**(-i-1) * bit return valueS = 0e = -1E = e + 1023M = '.001100110011001100110011001100110011001100110011010'print((-1)**0 * 2**(1022-1023) * (bin2dec('1' + M))) # 0.30000000000000004","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://nullcc.github.io/categories/计算机基础/"}],"tags":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://nullcc.github.io/tags/计算机基础/"},{"name":"浮点数","slug":"浮点数","permalink":"https://nullcc.github.io/tags/浮点数/"}]},{"title":"CSAPP读书笔记(书已看完，剩下的读书笔记都在心里（逃。。)","slug":"CSAPP读书笔记(长期更新)","date":"2019-09-28T16:00:00.000Z","updated":"2022-04-15T03:41:13.013Z","comments":true,"path":"2019/09/29/CSAPP读书笔记(长期更新)/","link":"","permalink":"https://nullcc.github.io/2019/09/29/CSAPP读书笔记(长期更新)/","excerpt":"决定花时间温习一遍CSAPP，本文是 CSAPP《深入理解计算机系统》的读书笔记。","text":"决定花时间温习一遍CSAPP，本文是 CSAPP《深入理解计算机系统》的读书笔记。 第一章 计算机系统漫游编译系统的组成 硬件系统的基本组成 总线 I/O设备 主存 处理器 总线总线每次传送定长的字节块，称作字(word)，字中的字节数称为字长。字长是系统的基本参数，它的大小在不同计算机中不尽相同，常见的字长有4(32 bits)和8(64 bits)。 I/O设备每个I/O设备都通过一个控制器或适配器与I/O总线相连。 主存主存在物理上由一组动态随机存取存储器(DRAM)芯片组成，逻辑上它是一个线性的字节数组，每个字节都有唯一的地址（数组索引），从0开始。 处理器处理器是解释和执行存储在主存中指令的引擎，它的核心是一个大小为一个字的存储设备（寄存器），称作程序计数器(PC)，在任何时刻，PC都指向主存中的某条机器语言指令，即PC保存的是主存中的某个地址。由此很容易理解为何计算机可寻址的内存大小和它的字大小密切相关，因为PC的容量即是该处理器的字长，能保存的最大主存地址也就是一个字。比如字长为4字节的32位计算机最大寻址只能到4GB(2^32 = 4GB)。 处理器一直在不断地执行PC指向指令，接着更新PC，将其指向下一条指令，下一条指令的地址和刚被执行的上一条指令的地址不一定是相邻的。 处理器中包含一些拥有固定名字的寄存器，这些寄存器的大小是单个字长。 处理器在指令的要求下可能会执行下面几个典型的操作： 加载：从主存复制一个字节或一个字到寄存器，覆盖寄存器中原来的值。 存储：从寄存器复制一个字节或一个字到主存的某个位置，覆盖主存中这个位置原来的值。 操作：把两个寄存器的内容复制到ALU做算术运算，并将结果存放到另一个寄存器中，覆盖这个寄存器中原来的值。 跳转：从指令中提取一个字，并将这个字复制到PC中覆盖PC中原来的值。 存储器层次结构存储器层次结构呈金字塔状。从上至下，设备的访问速度越来越慢，容量越来越大，单位字节的造价越来越低。存储器层次结构的主要思想是上一层存储器作为下一层存储器的高速缓存。如下图： 操作系统管理硬件操作系统是介于硬件和应用程序之间的一层软件系统，所有应用程序对硬件的操作都必须经过操作系统。 操作系统的两个基本功能是： 防止硬件被失控的应用程序滥用。 向应用程序提供简单一致的机制来控制复杂而又通常大不相同的低级硬件设备。 操作系统提供了三个抽象概念来实现这两个基本功能： 进程 虚拟内存 文件 进程操作系统提供了一种假象：系统上只有这个进程在运行，使程序看上去独占处理器、主存和I/O设备。实际上在一个系统上可以同时运行多个进程，进程数是可以多于CPU个数的。CPU通过在进程间快速切换来给人以所有进程都在并发执行的假象。 为了达到CPU在进程间切换的效果，操作系统负责管理进程运行的上下文，上下文包括PC、寄存器的当前值和主存的内容等。单处理器在任一时刻只能运行一个进程的代码。当操作系统决定要进行进程切换时，会先保存当前进程的上下文信息，然后将新进程的上下文恢复，并将控制权传递到新进程。新进程就会从它上次暂停的地方继续往下运行。 注：进程是操作系统进行资源分配的最小单位 线程在操作系统中，一个进程可以又多个称为线程的执行单元构成，每个线程都运行在进程的上下文中。同一进程中的多个线程共享代码和全局数据。 注：线程是操作系统进行任务调度和执行的最小单位 虚拟内存虚拟内存提供了一种假象：每个进程都在独占地使用内存。每个进程看到的内存都是一致的，称为虚拟地址空间。下图是Linux的虚拟地址空间示意图(以hello程序为例)： 文件文件就是字节序列。每个I/O设备，包括键盘、磁盘、显示器、打印机和网络都可以看成文件。系统中的所有输入输出都是通过调用一组称为Unix I/O的系统调用读写文件来实现的。 文件的概念简单而强大，它屏蔽了所有底层硬件的实现细节，通过一致的视图来操作这些硬件。这使得不同厂商提供的设备都能运行在同一台计算机上。 重要主题Amdahl’s Law (阿姆达尔定律)阿姆达尔定律的主要思想是，当我们对系统的某个部分加速时，其对系统整体性能的影响取决于该部分的重要性和加速程度。其公式如下： S=1/(1-a+a/n) 其中a为被加速部分占原整体计算的比例，n为性能提升比例。 按照这个公式，可以推导出两个极端极端情况： 当a=1时，\b\bS=n。整体计算没有串行部分而全部都是并行部分，加速比为n，此时可以通过增加处理节点来提高整体性能。 当a=0时，S=1。整体计算全部都是串行部分，加速比为1，此时无法通过增加处理节点来提供整体性能。 当n-&gt;+∞时，S=1/(1-a)。这是一种极限情况。举个例子假如a=0.6，n-&gt;+∞则S=2.5，也就是说当被优化部分的代码占整体计算的60%时，无论怎么优化这部分代码的性能，整体获得的加速比最高不超过2.5。 并发和并行在系统中有三个层次的并发和并行： 线程级并发 指令级并行 单指令、多数据并行 第二章 信息的表示和处理整数的表示虽然只能编码一个相对较小的数值范围，但这种表示是精确的；浮点数虽然能编码一个较大的数值范围，但这种表示是近似的。 信息存储 计算机使用字节(byte, 1byte=8bits)而不是单独的位来作为最小寻址单位。 机器级程序将内存视为一个非常大的字节数组，称为虚拟内存。内存的每个字节都由一个唯一的数字来标识，称为它的地址，所有可能的地址的集合就称为虚拟地址空间。 虚拟地址空间是一个逻辑上的概念，实际上计算机将动态随机访问存储器(DRAM)、闪存、磁盘存储器、特殊硬件和操作系统软件结合起来，为程序提供一个看上去统一的字节数组。 十六进制表示法由于二进制表示法太冗长，人们常用十六进制表示法，可用值为0-9A-F。人们常以0x开头表示十六进制值。 在十六进制和二进制之间相互转换非常容易，如果给定一个二进制序列要转换为十六进制，只需从低位到高位以4位一组分组，最左边的一组位数若不足4位可以在左边补0。然后将每个4位组直接转换为0-9A-F中的一个值即可。将十六进制转换为二进制就是将每一位以其相应的二进制位串代替即可。 字数据大小字长(word size)指明了指针数据的标称大小，字长决定了虚拟地址空间的最大大小。对于一个字长为w的机器而言，其虚拟地址空间范围为0-2^w-1，程序最多访问2^w个字节。 寻址和字节顺序有两种字节顺序： 小端法(little endian)是最低有效字节在最前面。 大端法(big endian)是最高有效字节在最前面。 比如一个int类型的变量值为0x01234567，位于地址0x100处，在两种字节顺序下的情况如下： 小端法： 12... 0x100 0x101 0x102 0x103 ... 67 45 23 01 大端法： 12... 0x100 0x101 0x102 0x103 ... 01 23 45 67 对于选择哪种字节顺序并没有任何技术上的理由。 表示字符串C语言中的字符串被编码成一个以null(其值为0)字符结尾的字符数组，每个字符都由某个标准编码来表示，如最常见的ASCII字符编码。ASCII字符集适合编码英文文档，如果要支持多语言文字，就需要使用Unicode编码。 表示代码不同机器类型使用不同的且不兼容的指令和编码方式，因此二进制代码是不兼容的。 布尔代数简介布尔代数的操作符： 非：~ 与：&amp; 或：| 异或：^ ~ 0 1 1 0 &amp; 0 1 0 0 0 1 0 1 &#124; 0 1 0 0 1 1 1 1 ^ 0 1 0 0 1 1 1 0 布尔代数的一些数学属性： 布尔运算&amp;对|的分配律：a&amp;(b|c) 等价于 (a&amp;b)|(a&amp;c)。 布尔运算|对&amp;的分配律：a|(b&amp;c) 等价于 (a|b)&amp;(a|c)。 对任何值a来说，有a^a=0，(a^b)^a=b。 位向量还可以用来表示有限集合，比如位向量a=[01101001]表示集合{0,3,5,6}。 C语言中的位级运算 按位或：| 按位与：&amp; 取反：~ 按位异或：^ C语言中的逻辑运算 或：|| 与：&amp;&amp; 非：! 逻辑运算认为所有非零的参数都表示TRUE，而参数0表示FALSE。逻辑运算的结果为1或0，表示TRUE或FALSE。 C语言中的移位运算移位运算是从左至右可结合的，所以x&lt;&lt;j&lt;&lt;k等价于(x&lt;&lt;j)&lt;&lt;k 有3种移位运算： 左移 逻辑右移 算数右移 右移分两种：逻辑右移和算数右移。逻辑右移x&gt;&gt;k是在左端补k个0，而算数右移x&gt;&gt;k是在左端补k个最高有效位的值。 实际上，几乎所有的编译器和机器组合都对有符号数使用算数右移，另外对于无符号数，右移必须是逻辑的。 整数表示整型数据类型32位程序上C语言整型数据类型的典型取值范围： C数据类型 最小值 最大值 [signed] char -128 127 unsigned char 0 255 short -32768 32767 unsigned short 0 65535 int -2147483648 2147483647 unsigned 0 4294967295 long -2147483648 2147483647 ungigned long 0 4294967295 int32_t -2147483648 2147483647 uint32_t 0 4294967295 int64_t -9223372036854775808 9223372036854775807 uint64_t 0 18446744073709551615 64位程序上C语言整型数据类型的典型取值范围： C数据类型 最小值 最大值 [signed] char -128 127 unsigned char 0 255 short -32768 32767 unsigned short 0 65535 int -2147483648 2147483647 unsigned 0 4294967295 long -9223372036854775808 9223372036854775807 ungigned long 0 18446744073709551615 int32_t -2147483648 2147483647 uint32_t 0 4294967295 int64_t -9223372036854775808 9223372036854775807 uint64_t 0 18446744073709551615 C语言的整型数据类型的保证的取值范围： C数据类型 最小值 最大值 [signed] char -127 127 unsigned char 0 255 short -32767 32767 unsigned short 0 65535 int -32767 32767 unsigned 0 65535 long -2147483647 2147483647 ungigned long 0 4294967295 int32_t -2147483648 2147483647 uint32_t 0 4294967295 int64_t -9223372036854775808 9223372036854775807 uint64_t 0 18446744073709551615 另外，C/C++都支持有符号数（默认）和无符号数，Java只支持有符号数。 无符号数的编码\b\b原理：无符号数编码的定义 无符号数的二进制表示有一个很重要的属性，即每个介于0-2^w-1之间的数都有唯一一个w位的值编码。 原理：无符号数编码的唯一性 补码编码补码是最常见的计算机编码有符号数的方式（另外两种方式为反码和原码）。 原理：补码编码的定义 原理：补码编码的唯一性 一些重要的数字的补码表示： 数 8位字长 16位 32位 64位 UMax 0xFF255 0xFFFF65535 0xFFFFFFFF4294967295 0xFFFFFFFFFFFFFFFF18446744073709551615 TMin 0x80-128 0x8000-32768 0x80000000-2147483648 0x8000000000000000-9223372036854775808 TMax 0x7F127 0x7FFF32767 0x7FFFFFFF2147483647 0x7FFFFFFFFFFFFFFF9223372036854775807 -1 0xFF 0xFFFF 0xFFFFFFFF 0xFFFFFFFFFFFFFFFF 0 0x00 0x0000 0x00000000 0x0000000000000000 以下几点值得注意： 补码的取值范围是不对称的：|TMin| = |TMax| + 1 最大的无符号数值刚好比补码的最大值的两倍大1：UMax = 2TMax + 1 C库文件&lt;limits.h&gt;中定义了一组常量来限定编译器运行的这台机器的不同整型数据类型的取值范围，比如INT_MAX、INT_MIN和UINT_MAX。 有符号数的其他表示方法： 反码，除了最高有效位的权是-(2^w-1 - 1)而不是-2^w-1，其余和补码一样。 原码，最高有效位是符号位。 需要注意的是，反码和原码对0的编码表示有两种，[00…0]都解释为+0，而-0在原码中为[10…0]，在反码则为[11…1]。目前几乎所有现代机器都使用补码扁食有符号数。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://nullcc.github.io/categories/读书笔记/"}],"tags":[{"name":"Basics","slug":"Basics","permalink":"https://nullcc.github.io/tags/Basics/"}]},{"title":"TypeScript中利用transformer获取interface keys","slug":"TypeScript中利用transformer获取interface keys","date":"2019-07-17T16:00:00.000Z","updated":"2022-04-15T03:41:13.021Z","comments":true,"path":"2019/07/18/TypeScript中利用transformer获取interface keys/","link":"","permalink":"https://nullcc.github.io/2019/07/18/TypeScript中利用transformer获取interface keys/","excerpt":"本文分成四个部分： 需求和灵感 TypeScript的抽象语法树简介 TypeScript transformer简介 编写获取TypeScript interface keys的transformer","text":"本文分成四个部分： 需求和灵感 TypeScript的抽象语法树简介 TypeScript transformer简介 编写获取TypeScript interface keys的transformer 需求和灵感\b\b\b\b使用过TypeScript写代码的同学都对interface这个东西不陌生，借助interface来定义一些纯值对象的类型是再简单不过了。最开始我的需求很简单，想用interface来定义一个HTTP API的response DTO，在对一个API进行测试的时候，可以验证这个API的response结构是否和我用interface定义的结构相同。 刚开始想到可以使用ES 6的class来定义DTO，然后通过在运行时获取class的属性。这确实可以，但是用起来有点麻烦，比如下面的代码： 12345class X &#123; a: number; b: string;&#125;console.log(Object.getOwnPropertyNames(new X())); // [] 这还不够，需要对每个属性赋值： 12345class X &#123; a = 0; b = '';&#125;console.log(Object.getOwnPropertyNames(new X())); // [ 'a', 'b' ] 或者在X的constructor里初始化一下属性（如果只是为了拿到属性名字，直接对每个属性赋值null即可）： 12345678910class X &#123; a: number; b: string; constructor() &#123; this.a = null; this.b = null; &#125;&#125;console.log(Object.getOwnPropertyNames(new X())); // [ 'a', 'b' ] 虽然这样做也许可行，但是很快我就否定了这种用法。我只是想简单地声明一种类型，然后再需要的时候可以获取这个类型的所有属性。现在不仅要显式初始化所有属性（在constructor中或者直接在class声明属性的时候赋值），还要用new生成一个实例，实在不够优雅。其实在TypeScript中声明DTO一类的东西用interface会好一些，声明的代码简洁，支持直接嵌套属性，也可以声明属性的类型为其他interface，这和真实的HTTP Response Data的结构几乎一模一样： 12345678910111213interface X &#123; a: number; b: &#123; c: string; d: Y; &#125;;&#125;interface Y &#123; u: string; v: &#123; w: number; &#125;&#125; 遗憾的是，虽然interface很适合用来描述HTTP Response Data，但正常情况下如果想在运行时获取interface的keys用来和真正的HTTP Response Data结构做对比是不行的，因为TypeScript的interface实际上并不存在于runtime，要理解这个问题需要知道TypeScript针对JavaScript提供了一整套的类型辅助系统，但仅仅是辅助，最终的代码还是要转换成JavaScript来执行。由于JavaScript中并不存在interface，因此也就无法在runtime获得interface的keys了。 不过也不是完全没有希望，经过一番搜索，我发现了ts-transformer-keys这个包，该包宣称可以获得interface的keys。仔细研究了一下，发现这个包提供一个keys&lt;T&gt;()方法，其实现原理是使用了自定义的transformer在将代码转换成JavaScript时获取了interface的信息，然后修改了调用keys&lt;T&gt;()处的抽象语法树(Abstract Syntax Tree, AST)节点信息。换句话说，这个包提供的transformer在将代码转换成JavaScript时直接从AST中找到相应interface的keys，然后创建一个包含所有keys数组，并将这个数组直接输出到转换出来的JavaScript代码中。 举个简单的例子： 12345interface Foo &#123; a: number; b: string;&#125;console.log(keys&lt;Foo&gt;()); 上面这几行代码在被转换成JavaScript时被替换成了下面这行： 1console.log([\"a\", \"b\"]); 正如上面所描述的，ts-transformer-keys对AST Nodes做了遍历-转换，这种能力正是我所需要的。进一步说，由于response DTO内部经常是嵌套结构的，因此很自然想到是否可以支持嵌套interface，比如下面这种情况： 123456789interface Foo &#123; a: number; b: Bar;&#125;interface Bar &#123; c: boolean; d: string;&#125;console.log(keys&lt;Foo&gt;()); 但是ts-transformer-keys的输出还是只有a和b，看来ts-transformer-keys尚未支持这种用法。 1console.log([\"a\", \"b\"]); 再进一步，我还想要得到interface各个key的类型和存在性，目前ts-transformer-keys也不支持。不过没关系，知道了内部的实现原理，完全可以自己写一个transformer。 TypeScript的抽象语法树简介在真正开始编写自己的transformer之前，有必要简单了解一下TypeScript的抽象语法树和TypeScript对操作抽象语法树所提供的支持。 抽象语法树(Abstract Syntax Tree，AST)，下文简称为AST，是源代码语法结构的一种抽象表示。为了更直观地观察TypeScript的AST，可以借助ts-ast-viewer这个工具来以树形结构将其可视化。先看一个基本的TypeScript interface的抽象语法树表示，假设有如下代码： 1234interface Foo &#123; a?: number; b: string;&#125; 使用ts-ast-viewer可以得到上面代码的AST结构： 从图中可以很清楚地看到Foo的AST表示，另外在右边的Node部分，还能查看到其AST中具体节点的信息，对于TypeScript的interface我们关心的属性名称、存在性和类型都可找到相应的字段来对应。 图形化表示如下： 源代码的几乎每一个细节，在AST中都有体现。让我们从上到下走马观花一下： 最顶层是SourceFile，每一个TypeScript源代码文件都会对应一个SourceFile。 SourceFile下直接包含的SyntaxList包括了这个文件中的所有语法结构，在这里只有这个interface声明，如果还有其他语法结构，也将被包含在内。 InterfaceDeclaration表示这个interface的声明。 InterfaceKeyword表示关键字interface。 紧接着的Identifier对应的是interface的名字Foo。 OpenBraceToken表示{。 接下来又是一个SyntaxList，这个SyntaxList和刚才看到的那个不一样，它只包括了interface Foo中声明的所有语法结构，这样的结构划分有点类似作用域。 之后的PropertySignature是一个属性签名，表示a?: number;。 PropertySignature下的一些属性，Identifier表示属性名a，QuestionToken表示?，ColonToken表示:，NumberKeyword表示属性名a的类型是number，SemicolonToken则表示;。 后面的结构和前面差不多就不赘述了。 值得一提的是，在TypeScript的类型声明文件typeacript.t.ts的SyntaxKind这个enum声明中，可以找到上面列举的AST语法结构类型的声明，编写transformer的时候我们还会用到它。另外，之前提到ts-transformer-keys是使用transformer来遍历AST Nodes以获取interface keys，并就地创建一个Array，将keys数组（是一个字符串数组）复制给原来TypeScript代码中keys&lt;T&gt;()对应的左值。因此我们还需要能遍历，修改和创建AST Nodes，实际上TypeScript对这些操作已经提供了支持，具体细节之后会谈到。 上面AST内部的细节部分将在实际编写transformer的时候再来研究，现在只需要大致知道它的结构就可以了。 TypeScript transformer简介在介绍transformer之前需要大致了解一下TypeScript的编译过程。 在TypeScript的Wiki中可以找到一篇和TypeScript内部架构和编译过程有关的文章，大部分网络上涉及TypeScript编译过程的文章大都参考它：TypeScript Architectural Overview。 根据文章中的介绍，TypeScript的核心编译过程中涉及的编译组件主要有下面几个： Pre-processor: 预处理器（包含Scanner）。 Parser: 语法分析器。 Binder: 绑定器。 Type resolver/ Checker: 类型检查器，解析每种类型的构造，负责处理、检查针对每个类型的语义操作，并生成合适的诊断信息。 Emitter：\b生成器，负责根据输入的.ts和.d.ts文件生成最终的结果，它有三种可能的输出：JavaScript源码(.js)、类型定义文件(.d.ts)或source map文件(.js.map)，其中类型定义文件可以帮助开发者在各种IDE中获取TypeScript的类型信息，source map文件则是一个存储源代码与编译代码对应位置映射的信息文件，在debug时我们需要利用source map文件来找到实际运行的代码(最终生成的.js文件)和其原始代码(开发者实际编写的.ts文件)的位置对应关系。 TypeScript的编译过程简单归纳如下： 在编译过程的开始阶段，输入是一些.ts源代码，Pre-processor会计算出有哪些源代码文件将参与编译过程（它会查找import语句和用///的引用语句），并在内部调用扫描器(Scanner)对所有源文件进行扫描，并封装成Tokens流，作为之后Parser的输入。 Parser以预处理器产生的Tokens流作为输入，根据语言语法规则生成抽象语法树(AST)，每个源文件的AST都有一个SourceFile节点。 Binder会遍历AST，并使用符号(Symbol)来链接相同结构的声明（例如对于具有相同结构的interface或模块，或者同名的函数或模块）。这个机制能帮助类型系统推导出这些具名声明。Binder也会处理作用域，确保每个Symbol都在正确的作用域中被创建。到目前为止，编译过程已经对每个单独的.ts文件进行了处理，得到了每个.ts文件的AST（每个AST都有一个SourceFile节点作为根节点）。接下来还需要将所有.ts文件的SourceFile合并在一起形成一个程序(Program)，TypeScript提供了一个ts.createProgramAPI来创建Program。我们知道源代码文件经常互相引用，下一步还将处理这些引用关系。 生成Program后，TypeChecker会负责计算出不同SourceFile中的Symbol引用关系，并将Type赋值给Symbol，并在此时生成语义诊断（如果有错误的话）。 对于一个Program，会生成一个Emitter，Emitter要做的就是针对每个SourceFile生成输出(.js/.d.ts/.js.map)。 另外，在TypeScript的Wiki还能找到一篇比较“残缺”的文章（估计是项目开发人员忙于具体实现懒得更新Wiki了），提到了transformer：TypeScript Compiler-Internals 摘录transformer部分的内容如下，其中translated和transforms颇为微妙： The transformer is nearing completion to replace the emitter. The change in name is because the emitter translated TypeScript to JavaScript. The transformer transforms TypeScript or JavaScript (various versions) to JavaScript (various versions) using various module systems. The input and output are basically both trees from the same AST type, just using different features. There is still a small printer that writes any AST back to text. 这里对emitter的功能描述是translated TypeScript to JavaScript，emitter的作用是将TypeScript代码翻译成JavaScript代码。而翻译的意思是保持原文意思不变，也就是说emitter对TypeScript代码没有添油加醋，是照原样转成JavaScript的。而对transformer的功能描述是transforms TypeScript or JavaScript (various versions) to JavaScript (various versions) using various module systems，这里的transforms还有转换、变换的功能。 一言以蔽之，transformer对开发者暴露了AST，使我们能按照我们的意愿遍历和修改AST（这种修改包括删除、创建和直接修改AST Nodes）。 有了这些信息做铺垫后，可以用一张流程图来表示TypeScript的编译过程： 编写获取TypeScript interface keys的transformer终于到了实际写代码的环节了。在真正实现获取interface keys的transformer之前我们还有几个准备工作要做： 实现一个最简单的transformer，之后的工作将在此基础上展开。 研究如何将transformer集成到TypeScript项目中。 首先我们需要一种能在项目中使用transformer的方式，这里我选择ttypescript，因为它使用起来非常简单，另外还有一种方式是使用ts-loader结合webpack，篇幅关系这里就只介绍使用ttypescript的方式。 以ttypescript提供的例子为基础，我们可以先写一个基础的transformer（部分代码来自于ts-transformer-keys）： 1234567891011121314151617181920212223242526272829303132333435363738// src/transformer.tsimport * as ts from 'typescript';export default (program: ts.Program): ts.TransformerFactory&lt;ts.SourceFile&gt; =&gt; &#123; return (ctx: ts.TransformationContext) =&gt; &#123; return (sourceFile: ts.SourceFile): ts.SourceFile =&gt; &#123; const visitor = (node: ts.Node): ts.Node =&gt; &#123; return ts.visitEachChild(visitNode(node, program), visitor, ctx); &#125;; return &lt;ts.SourceFile&gt; ts.visitEachChild(visitNode(sourceFile, program), visitor, ctx); &#125;; &#125;;&#125;const visitNode = (node: ts.Node, program: ts.Program): ts.Node =&gt; &#123; const typeChecker = program.getTypeChecker(); if (!isKeysCallExpression(node, typeChecker)) &#123; return node; &#125; return ts.createStringLiteral('will be replaced by interface keys later');&#125;;const indexTs = path.join(__dirname, './index.ts');const isKeysCallExpression = (node: ts.Node, typeChecker: ts.TypeChecker): node is ts.CallExpression =&gt; &#123; if (!ts.isCallExpression(node)) &#123; return false; &#125; const signature = typeChecker.getResolvedSignature(node); if (typeof signature === 'undefined') &#123; return false; &#125; const &#123; declaration &#125; = signature; return !!declaration &amp;&amp; !ts.isJSDocSignature(declaration) &amp;&amp; (path.join(declaration.getSourceFile().fileName) === indexTs) &amp;&amp; !!declaration.name &amp;&amp; declaration.name.getText() === 'keys';&#125;; 几个地方解释一下： 在导出方法中，ts.visitEachChild可以使用开发者提供的visitor来访问AST Node的每个子节点，并且在visitor中允许返回一个相同类型的新节点来替换当前被访问的节点。 visitNode接受一个ts.Node和ts.Program类型的参数会在访问指定节点的每个子节点时被调用，这个方法需要放回一个ts.Node类型的对象，如果不想对当前节点做任何改变的话，直接返回实参中的node即可，如果想要做一些转换，那就需要自己编码实现了，这也是这个transformer实际发挥作用的地方。目前这里的做法是遇到keys&lt;T&gt;()调用就将节点替换为一个字符串’will be replaced by interface keys later’。 这里会沿用ts-transformer-keys的调用方式keys&lt;T&gt;()，我们需要判断调用点，isKeysCallExpression就是用来判断源码中调用keys&lt;T&gt;()的地方。 写个测试来验证一下： 123456789// test/transformer.test.tsimport &#123; keys &#125; from '../index';describe('Test transformer.', () =&gt; &#123; test('Should output \\\"will be replaced by interface keys later\\\".', () =&gt; &#123; interface Foo &#123;&#125; expect(keys&lt;Foo&gt;()).toEqual('will be replaced by interface keys later'); // true &#125;);&#125;); 测试通过说明我们的transformer生效了。 接下来要进入本文最重要的部分（请原谅我前面铺垫了这么多=。=）：编写获取interface keys的代码了。在第一部分已经列出了一个包含interface的SourceFile的AST结构，不过里面的interface的结构是平坦的，没有嵌套的层级关系。而我们的目的是能够支持具有层级关系和嵌套的interface，一个有层级关系的interface的AST结构如下： 我们需要嵌套地对interface的property做处理，完整的代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128import * as ts from 'typescript';import * as path from 'path';export default (program: ts.Program): ts.TransformerFactory&lt;ts.SourceFile&gt; =&gt; &#123; return (ctx: ts.TransformationContext) =&gt; &#123; return (sourceFile: ts.SourceFile): ts.SourceFile =&gt; &#123; const visitor = (node: ts.Node): ts.Node =&gt; &#123; return ts.visitEachChild(visitNode(node, program), visitor, ctx); &#125;; return &lt;ts.SourceFile&gt; ts.visitEachChild(visitNode(sourceFile, program), visitor, ctx); &#125;; &#125;;&#125;interface InterfaceProperty &#123; name: string; optional: boolean;&#125;const symbolMap = new Map&lt;string, ts.Symbol&gt;();const visitNode = (node: ts.Node, program: ts.Program): ts.Node =&gt; &#123; if (node.kind === ts.SyntaxKind.SourceFile) &#123; (&lt;any&gt;node).locals.forEach((value: any, key: string) =&gt; &#123; if (!symbolMap.get(key)) &#123; symbolMap.set(key, value); &#125; &#125;); &#125; const typeChecker = program.getTypeChecker(); if (!isKeysCallExpression(node, typeChecker)) &#123; return node; &#125; if (!node.typeArguments) &#123; return ts.createArrayLiteral([]); &#125; const type = typeChecker.getTypeFromTypeNode(node.typeArguments[0]); let properties: InterfaceProperty[] = []; const symbols = typeChecker.getPropertiesOfType(type); symbols.forEach(symbol =&gt; &#123; properties = [ ...properties, ...getPropertiesOfSymbol(symbol, [], symbolMap) ]; &#125;); return ts.createArrayLiteral(properties.map(property =&gt; ts.createRegularExpressionLiteral(JSON.stringify(property))));&#125;;const getPropertiesOfSymbol = (symbol: ts.Symbol, outerLayerProperties: InterfaceProperty[], symbolMap: Map&lt;string, ts.Symbol&gt;): InterfaceProperty[] =&gt; &#123; let properties: InterfaceProperty[] = []; let propertyPathElements = JSON.parse(JSON.stringify(outerLayerProperties.map(property =&gt; property))); const property = symbol.escapedName; propertyPathElements.push(property); let optional = true; for (let declaration of symbol.declarations) &#123; if (undefined === (&lt;any&gt;declaration).questionToken) &#123; optional = false; break; &#125; &#125; const key = &lt;InterfaceProperty&gt; &#123; name: propertyPathElements.join('.'), optional, &#125;; properties.push(key); const propertiesOfSymbol = _getPropertiesOfSymbol(symbol, propertyPathElements, symbolMap); properties = [ ...properties, ...propertiesOfSymbol, ]; return properties;&#125;;const isOutermostLayerSymbol = (symbol: any): boolean =&gt; &#123; return symbol.valueDeclaration &amp;&amp; symbol.valueDeclaration.symbol.valueDeclaration.type.members;&#125;;const isInnerLayerSymbol = (symbol: any): boolean =&gt; &#123; return symbol.valueDeclaration &amp;&amp; symbol.valueDeclaration.symbol.valueDeclaration.type.typeName;&#125;;const _getPropertiesOfSymbol = (symbol: ts.Symbol, propertyPathElements: InterfaceProperty[], symbolMap: Map&lt;string, ts.Symbol&gt;): InterfaceProperty[] =&gt; &#123; if (!isOutermostLayerSymbol(symbol) &amp;&amp; !isInnerLayerSymbol(symbol)) &#123; return []; &#125; let properties: InterfaceProperty[] = []; let members: any; if ((&lt;any&gt;symbol.valueDeclaration).type.symbol) &#123; members = (&lt;any&gt;symbol.valueDeclaration).type.members.map((member: any) =&gt; member.symbol); &#125; else &#123; const propertyTypeName = (&lt;any&gt;symbol.valueDeclaration).type.typeName.escapedText; const propertyTypeSymbol = symbolMap.get(propertyTypeName); if (propertyTypeSymbol) &#123; if (propertyTypeSymbol.members) &#123; members = propertyTypeSymbol.members; &#125; else &#123; members = (&lt;any&gt;propertyTypeSymbol).exportSymbol.members; &#125; &#125; &#125; if (members) &#123; members.forEach((member: any) =&gt; &#123; properties = [ ...properties, ...getPropertiesOfSymbol(member, propertyPathElements, symbolMap), ]; &#125;); &#125; return properties;&#125;;const indexTs = path.join(__dirname, './index.ts');const isKeysCallExpression = (node: ts.Node, typeChecker: ts.TypeChecker): node is ts.CallExpression =&gt; &#123; if (!ts.isCallExpression(node)) &#123; return false; &#125; const signature = typeChecker.getResolvedSignature(node); if (typeof signature === 'undefined') &#123; return false; &#125; const &#123; declaration &#125; = signature; return !!declaration &amp;&amp; !ts.isJSDocSignature(declaration) &amp;&amp; (path.join(declaration.getSourceFile().fileName) === indexTs) &amp;&amp; !!declaration.name &amp;&amp; declaration.name.getText() === 'keys';&#125;; 完整的repo可以移步ts-interface-keys-transformer。 使用该transformer非常简单，首先安装ttypescript： 1npm i ttypescript 然后在tsconfig.json的compilerOptions下增加如下信息： 123&quot;plugins&quot;: [ &#123; &quot;transform&quot;: &quot;ts-interface-keys-transformer/transformer&quot; &#125;] 例子如下： 12345678910111213141516171819202122232425262728import &#123; keys &#125; from 'ts-interface-keys-transformer';interface Foo &#123; a: number; b?: string; c: &#123; d: number; e?: boolean; &#125; f: Bar;&#125;interface Bar &#123; x: string; y: number;&#125;console.log(keys&lt;Foo&gt;());// output:// [ &#123; name: 'a', optional: false &#125;,// &#123; name: 'b', optional: true &#125;,// &#123; name: 'c', optional: false &#125;,// &#123; name: 'c.d', optional: false &#125;,// &#123; name: 'c.e', optional: true &#125;,// &#123; name: 'f', optional: false &#125;,// &#123; name: 'f.x', optional: false &#125;,// &#123; name: 'f.y', optional: false &#125; ] 在build TypeScript项目时，一般用的是tsc命令，现在由于使用了ttypescript，需要改用ttsc，这里有一个ts-interface-keys-transformer-demo展示了用法。 参考资料 TypeScript Architectural Overview TypeScript Compiler-Internals ts-transformer-keys ts-ast-viewer","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"typescript","slug":"typescript","permalink":"https://nullcc.github.io/tags/typescript/"}]},{"title":"node.js中利用IPC和共享内存机制实现计算密集型任务转移","slug":"node.js中利用IPC和共享内存机制实现计算密集型任务转移","date":"2019-03-22T16:00:00.000Z","updated":"2022-04-15T03:41:13.025Z","comments":true,"path":"2019/03/23/node.js中利用IPC和共享内存机制实现计算密集型任务转移/","link":"","permalink":"https://nullcc.github.io/2019/03/23/node.js中利用IPC和共享内存机制实现计算密集型任务转移/","excerpt":"node.js是单进程单线程运行的，如果遇到一些计算密集型的操作应该怎么办呢？本文提供了一种思路。","text":"node.js是单进程单线程运行的，如果遇到一些计算密集型的操作应该怎么办呢？本文提供了一种思路。 需求最近在帮Web自动化测试开发小组编写一个基于Allure的日志插件，这里先简要介绍一下需求的上下文和这个插件的职责。 Allure本身是一个本地的Log Reporting工具，用户可以在将test case的日志使用Allure提供的API写入本地文件，之后可以直接在本地启动Allure Web Server查看测试的运行情况，这种日志收集方式针对本地调试非常方便。 这个日志插件是基于一个现有的自研Web测试框架设计和开发的，每次跑一遍测试都称为一次Run，每个Run下有若干个test cases，每个test case下又有若干steps，且step是可以有sub steps的（就是嵌套step）。因此整个运行时的数据结构是一个树形结构，该结构如下图所示： 在Run级别，框架提供on start run和on end run两个回调函数，在test case级别，框架也提供on start test和on end test两个回调函数，在这些回调函数内部用户可以注册自己的操作。针对steps则是需要用户提供一个针对on log handler的回调函数，每次有log输出时，框架都会调用这个函数。另外测试的执行端由selenium grid控制，具体测试运行在各个slave机器上，test case运行的并发数根据现有的资源数量可以达到几十至上百，考虑到资源有限，CI Daily Run一般设置并发数在60左右。 一个test case的工作流程如下图： 该日志插件的需求（只列出和本文关系密切的需求）： 需要在每次Run的时候将test cases和steps整理出来。 对于那些抛出异常的cases，需要判断其抛出的异常信息是否是known failure，如果是，需要在test的元数据中标明known failure issue name，并将test状态设置为Broken，否则设置为Failed。known failure是一个很长的正则表达式列表（本例中的场景如果转换成字符串大约有300+KB），这个列表将在运行test cases之前通过一个HTTP API从远端获得，程序需要遍历它来匹配异常信息判断是否是known failure。本例中由于使用了Allure这种本地日志收集工具，不可避免的需要在本地对失败case进行known failure的匹配。 整理一下上面列出的信息： 所有log都是以异步事件的形式发送给用户提供的”onLogHandler”的。 测试运行的并发数较大（几十至上百）。 在本地检测失败case的known failure需要遍历一个很长的正则表达式列表，这属于计算密集型操作。 最初实践最开始的解决方案相当简单粗暴，写一个方法，接受两个参数，一个是异常信息字符串，一个是known failure的正则数组。当某个test case抛出异常时，获取到它的异常信息字符串，直接调用这个方法去匹配。开发环境下因为跑的case不多，这么做完全没问题。到了测试环境压测时，发现仅仅30个并发下，很快就会Out Of Memory (下文简称OOM)。开始以为是对node进程分配的内存太小了，于是调高了分配的内存，但这也仅仅只能延缓OOM出现的时间而已。 问题分析之后详细分析了日志，发现OOM一般出现在大量case抛出异常之后，可以想到可能是由于正则匹配是计算密集型操作，node长时间执行CPU密集型操作时，是无法去执行其各个异步回调队列中的回调函数的。前文提到当有log产生时，测试框架都会调用我们设定的onLogHanlder去处理。在并发数比较高且test case中输出log较多的时候，如果此时node进程执行大量计算操作，时间一长node的异步回调事件队列中的回调函数得不到处理，异步事件队列长度疯狂增长，这相当于把对异步回调事件的处理“饿死了”，时间一长，由于异步事件堆积内存就不够用了。这里的知识点涉及node的异步回调处理模型。 解决方案既然node主进程需要处理大量异步事件，那一个可行的办法就是将这些计算密集型操作从主进程中分离出去。可以考虑使用IPC的方式，利用其它进程来处理这部分计算工作。我们可以使用node的child_process模块fork出一个子进程出来执行这些消耗CPU的操作。由于这些子进程只负责处理计算，并不负责处理异步事件，所以不用担心之前在主进程中发生异步事件“被饿死”的问题。 上文中还有一个情况还未说明，上文提到的known failure rules是需要从某个外部HTTP API中获取，最开始的做法是在初始化测试框架的时候获取一次，作为参数传递给end run hook，在end run hook中调用检测函数进行匹配。很容易想到用child_process生成一个子进程，并将这个规则列表传递给子进程的方式。首先我们不可能在每个子进程中单独去获取，因为这效率太低了，那就只能从主进程向子进程传递这个列表了。但是对命令行来说，传递这么大的参数有些不太合适，而且就算能用命令行参数传递，每次都要为300KB+的数据进行一次内存申请和复制，效率也不高。 于是想到可以采用共享内存的方式，在主进程中开辟一块专用内存区域共享给子进程，这样每个子进程在获取known failure rules的时候实际上只需要读一块已经就绪的内存。主进程利用IPC的方式将这块内存的key传递给子进程，子进程接收到主进程发送过来的内存key时，将这块内存的值读出并解析，接着直接进行匹配就好了。 共享内存方案的示意图如下： 下面用主进程和子进程的两段代码进行说明： 主进程：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import * as shm from 'shm-typed-array';import &#123; fork, ChildProcess, ForkOptions &#125; from 'child_process';const KNOWN_FAILURE_RULES_API = '...';const fetchKnownFailureRules = (endpoint: string): any[] =&gt; &#123; // 从HTTP API获取known failure rule lists，代码省略&#125;const promiseFork = (memoryKey, path: string, args: ReadonlyArray&lt;string&gt;, options?: ForkOptions): Promise&lt;string | null&gt; =&gt; &#123; return new Promise&lt;string | null&gt;((resolve, reject) =&gt; &#123; const child = fork(path, args, options); child.on('message', res =&gt; &#123; child.kill(); resolve(res); &#125;); child.on('error', err =&gt; &#123; child.kill(); reject(err); &#125;); child.stderr.on('data', data =&gt; &#123; child.kill(); reject(data.toString()); &#125;); child.on('exit', (code, signal) =&gt; &#123; child.kill(); reject(); &#125;); child.send(memoryKey); &#125;);&#125;;(async () =&gt; &#123; const knownFailureRules = await fetchKnownFailureRules(KNOWN_FAILURE_RULES_API); // 将known failure rule lists转换成Uint16Array const arr = Uint16Array.from(Buffer.from(JSON.stringify(knownFailureRules))); // 创建shared memory const data = shm.create(arr.length, 'Buffer'); if (!data) &#123; return; &#125; // 拷贝known failure rule lists的Uint16Array至shared memory for (let i = 0; i &lt; data.length; i++) &#123; data[i] = arr[i]; &#125; try &#123; const issueName = await promiseFork( data.key, 'match-known-failure.js', // match-known-failure.js是用来匹配known failure的脚本文件 ['test-name', 'error-message'] // 这里作为一个演示，test name和error message都是模拟数据 &#123; silent: true &#125; ); console.log(issueName); &#125; catch (err) &#123; console.log(err); &#125;&#125;)(); 子进程：12345678910111213141516171819// match-known-failure.jsconst shm = require('shm-typed-array');const matchKnownFailure = (testName, errorMessage, rules) =&gt; &#123; // 使用正则表达式匹配known failure rule lists，代码省略&#125;const testName = process.argv[2];const errorMessage = process.argv[3];process.on('message', async key =&gt; &#123; // 获取shared memory的数据 const data = shm.get(key, 'Buffer'); if (data) &#123; const rules = JSON.parse(data.toString()); const res = matchKnownFailure(testName, errorMessage, rules); process.send(res); &#125;&#125;); 另外共享内存区域的大小也是有限制的，我们需要在程序结束时手动释放这部分内存，其中sharedMemoryKey是向操作系统申请共享内存时得到的一个唯一key值，代码如下： 12345678910111213141516async clearSharedMemory(sharedMemoryKey) &#123; return new Promise((resolve, reject) =&gt; &#123; console.log('clear shared memory...'); exec(`ipcrm -M $&#123;sharedMemoryKey&#125;`, (error, stdout, stderr) =&gt; &#123; if (error) &#123; reject(error); &#125; resolve(); &#125;); &#125;);&#125;// 在进程结束时清理shared memoryprocess.on('exit', async () =&gt; &#123; await knownFailureFinder.clearSharedMemory(sharedMemoryKey);&#125;); 为了保持简单这里只列出了当’exit’事件发生的处理，其实在异常发生或者程序收到一些系统信号时也应该做这个清除处理。另外这个方案目前只在Linux和Mac OS X下测试通过，时间关系并未在Windows下做适配。 共享内存方案的一些潜在问题共享内存的优点是进行进程间通信非常方便，多个进程可以共享同一块内存，省去了数据拷贝的开销，效率很高。但是在使用共享内存的时候还需要注意，共享内存本身并没有提供同步机制，一切同步操作都需要开发者自己完成。在本文的例子中，由于known failure rules对于所有子进程都是只读的，不存在修改共享内存区域数据的问题，因此也不需要任何同步机制。但在一些需要修改共享内存区域的情况下，还需要开发者手动控制同步。 其他解决方案针对node的计算密集型任务的处理方法，还有很多其他解决方案，以下列举几个： 编写node的C++扩展来承担这部分计算工作。 子进程部分可以改用child_process的exec或者spawn调用一些性能更好的语言写的外部程序，比如C/C++和Rust。 将子进程替换为RPC调用外部服务，但是这种方式比较适合那些传参消耗小的计算任务。 写在最后之前有人问我，我不需要在本地实时分析test case的known failure，我有一个外部服务提供了专门的API可以异步地做这件事，那这种方案不就没用了吗？这个问题很好，如果已经有了外部服务做这件事，确实可以反过来只将test name和error message发送给外部服务，由外部服务进行匹配。本文旨在分享在node.js中遇到计算密集型操作时如何保证主进程不因CPU被长时间占用而阻塞异步事件队列的一种可能方案，文中的例子可能不具有代表性，不过作为一个例子它已经够用了。每个解决方案都有其自身的限制性和适用场景，将分析test case的known failure交给外部服务其实也是一种计算任务转移（当然前提是你已经有了这个外部服务），实际应用中适用哪种方案需要根据具体情况定夺。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"node","slug":"node","permalink":"https://nullcc.github.io/tags/node/"}]},{"title":"OpenResty入坑笔记(1)——搭建基本的OpenResty开发环境","slug":"OpenResty入坑笔记(1)——搭建基本的OpenResty开发环境","date":"2019-01-17T16:00:00.000Z","updated":"2022-04-15T03:41:13.016Z","comments":true,"path":"2019/01/18/OpenResty入坑笔记(1)——搭建基本的OpenResty开发环境/","link":"","permalink":"https://nullcc.github.io/2019/01/18/OpenResty入坑笔记(1)——搭建基本的OpenResty开发环境/","excerpt":"本文简单记录了一下搭建基本的OpenResty开发环境的过程，并展示了简单的统计endpoint访问次数的功能。","text":"本文简单记录了一下搭建基本的OpenResty开发环境的过程，并展示了简单的统计endpoint访问次数的功能。 安装由于OpenResty基于Nginx，所以首先需要安装Nginx，这个步骤这里就不写了，网上可以找到很详细的安装过程。OpenResty的安装，可以参考官方网站的安装文档，也非常容易。 项目结构需要说明的是，由于本文是一个搭建教程，所以只会列出最基本的一些目录和文件，随着后续教程的深入，将添加更多的目录和文件。先来快速浏览一下这个demo中项目的文件结构： 12345678910111213views-count|-- bin| |-- start.sh| |-- stop.sh|-- config| |-- mine.types| |-- nginx.conf|-- logs| |-- access.log| |-- error.log|-- lua| |-- controller.lua| |-- init.lua 重要目录和文件详解bin目录下放置的是Nginx的启动和停止的shell脚本。 start.sh内容如下： 1234567891011121314# start.shnginx_started=$(ps -ef | grep nginx | grep -v 'grep')if [ -z \"$nginx_started\" ]; then # nginx is not started, start nginx echo 'Start Nginx...' nginx -t -p `pwd` -c config/nginx.conf nginx -p `pwd` -c config/nginx.confelse # nginx is started, reload nginx echo 'Reload Nginx...' nginx -t -p `pwd` -c config/nginx.conf nginx -s reload -p `pwd` -c config/nginx.conffi stop.sh内容如下： 12# stop.shnginx -s quit config目录下的mime.types文件是Nginx会用到MIME Type，这个文件的内容基本是固定的： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138# mime.typestypes &#123; # Data interchange application/atom+xml atom; application/json json map topojson; application/ld+json jsonld; application/rss+xml rss; # Normalize to standard type. # https://tools.ietf.org/html/rfc7946#section-12 application/geo+json geojson; application/xml xml; # Normalize to standard type. # https://tools.ietf.org/html/rfc3870#section-2 application/rdf+xml rdf; # JavaScript # Servers should use text/javascript for JavaScript resources. # https://html.spec.whatwg.org/multipage/scripting.html#scriptingLanguages text/javascript js mjs; # Manifest files application/manifest+json webmanifest; application/x-web-app-manifest+json webapp; text/cache-manifest appcache; # Media files audio/midi mid midi kar; audio/mp4 aac f4a f4b m4a; audio/mpeg mp3; audio/ogg oga ogg opus; audio/x-realaudio ra; audio/x-wav wav; audio/x-matroska mka; image/bmp bmp; image/gif gif; image/jpeg jpeg jpg; image/jxr jxr hdp wdp; image/png png; image/svg+xml svg svgz; image/tiff tif tiff; image/vnd.wap.wbmp wbmp; image/webp webp; image/x-jng jng; video/3gpp 3gp 3gpp; video/mp4 f4p f4v m4v mp4; video/mpeg mpeg mpg; video/ogg ogv; video/quicktime mov; video/webm webm; video/x-flv flv; video/x-mng mng; video/x-ms-asf asf asx; video/x-ms-wmv wmv; video/x-msvideo avi; video/x-matroska mkv mk3d; # Serving `.ico` image files with a different media type # prevents Internet Explorer from displaying then as images: # https://github.com/h5bp/html5-boilerplate/commit/37b5fec090d00f38de64b591bcddcb205aadf8ee image/x-icon cur ico; # Microsoft Office application/msword doc; application/vnd.ms-excel xls; application/vnd.ms-powerpoint ppt; application/vnd.openxmlformats-officedocument.wordprocessingml.document docx; application/vnd.openxmlformats-officedocument.spreadsheetml.sheet xlsx; application/vnd.openxmlformats-officedocument.presentationml.presentation pptx; # Web fonts font/woff woff; font/woff2 woff2; application/vnd.ms-fontobject eot; font/ttf ttf; font/collection ttc; font/otf otf; # Other application/java-archive ear jar war; application/mac-binhex40 hqx; application/octet-stream bin deb dll dmg exe img iso msi msm msp safariextz; application/pdf pdf; application/postscript ai eps ps; application/rtf rtf; application/vnd.google-earth.kml+xml kml; application/vnd.google-earth.kmz kmz; application/vnd.wap.wmlc wmlc; application/x-7z-compressed 7z; application/x-bb-appworld bbaw; application/x-bittorrent torrent; application/x-chrome-extension crx; application/x-cocoa cco; application/x-java-archive-diff jardiff; application/x-java-jnlp-file jnlp; application/x-makeself run; application/x-opera-extension oex; application/x-perl pl pm; application/x-pilot pdb prc; application/x-rar-compressed rar; application/x-redhat-package-manager rpm; application/x-sea sea; application/x-shockwave-flash swf; application/x-stuffit sit; application/x-tcl tcl tk; application/x-x509-ca-cert crt der pem; application/x-xpinstall xpi; application/xhtml+xml xhtml; application/xslt+xml xsl; application/zip zip; text/css css; text/csv csv; text/html htm html shtml; text/markdown md; text/mathml mml; text/plain txt; text/vcard vcard vcf; text/vnd.rim.location.xloc xloc; text/vnd.sun.j2me.app-descriptor jad; text/vnd.wap.wml wml; text/vtt vtt; text/x-component htc;&#125; nginx.conf是本文讨论的重点： 123456789101112131415161718192021222324252627282930313233343536373839worker_processes 1; # 工作进程个数，一般设置成和CPU核数相同。Nginx有两种进程，一种是主进程master process，另一种是工作进程events &#123; worker_connections 1024; # 单个工作进程允许同时建立的最大外部连接数量，一个工作进程在建立一个连接后会同时打开一个文件描述符，该参数受限于进程最大可打开文件数&#125;http &#123; include mime.types; default_type application/octet-stream; gzip on; sendfile on; keepalive_timeout 65; # log format log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log logs/access.log main; error_log logs/error.log error; lua_shared_dict statistics 1m; # 声明了一个1M大小的共享字典，改数据结构在所有工作进程之间共享 # lua packages lua_package_path &quot;lualib/?.lua;;&quot;; # Lua扩展库搜索路径，&apos;;;&apos;是默认路径 lua_package_cpath &quot;lualib/?.so;;&quot;; # Lua C扩展库搜索路径，&apos;;;&apos;是默认路径 init_by_lua_file &quot;lua/init.lua&quot;; # 当Nginx master进程（如果有）加载Nginx配置文件时，在全局Lua虚拟机上运行该指令指定的lua文件 server &#123; listen 8088; server_name localhost; location /lua &#123; default_type text/html; lua_code_cache off; # 控制是否缓存lua代码，生产环境中强烈建议打开，否则性能会下降，开发环境为了调试方便可以关闭 content_by_lua_file &quot;lua/controller.lua&quot;; # 作为内容处理程序，为每个请求执行该文件中的lua代码 &#125; &#125;&#125; lua目录下存放的都是lua脚本文件，init.lua文件用来存放一些在master进程启动并加载配置文件时执行的初始化操作，一般用来加载需要用到的第三方库，也可以用来初始化一些共享变量，下面的代码初始化了statistics这个共享变量中的views这个key的值为0，之后我们将用这个key来统计用户访问某个endpoint的次数： init.lua: 123# init.lualocal statistics = ngx.shared.statistics;statistics:set(\"views\", 0); controller.lua: 12345# controller.lualocal statistics = ngx.shared.statistics;statistics:incr(\"views\", 1);local views = statistics:get('views');ngx.say('hello world ' .. views); controller.lua中，每个请求都会递增views的值，并在结果中输出，需要特别说明的一点是，为了调试这个功能，我们必须把lua_code_cache设置为on。这是因为如果lua_code_cache为off，init_by_lua将在每个请求上执行，因为这种情况下，每个请求中都会去创建一个全新的Lua虚拟机而非共享同一个Lua虚拟机。在将lua_code_cache设置为on后，运行sh bin/start.sh，第一次访问http://localhost:8088/lua将输出： 1hello world 1 第二次访问将输出： 1hello world 2 每次访问都会将statistics共享变量中views的值递增1。这就实现了一个基本的统计endpoint访问次数的功能。如果后台需要统计所有的访问次数，我们只需要定期回写该值，并重置statistics共享变量中views的值为0即可。 总结在阅读了本文后，读者应该对OpenResty的项目搭建有了一个大致的了解。我们只介绍了很少的一部分概念，写了几行Lua代码就实现了一个简单的访问计数器。在之后的文章中，我们将深入OpenResty的世界，了解OpenResty的运行机制，还有很长一段路要走。 值得一提的是，到现在为止我们的工作还没有涉及任何应用服务器。这就是OpenResty的作用所在：将和业务无关的事情剥离出整个应用服务层，应用服务层可以专注于做业务相关的事情。在之后还可以看到，我们可以使用Lua实现更多这类“公共功能”，比如读缓存、LRU Cache、写日志、应用防火墙、模板渲染、静态文件合并、负载均衡、流量控制等。","categories":[{"name":"web后端","slug":"web后端","permalink":"https://nullcc.github.io/categories/web后端/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://nullcc.github.io/tags/nginx/"},{"name":"openresty","slug":"openresty","permalink":"https://nullcc.github.io/tags/openresty/"}]},{"title":"AOP在JavaScript和TypeScript中的应用","slug":"AOP在JavaScript和TypeScript中的应用","date":"2019-01-10T16:00:00.000Z","updated":"2022-04-15T03:41:13.013Z","comments":true,"path":"2019/01/11/AOP在JavaScript和TypeScript中的应用/","link":"","permalink":"https://nullcc.github.io/2019/01/11/AOP在JavaScript和TypeScript中的应用/","excerpt":"本文将简单聊聊AOP在JavaScript和TypeScript中的应用。本文是之前一篇文章&lt;AOP_in_JavaScript_and_TypeScript&gt;的中文版。","text":"本文将简单聊聊AOP在JavaScript和TypeScript中的应用。本文是之前一篇文章&lt;AOP_in_JavaScript_and_TypeScript&gt;的中文版。 AOP概览Aspect Oriented Programming (AOP)，中文意思是“面向切面编程”。AOP的作用用一句话概括就是将业务逻辑和非业务逻辑的代码分开，减少它们的耦合性。 这么说比较抽象，我们具体点说，在使用selenium-webdriver做一些web自动化测试时，我们经常需要执行一些辅助的操作，比如记录日志、截屏保存等。这些操作本身和测试的业务逻辑没有强关联性，毕竟没有记录日志的操作我们的自动化测试代码也能运行，但是我们大部分时候也确实需要这些辅助操作。我们希望在业务建模阶段不需要考虑这些辅助函数的事情。 还有一个例子比如我们要在自动化测试的每个步骤后截屏保存，并记录每个步骤的耗时。最简单的做法就是把截屏代码和计算耗时的代码嵌入到每个步骤中。但这么做的问题也显而易见，\bstep多了以后代码难以维护。AOP则可以优雅地解决这类问题。 AOP和OOP的对比大部分人对面向对象编程(OOP)比较熟悉。当我们获得一个需求时，首先要分析需求，然后抽取出一些领域模型。每个领域模型都有它的属性和方法。人们使用封装、组合、继承、多态和设计模式来以OOP的方式构建软件。 如果你有过用OOP的方式构建软件的经历就会发现OOP是对静态事物建模的。换句话说，OOP是比较擅长的领域是对名词建模。比如，我们有一个Employee类，它有如下属性：age、title和department，还有一些方法：work、takeABreak和loginAdminSystem。属性用来描述对象的特征，方法则决定了对象能够执行什么样的操作。我们可以写出下面这样的面向对象代码： 1234567891011121314151617181920212223242526272829class Employee &#123; private name: string; private age: number; private title: string; private department: string; constructor(name: string, age: number, title: string, department: string) &#123; this.name = name; this.age = age; this.title = title; this.department = department; &#125; public work() &#123; // code for working... &#125; public takeABreak() &#123; // code for taking a break... &#125; public loginAdminSystem() &#123; // code for logining admin system, it's a sensitive operation &#125;&#125;const employee = new Employee('Bob', 35, 'Software Development Engineer', 'Devlopment');employee.work();employee.takeABreak(); 上面的代码都是和Employee类强关联的业务逻辑，毫无疑问，OOP非常适合做这类描述对象和其行为的事情。 但有时我们希望能加入更多“动态”东西，比如我们希望在用户执行一些敏感操作的时候记录日志。如果使用OOP来实现，就必须修改相关敏感操作的代码，加入记录日志的代码： 123456...public loginAdminSystem() &#123; // added: code for logging some information // code for logining admin system&#125;... 这段代码可以工作，但并不优雅。实际上这种做法违反了OCP(开闭原则)。记录日志的操作和这个敏感操作并无强关联性，它只是辅助性的代码。因此最好不要为了加入一个记录日志的辅助功能而去修改业务逻辑代码。 如何处理这种情况？可以尝试下AOP。简单来说，可以在特定操作前后暴露两个切面：一个在特定操作前，另一个在特定操作后，然后再运行时动态地将其他辅助性函数织入进去。因此AOP实际上是针对动词的。通过将OOP和AOP相结合，我们的代码将变得更加优雅，且有良好的扩展性。 下面是一个简单的例子：函数包装。假设我们有一个函数”op”，我们将一些日志操作加入其前后： 1234567891011let op = () =&gt; &#123; console.log('executing op...');&#125;;let oriOp = op;op = () =&gt; &#123; console.log('before op...'); oriOp(); console.log('after op...');&#125; 这次我们不是修改原函数而是包装它。 上面的例子只是一种非常简单的情形，实际项目中的AOP代码要比上面的示例复杂得多。一般来说我们需要一些“元编程”技术来实现AOP。但基本原则和本质和上面的代码是相似的。值得一提的是，AOP是一种编程理念，并不局限于某种编程语言，大部分编程语言都可以以AOP的方式来编程。 下面将针对之前提到的，在Web自动化测试中加入如记录日志、截图保存和计算步骤耗时等辅助性功能，给出几个具体的实现来详细说明如何在JavaScript和TypeScript中实现AOP。 解决方案1 —— 简单的方法钩子看过上面的介绍后，最直接的想法就是，可以将那些业务方法用前置/后置处理器一一包装起来，也就是加入方法钩子。解决方案1使用方法钩子（前置/后置动作）来将原方法包装成一个新方法，我们把辅助性功能放在钩子中。 代码在base driver和method hook driver. 这种方案有一个明显的缺点：如果前置方法和后置方法之间有关联，将难以处理。比如如果要记录一个步骤的耗时，前置方法和后置方法是这样的： 123456789101112// before actionconst recordStartTime = async () =&gt; &#123; const start = new Date().getTime(); return start;&#125;;// after actionconst recordEndTime = async start =&gt; &#123; const end = new Date().getTime(); const consume = end - start; console.log(`time consume: $&#123;consume&#125;ms`);&#125;; 且其中需要用到一个”registerHooksForMethods”方法： 123456789101112131415161718public registerHooksForMethods( methods: string[], beforeAction: Function, afterAction: Function ) &#123; const self = this; methods.forEach(method =&gt; &#123; const originalMethod = self[method]; // original method reference if (originalMethod) &#123; self[method] = async (...args) =&gt; &#123; // wrap original method const beforeActionRes = await beforeAction(); const methodRes = await originalMethod.call(self, ...args); await afterAction(beforeActionRes, methodRes); return methodRes; &#125;; &#125; &#125;); &#125; registerHooksForMethods方法接受三个参数，用来将一组原方法分别使用前置/后置处理器包装一组对应的新方法。这种实现其实是比较不优雅的，而且很难扩展。因此我们需要继续寻找更好的方案。 解决方案2 ——— 静态洋葱模型静态洋葱模型受到Koa的启发，这个模型很有意思，对一个方法的执行流程就像一个箭头通过一整颗洋葱： 代码在base driver and static onion driver. 洋葱内部每一层都被上面一层所完全包裹，我们将业务方法置于洋葱的最内部，到达业务方法和离开业务方法都将穿越其外层，而且除了业务方法之外，每层都会被穿越两次。每一层都是一个”中间件”。 静态洋葱模型比刚才的钩子方法要好不少，这里使用装饰器方法来实现它： 123456789101112131415161718192021// decoratorexport const webDriverMethod = () =&gt; &#123; return (target, methodName: string, descriptor: PropertyDescriptor) =&gt; &#123; const desc = &#123; value: \"webDriverMethod\", writable: false &#125;; Object.defineProperty(target[methodName], \"__type__\", desc); &#125;;&#125;;// in BaseWebDriver class, a web driver method@webDriverMethod()public async findElement( by: By, ec: Function = until.elementLocated, timeout: number = 3000) &#123; await this.webDriver.wait(ec(by), timeout); return this.webDriver.findElement(by);&#125; 调用use方法来增加一个中间件： 123456789101112131415161718192021222324252627282930public use(middleware) &#123; const webDriverMethods = this.getWebDriverMethods(); const self = this; for (const method of webDriverMethods) &#123; const originalMethod = this[method]; if (originalMethod) &#123; this[method] = async (...args) =&gt; &#123; let result; const ctx = &#123; methodName: method, args &#125;; await middleware(ctx, async () =&gt; &#123; result = await originalMethod.call(self, ...args); &#125;); return result; &#125;; // check this: we must decorate new method every time when adding a middleware this.decorate(this[method]); &#125; &#125;&#125;private decorate(method) &#123; const desc = &#123; value: \"webDriverMethod\", writable: false &#125;; Object.defineProperty(method, \"__type__\", desc);&#125; 静态洋葱模型有个小缺点：每增加一个中间件，都必须手动在相关函数上面增加一个装饰器。为了偷懒，我们还可以实现得更动态一些，这就有个方案3。 解决方案3 —— 动态洋葱模型代码在base driver and dynamic onion driver. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768export class DynamicOnionWebDriver extends BaseWebDriver &#123; protected webDriver: WebDriver; private middlewares = []; constructor(webDriver) &#123; super(webDriver); const methods = this.getWebDriverMethods(); const self = this; for (const method of methods) &#123; const desc = &#123; enumerable: true, configurable: true, get() &#123; if (methods.includes(method) &amp;&amp; this.compose) &#123; const ctx = &#123; // put some information in ctx if necessary methodName: method, &#125; const originFn = async (...args) =&gt; &#123; return this.methodMap[method].call(self, ...args); &#125;; const fn = this.compose(); return fn.bind(null, ctx, originFn.bind(self)); &#125; return this.methodMap[method].bind(this); &#125;, set(value) &#123; this[method] = value; &#125; &#125;; Object.defineProperty(this, method, desc); &#125; &#125; public use(middleware) &#123; if (typeof middleware !== \"function\") &#123; throw new TypeError(\"Middleware must be a function!\"); &#125; this.middlewares.push(middleware); &#125; private compose() &#123; const middlewares = this.middlewares; const self = this; return async (ctx, next, ...args) =&gt; &#123; let res; const dispatch = async i =&gt; &#123; let fn = middlewares[i]; if (i === middlewares.length) &#123; fn = next; &#125; if (!fn) &#123; return Promise.resolve(); &#125; try &#123; if (i === middlewares.length) &#123; res = await Promise.resolve(fn.call(self, ...args)); return res; &#125; return Promise.resolve(fn(ctx, dispatch.bind(null, i + 1))); &#125; catch (err) &#123; return Promise.reject(err); &#125; &#125;; await dispatch(0); return res; &#125;; &#125;&#125; 动态洋葱模型要比之前两个方案复杂很多，我们使用Object.defineProperty来定义自己的getter，这些getter将对每个使用了webDriverMethod装饰器的方法生效。compose方法非常重要，它用来将所有中间件和原函数组合到一起，compose是在koa-compose的核心代码基础上修改得来的。getter将调用compose函数来将原函数和所有中间件包装成一个新函数返回。有了这种动态包装机制，就不需要每次增加中间的时候都要手动在原函数上添加装饰器了。 动态洋葱模型的代码比较难以理解，但绝对值得我们好好去学习。 顺便一提，本文中除了方法钩子这个名称外，静态洋葱模型和动态洋葱模型都是我自己发明的，如果读者有更好的名字，可以和我交流。 示例Repots-aop-example 运行测试1npm test 更多信息 什么是面向切面编程AOP Koa Web Framework Object.defineProperty()","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"js","slug":"js","permalink":"https://nullcc.github.io/tags/js/"},{"name":"aop","slug":"aop","permalink":"https://nullcc.github.io/tags/aop/"}]},{"title":"RxJS入坑笔记(一)——基础知识","slug":"RxJS入坑笔记(一)——基础知识","date":"2018-12-31T16:00:00.000Z","updated":"2022-04-15T03:41:13.020Z","comments":true,"path":"2019/01/01/RxJS入坑笔记(一)——基础知识/","link":"","permalink":"https://nullcc.github.io/2019/01/01/RxJS入坑笔记(一)——基础知识/","excerpt":"这是一个系列文章，主要记录我自己学习RxJS时的学习笔记和遇到的问题。本文关注RxJS实践环境的搭建和一些基础知识。","text":"这是一个系列文章，主要记录我自己学习RxJS时的学习笔记和遇到的问题。本文关注RxJS实践环境的搭建和一些基础知识。 搭建开发环境学习一门编程技术最好的方式是一边看教程一边实践，RxJS也不例外。让我们快速搭建一个开发环境来写一些代码： 1create-react-app rxjs-samples --scripts-version=react-scripts-ts 我们使用create-react-app这个脚手架来搭建一个React编程环境，并使用TypeScript来编写代码。 接着安装RxJS的一些依赖： 1yarn add rxjs rxjs-compat --save 1yarn add tslint --dev 现在可以直接运行下列命令在开发环境中运行我们的应用： 1yarn start 看到一个旋转的React的Logo说明一切正常。 快速开始——使用GitHub API获取用户信息这里并不打算从一个传统的”Hello world”例子开始，而是直接展示RxJS相较于传统事件响应编程的不同之处。这个示例很简单，提供一个输入框，用户在输入框中输入内容后，将使用输入内容在GitHub API中搜索用户信息，并将用户信息展示出来。这里面还有两个要求： 控制1秒内只响应一次输入框的变化 只有输入框的内容有变化时才调用API获取用户信息 如果使用传统的事件响应式编程，代码大概是下面这样的： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546const usernameInput = document.querySelector('#username');let lastInputValue = '';let lastInputTime = null;let timer = null;usernameInput.addEventListener('input', event =&gt; &#123; if (!lastInputTime) &#123; lastInputTime = new Date().getTime(); &#125; const now = new Date().getTime(); const interval = now - lastInputTime; const inputValue = event['target']['value']; lastInputTime = now; if (interval &lt; 1000) &#123; clearTimeout(timer); timer = setTimeout(() =&gt; &#123; if (lastInputValue !== inputValue) &#123; lastInputValue = inputValue; getUser(inputValue); &#125; lastInputTime = null; &#125;, 1000); return; &#125; else &#123; if (inputValue !== lastInputValue) &#123; lastInputValue = inputValue; clearTimeout(timer); getUser(inputValue); return; &#125; &#125;&#125;);const getUser = username =&gt; &#123; $.ajax(&#123; type: 'GET', url: `https://api.github.com/users/$&#123;username&#125;`, success: data =&gt; &#123; const pre = document.createElement('pre'); pre.innerHTML = JSON.stringify(data); document.getElementById('results').appendChild(pre); &#125; &#125;);&#125;; 看了这段代码有何感想？我说几点我的感想，首先控制流对业务代码的侵入性高，不易扩展。且代码冗长不简洁，还需要引入一些外部变量。虽然也实现了想要的功能，但总感觉不是做这件事的最佳方式。 再来看看用RxJS的实现： 123456789101112131415161718const usernameInput = document.getElementById('username') as FromEventTarget&lt;any&gt;;fromEvent(usernameInput, 'input') .map((event: any) =&gt; event.currentTarget.value) .debounceTime(1000) .distinctUntilChanged() .switchMap((username: string) =&gt; ajax(`https://api.github.com/users/$&#123;username&#125;`)) .map((data: any) =&gt; data.response) .subscribe( (val: any) =&gt; &#123; const pre = document.createElement('pre'); pre.innerHTML = JSON.stringify(val); const res = document.getElementById('results') as HTMLElement; res.appendChild(pre); &#125;, (err: Error) =&gt; &#123; alert(err.message) &#125; ); 这段代码不但实现了我们想要的功能，而且还非常优雅美观。没错，这就是我们想要的。 RxJS的基础概念核心数据类型下面是官方中文文档中对RxJS核心数据类型和Observable概念的简单说明： RxJS有一个核心类型Observable，以及围绕Observable的一些其他类型：Observer、 Subscription、Subject和Operators。 Observable（可观察对象）：可观察对象代表一个可观测的未来值或事件的集合。 Observer（观察者）：一个回调函数的集合，负责处理由Observable发出的值。 Subscription（订阅）：当一个Observable被订阅时才会真正得发出值。 Operators（操作符）：是一些纯函数，我们使用函数式编程的方法来处理集合。 单个值 多个值 拉取 Function Iterator 推送 Promise Observable 当调用一个函数时，实际上是主动地拉取一个值，而使用迭代器时我们可以主动地拉取多个值。在异步编程中，Promise一旦被创建出来就会立即执行，而后的then实际上是接受Promise决议后推送过来的值，且Promise至多只能推送一个值。Observable则可以同步或异步地推送多个值。 基本模式// todo 操作符创建操作符——Creation Operators可以使用创建操作符来创建Observable，列出如下： create empty from fromEvent interval of range throwError timer create操作符12345678910111213141516const observable = Observable.create(observer =&gt; &#123; observer.next(1); observer.next(2); setTimeout(() =&gt; &#123; observer.next(3); observer.complete(); observer.next(5); &#125;, 1000); observer.next(4);&#125;);const subscribe = observable.subscribe( val =&gt; &#123; console.log(val); &#125;); 运行上面的代码会立即打印1、2和4，并在约1000毫秒后打印3，然后结束，并不会打印5。 我们可以使用create操作符很容易地创建一个Observable对象，然后随意地发出值。另外可以使用observer.complete()结束整个事件流。下面的例子是一个自然数发生器（周期时钟），每隔1000毫秒发出下一个自然数： 123456789101112const observable = Observable.create(observer =&gt; &#123; let num = 0; setInterval(() =&gt; &#123; observer.next(++num); &#125;, 1000);&#125;);const subscribe = observable.subscribe( val =&gt; &#123; console.log(val); &#125;); empty操作符empty操作符会直接使Observable直接结束： 12345678910const observable = empty();const subscribe = observable.subscribe(&#123; next: val =&gt; &#123; console.log(val); &#125;, complete: () =&gt; &#123; console.log('complete'); &#125;&#125;) 直接打印出’complete’。 from操作符from操作符可以从一个可迭代对象(Array, Map, Promise)中创建一个Observable对象： 1234567const observable = Observable.from([1, 2, 3, 4, 5, 6]);const subscribe = observable.subscribe( val =&gt; &#123; console.log(val); &#125;) 这段代码会依次打印1, 2, 3, 4, 5, 6 使用Map： 1234567891011const map = new Map();map.set('foo', 1);map.set('bar', 2);const observable = Observable.from(map);const subscribe = observable.subscribe( val =&gt; &#123; console.log(val); &#125;) 这段代码会打印出： [“foo”, 1][“bar”, 2] 使用Promise： 1234567891011const promise = new Promise((resolve, reject) =&gt; &#123; resolve(100);&#125;);const observable = Observable.from(promise);const subscribe = observable.subscribe( val =&gt; &#123; console.log(val); &#125;) 这段代码直接打印出100。 fromEvent操作符fromEvent操作符在“快速开始”一节中已经展示了，fromEvent接受一个FromEventTarget对象和一个event name。 interval操作符interval操作符非常简单，接受一个以毫秒为单位的时间参数，每隔这个时间发出一个自增的数字： 1234567const observable = interval(1000);const subscribe = observable.subscribe( val =&gt; &#123; console.log(val); &#125;) of操作符of操作符接收不定个数的参数，并依次发射每个参数： 1234567const observable = Observable.of(1, 2, 3);const subscribe = observable.subscribe( val =&gt; &#123; console.log(val); &#125;) 打印出1, 2, 3。 还可以传入一些其他类型的参数： 1234567const observable = Observable.of(1, 2, 3, [4, 5], &#123; a:1, b:2 &#125;, function() &#123; console.log(10); &#125;);const subscribe = observable.subscribe( val =&gt; &#123; console.log(val); &#125;) 这将依次打印出： 123[4, 5]{a: 1, b: 2}ƒ () { console.log(10); } range操作符可以使用range操作符指定整数的起点和终点（闭区间），并依次发出这些数字： 1234567const observable = Observable.range(1, 10);const subscribe = observable.subscribe( val =&gt; &#123; console.log(val); &#125;) 将打印出1到10的整数。 throwError操作符throw操作符发出一个异常： 12345678910111213const observable = throwError('Got an error.');const subscribe = observable.subscribe(&#123; next: val =&gt; &#123; console.log(val); &#125;, complete: () =&gt; &#123; console.log('complete'); &#125;, error: err =&gt; &#123; console.error(err); &#125;&#125;) 这段代码将打印错误：”Got an error.”。 timer操作符timer操作符可以接受两个参数，第一个参数表示经过多长时间后发出一个值（从0开始自增），第二个参数表示之后每隔多长时间发出一个值： 1234567const observable = timer(1000);const subscribe = observable.subscribe( val =&gt; &#123; console.log(val); &#125;) 这段代码将在1000毫秒后发出1。 传入第二个参数的情况： 1234567const observable = timer(1000, 3000);const subscribe = observable.subscribe( val =&gt; &#123; console.log(val); &#125;) 这段代码将在1000毫秒后发出1，之后每隔3000毫秒发出自增的数字。 过滤操作符——Filtering OperatorsdebounceTime操作符debounceTime操作符会丢弃所有在指定时间间隔内发出的值。在刚才通过GitHub API获取用户信息的例子中已经演示了如何使用，这里再看下面这个计数器的例子，我们限制了用户点击”+”和”-“的速度每秒不能超过一次： 123456&lt;div&gt; &lt;h1&gt;Simple Counter&lt;/h1&gt; &lt;button id='decrBtn'&gt;-&lt;/button&gt; &lt;button id='incrBtn'&gt;+&lt;/button&gt; &lt;p id='counter'&gt;0&lt;/p&gt;&lt;/div&gt; 12345678910111213141516const inceStream = document.querySelector('#incrBtn');const decrStream = document.querySelector('#decrBtn');const s1 = fromEvent(inceStream, 'click').mapTo(1);const s2 = fromEvent(decrStream, 'click').mapTo(-1);merge(s1, s2) .debounce(() =&gt; timer(1000)) .startWith(0) .scan((acc, curr) =&gt; acc + curr) .subscribe( res =&gt; &#123; const counter = document.getElementById('counter'); counter.innerText = res.toString(); &#125; ); distinctUntilChanged操作符distinctUntilChanged控制了只有在当前值和上一次值不同时才发出值，它经常和debounceTime操作符一起用来控制对事件流进行控制。比如我们将一个文本框的输入内容作为查询参数，构造请求发往后端。distinctUntilChanged和debounceTime配合使用可以简单有效地控制该文本框的输入事件触发发送HTTP请求的频率。GitHub API的例子很好地展示了这种用法。 下面这个例子只会发出连续相同的值序列中的第一个值： 123456789const observable = Observable.from([1, 1, 2, 1, 2, 3, 3]);const subscribe = observable .distinctUntilChanged() .subscribe( val =&gt; &#123; console.log(val); &#125; ); 打印出值如下： 12123 filter操作符filter操作符很好理解，给定一个过滤条件，只发出满足条件的值。下面的例子只会发出偶数值： 123456789const observable = Observable.from([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]);const subscribe = observable .filter(num =&gt; num % 2 === 0) .subscribe( val =&gt; &#123; console.log(val); &#125; ); 打印： 02468 take操作符take操作符会发出序列的前n个值： 123456789const observable = Observable.from([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]);const subscribe = observable .take(3) .subscribe( val =&gt; &#123; console.log(val); &#125; ); 打印： 012 takeUntil操作符takeUntil可以时在某个事件发生时，让一个observable直接发送complete信息： 1&lt;button id='btn'&gt;Stop!&lt;/button&gt; 123456789101112131415161718const btn = document.querySelector('#btn');const btnObservable = fromEvent(btn, 'click');const intervalObservable = interval(1000);const subscribe = intervalObservable .takeUntil(btnObservable) .subscribe(&#123; next: val =&gt; &#123; console.log(val); &#125;, error: err =&gt; &#123; console.error(err); &#125;, complete: () =&gt; &#123; console.log('complete!'); &#125; &#125;); 运行上面的代码，一开始将以1000毫秒为间隔自增打印自然数，当按下”Stop!”按钮时，intervalObservable结束，并打印出”complete!”。 错误处理操作符——Error Handling Operators// todo","categories":[{"name":"web前端","slug":"web前端","permalink":"https://nullcc.github.io/categories/web前端/"}],"tags":[{"name":"RxJS","slug":"RxJS","permalink":"https://nullcc.github.io/tags/RxJS/"}]},{"title":"Web API设计实践","slug":"Web API设计实践","date":"2018-12-16T16:00:00.000Z","updated":"2022-04-15T03:41:13.021Z","comments":true,"path":"2018/12/17/Web API设计实践/","link":"","permalink":"https://nullcc.github.io/2018/12/17/Web API设计实践/","excerpt":"本文将讨论Web API设计的实践。","text":"本文将讨论Web API设计的实践。 Web API的基本概念Web世界在发展了一段时间后，出现了很多在线服务，这些在线服务不但本身提供一些功能，还公开了自己的Web API。第三方应用可以以Web API的方式接入这些在线服务开发自己的应用，从而也衍生出很多周边服务，甚至以这些已存在的在线服务为基础搭建自己的核心服务。 人们在Web系统中使用URI(Uniform Resource Identifier, 统一资源标识符)来定位资源，顾名思义\bURI是用来表示资源的，因此URI应该（或者说大部分情况下，特殊情况之后会提到）是一个名词。另外HTTP协议中定义了一些HTTP Method来表示如何操作这个URI，这是动词。HTTP Method + URI就构成了一个基本的操作：动词 + 名词。 HTTP Method 和 Endpoints 的设计我们也经常可以在一些Web API上看到动词，例如下面这组操作用户的Web API： 1234GET http://api.example.com/v1/get_usersPOST http://api.example.com/v1/create_userPUT http://api.example.com/v1/update_user?id=123DELETE http://api.example.com/v1/delete_user?id=123 拿上述的第一个Web API为例，这个URI的作用是获取用户列表，它使用了HTTP GET方法。虽然这么设计Web API也能工作，但这么设计有一些问题：首先HTTP GET和URI中的get重复了，另外一般在URI中有动词也不太符合而且也不符合URI表示资源这个原则，这个URI这样设计会好很多： 1GET http://api.example.com/v1/users 其他几个Web API也有类似的问题，要么是HTTP Method和URI中的内容有重复，要么是URI包含了动词，首先这不大符合URI的设计规范，而且这么做也没有一个统一的标准，试想一下删除用户的Web API也可以设计成： 1DELETE http://api.example.com/v1/remove_user?id=123 如果没有统一的设计标准，一旦之后Web API数量增加，就会造成各种奇形怪状的Web API层出不穷，非常不利于维护，当然也影响美观。 下面以user资源为例，列出了各种操作对应的HTTP Method 和 Endpoints的规范设计方式： 操作含义 HTTP Method Endpoint 获取用户列表 GET /users 获取用户信息 GET /users/:id 创建用户 POST /users 更新用户(完整更新) PUT /users/:id 更新用户(部分更新) PATCH /users/:id 删除用户 DELETE /users/:id 首先说说为什么这么设计，对于一个Web系统中的某种资源来说，绝大部分情况下不止一个，也就是说资源是一个集合的概念，就算只有唯一一个资源，也可以看做是集合只有一个元素的特殊情况。 比较容易让人混淆的是PUT和PATCH方法的含义，其中PUT是指“完整更新”，客户端需要发送资源的完整信息来更新这个资源，PATCH是指“部分更新”，客户端只需要发送需要更新的个别字段即可完成资源的更新。以user这个资源举例说明的话，假设user有name, age, icon三个属性，有一个id为123的user如下： 12345&#123; \"name\": \"Foo\", \"age\": 29, \"icon\": \"http://www.example.com/icon.png&#125; 这时我们希望更新该user的age字段为30，如果使用PUT，body需要包含所有这三个属性，其中不打算做更新的字段保持原来的值即可： 12345&#123; \"name\": \"Foo\", \"age\": 30, \"icon\": \"http://www.example.com/icon.png&#125; 如果用PATCH则不必包含所有属性，只需要列出age字段即可： 123&#123; \"age\": 30&#125; 另外，在设计Web API的 HTTP Method 和 Endpoint 有以下几个需要注意的地方： 一般情况下（search之类的特殊URI例外），不应该在URI中出现动词，URI表示资源，应该是名词。 资源名称应该是复数形式。 注意根据Web API的功能选择适当的HTTP Method：GET操作不应该对服务器端资源造成任何修改，应该是幂等的。POST用来创建资源，PUT用来完整更新资源，PATCH用来局部更新资源，DELETE用来删除资源。 Endpoints中不要使用空格和需要编码的字符。 使用连字符来连接多个单词，常用的连字符有”-“和”_”，不建议使用驼峰法，因为URI本身并不区分字母大小写。 另外比较常见的 Web API Endpoint 经常是这样的： 12https://api.example.com/v1https://www.example.com/api/v1 注意如果主机名已经有”api”了，一般path中就不需要再出现”api”，否则path中会出现”api”以示这是一个Web API Endpoint。选择哪种方式其实也没有一个唯一答案。一般来说能选则第一种尽量选第一种。 URI中使用动词的特殊情况有时候一个行为可能无法很好地映射到一个资源上，一个典型的情形是搜索，典型的Web API搜索URI是这样的： 1https://api.example.com/v1/search?query=xxx 这么设计搜索API基本是一种约定俗成的规范，像这类特殊的URI其实可以不拘泥于动词 + 名词的形式，只要这个URI能准确表达出意图，一般也没什么问题。 查询参数分除了host, path以外，Web API还有一个很重要的组成部分：query，也就是查询参数。查询参数的作用是更详细的描述URI所指定的资源。对于是把一些参数放在path中还是放在query中，主要是看这个参数相对于资源的意义。我个人的理解是，如果这个参数具有唯一描述某个资源的能力，比如id，推荐将其放在path中。比如下面的实例描述了一个公司的某个职员： 1/api/v1/companies/123/employees/456 这里用公司ID和职员ID来唯一定位到资源。还有一种设计方式： 1/api/v1/companies/123?employee_id=456 这种方式当然也没问题，不过在URI的长度不是特别长的情况下，建议使用第一种方式。 有时候我们想通过一系列的参数来对资源进行查询，这一般是一种范围性的查询，不像用ID那样直接定位到唯一一个资源，此时可以使用query去设计URI。例如想要获取某个公司开发部门的，且性别为男的员工，且以名字升序排列： 1/api/v1/companies/123?department=development&amp;gender=male&amp;sort=name 通过区分参数的性质来设计，我们也能让API更加优雅。 分页在获取资源列表的时候，比如获取用户列表，由于用户可能非常多，一次性获取全部用户不现实，因此很自然地会用分页来获取。分页的方案大致可以分为两种：绝对位置分页和相对位置分页。 基于相对位置的分页方案使用页数和每页资源个数来分页获取用户： 1GET http://api.example.com/v1/users?page=2&amp;limit=50&amp;sort=+name 我们在查询参数中指定了page和limit，这种分页方案以页数为单位，每次获取由limit指定的个数的资源，并指定了排序规则为用户名升序排列(+为升序，-为降序)。这种分页方案很直接，但是有一个问题，由于指定了页数，也就是说我们需要skip前面几页的资源。这在数据库中的操作是这样的，首先查询出所符合查询条件的所有条目，然后skip掉指定数量的条目，skip数量=(page - 1) * limit。这时如果资源集合非常大，页数也指定得很大，数据库就需要skip掉非常多的条目，这会导致查询越来越慢。 基于页数和每页条目数的分页方案还有一种变体，就是指定offset和size，比如在基于页数和每页条目数的分页方案中的参数为page=2&amp;limit=50，也就是要跳过前面一页（50个个条目），对于offset和size的方案就是offset=50&amp;size=50。这两种方式其实质都是一样的，基于资源的相对位置来分页。 基于资源的相对位置来分页还有一个问题就是在数据插入/删除频繁的场景下回重复获取。比如记录A位于第50条，使用page=1&amp;limit=50获取时，记录A位于最后一个位置，如果在获取下一页之前，由于某种情况删除了1-50条之间的任意一条或几条，获取下一页的时候，记录A还将出现在返回列表中。 在数据量不大或者插入/删除不太频繁的场景下，基于相对位置的分页工作得还可以。但如果要彻底避免大量skip和重复获取的问题，就要使用基于绝对位置的分页方案了。 基于绝对位置的分页方案基于绝对位置的分页不再以资源在数据库中的顺序为参考点，而是以一个能快速定位具体资源的方式做为参考点，比如主键或者任何unique key。一般资源都有主键，可以考虑用下面这种方式来获取分页的资源列表： 1GET http://api.example.com/v1/users?max_id=12345&amp;limit=50 这种方案中的一般做法是，将当前获取到的资源列表的最后一条的unique key作为定位点，向后获取limit参数指定的数量的条目。数据库通过在这个unique key上加上合适的索引来加速这种查询，因此查询效率非常高。 返回“是否还有后续数据”为了让前端做分页，不可避免的需要告知前端“是否还有后续数据”的信息。这里面又有两种常见的情况： 需要知道总页数 不需要知道总页数 需要知道总页数的情况相当常见，比如我们有很多订单，前端对订单列表做分页，用户往往需要知道“总共有多少订单”、“分页的总数”这类信息。这时服务端需要维护资源总数的信息。但是实时计算出资源总数有时候不现实（比如那些动辄上百万个的资源），这时候后端会使用一些其他技巧来实现，不过这不在本文的讨论范围内。 还有一种情况是不需要知道总页数，比如新闻资讯列表、社交媒体的timeline等。假设此时每页有N个资源，那么当后端在实际获取资源时，每次都获取N+1数量的资源，如果能获取到N+1个，就说明还有下一页，否则当前页就是最后一页。如果还有下一页，就需要把多获取的这个排除掉，只返回N个给客户端。这种方式的成本和实现难度都很低。 授权在使用一些需要用户身份认证的Web API时需要做授权操作，OAuth 2.0已经成为Web API授权的事实标准，OAuth 2.0 支持4种授权模式： Authorization Code Implicit Resource Owner Password Credentials Client Credentials OAuth 2.0详细的信息可以查看这里OAuth 2.0介绍。 响应数据设计响应数据格式目前主流的Web API设计中，响应数据格式大部分是JSON。在比较早以前Web API中曾大量使用XML，但JSON由于其简洁易用性等优点很快被广大开发者所接受，慢慢替代了XML称为最主流的Web API响应数据格式。现在基本上很难找到哪个Web API是不支持JSON格式的，很多Web API甚至只支持JSON而不支持XML。 Web API 常用的HTTP状态码HTTP相应状态码有五大类：1xx, 2xx, 3xx, 4xx, 5xx： HTTP响应吗类型 含义 1xx 信息状态码 2xx 成功状态码 3xx 重定向状态码 4xx 客户端错误状态码 5xx 服务端错误状态码 可以参考常用的Web API HTTP状态码。 更详细的状态码——应用级别的状态码由于HTTP状态码只能表达问题的大类，在一些业务规则比较复杂的场景下，出错的时候我们希望服务端为客户端提供足够详细的出错信息，此时可以在响应体中提供应用级别的状态码和状态信息，一个参考例子： 12345&#123; \"error\": true, \"errorCode\": \"xxxx\", \"errorMessage\": \"xxxx\"&#125; 在设计应用级别的状态码和状态信息时也应该注意分类，并在Web API文档中详细说明各个状态码的含义。 HTTP中的缓存相比于从内存和硬盘中获取数据，网络请求的速度实在是太慢了，因此一些情况下将从服务器端获得的资源缓存起来就很关键，这能大大提高响应速度和降低服务器带宽/计算成本。 HTTP中的缓存概念大致分为两部分： 过期模型 验证模型 过期模型指明了一个资源何时过期，一旦资源过期，客户端就必须抛弃这个资源，重新从服务端获取。先来看看HTTP协议中和过期模型有关的响应首部： Expires Cache-Control 过期模型Expires响应首部1expires: Sat, 21 Dec 2019 09:32:24 GMT Expires响应首部指明了资源过期的时间点，表示资源在这个时间点之后是过期的，这是一个绝对值。需要注意的是Expires用的是服务器的时间，如果客户端和服务器时间不一致，会导致一些误差。 Cache-Control响应首部Cache-Control的用法比较多，比如可以指明资源要经过多少时间后才过期： 1cache-control: max-age=3600 这指明了资源经过1小时候过期，max-age的单位是秒。 或者可以指明某个资源不需要被客户端缓存： 1cache-control: no-store 还可以指明在请求该资源时，需要先询问服务器是否有更新的版本： 1cache-control: no-cache 验证模型过期模型只能通过查看响应首部中Expires和Cache-Control来得知资源的过期与否，验证模型则需要客户端向服务端询问资源的过期情况，这被称为“附带条件的请求”。客户端需要在请求中附带资源最后的更新日期(Last Modified Time)或实体标签(ETag)，比如 12etag: &quot;5c137a66-c1a3&quot;last-modified: Fri, 14 Dec 2018 09:39:50 GMT Last-Modified指明了资源最后一次更新的时间，ETag可以认为是资源的标识符，如果资源被更新了，它的标识符就会变化，这有资源的版本有点类似。 另外ETag还有“强验证”和“弱验证”两种，强验证大概是这样的： 1etag: &quot;5c137a66-c1a3&quot; 弱验证需要在双引号之前加上一个”W/“： 1etag: W/&quot;5c137a66-c1a3&quot; 二者的差别在于，强验证下，客户端缓存的资源和服务端的资源只要有任何一点不同，都会被判断为不同，需要重新从服务器获获取资源的最新数据。弱验证宽松很多，并不要求资源的完全一致，只要资源从使用意义来看没差别就不需要重新获取数据，比如一些网页上的广告信息。 这里不打算详细讲解HTTP缓存相关的内容，有需要可以参考Google和Mozilla官方关于HTTP Cache的资料： http-caching HTTP caching 在请求和响应中指明媒体类型和可接受的数据格式使用Content-Type指明媒体类型很重要，这关系到服务端是否能正确理解客户端发来的请求和客户端能否正确解析服务端发来的响应。例如在返回JSON格式数据的Web API中，响应首部中应该指明响应的Content-Type： 1234HTTP/1.1 200 OK...content-type: application/json; charset=utf-8... 如果客户端向服务端请求时所带的数据也是JSON格式的，也应该在请求头中说明： 12345POST /api/v1/projects HTTP/1.1Host: api.example.comAccept: application/jsonContent-Type: application/json... Content-Type相当于客户端和服务端对数据格式的协商内容，任何一方再和另一方通信时，指明Content-Type就相当于告知对方：我给你的数据是什么媒体类型的。另一方得到这个信息后就可以才去针对这个媒体类型的操作。比如一个创建商品的接口既可以接受JSON数据也可以接受XML数据，那么客户端在发送请求时就必须指明所发送的数据是什么媒体类型的，否则服务端很可能将无法正确处理请求。相反，如果一个获取商品信息的接口同时支持返回JSON和XML两种格式的数据，那么也同样要指明响应数据的格式，否则客户端可能无法正确解析。 另外，还可以通过Accept首部指明接受何种类型的数据，比如上面的POST请求中，指明了Accept: application/json，这就告知服务端，客户端只能接受JSON格式的数据。 总而言之，Content-Type和Accept首部对于客户端和服务端双方通信数据的格式约定非常重要。 定义私有首部有些时候HTTP协议中预定义的首部不能满足我们的需求，还需要定义私有首部。比如需要对客户端进行限速的场景，一般做法是指定一个X-RateLimit-Limit首部： 1X-RateLimit-Limit: 60 至于这个限速的时间单位是多少，不同应用的单位可能不一样，有使用小时的也有使用天的，需要开发者自己去查看Web API文档。 一般来说，以”X-“开头的首部是私有首部。 跨域现代Web应用中大量使用Ajax来获取数据，但浏览器的同源策略限制了这一技术的使用。同源策略简单说就是：协议名、主机、端口号这三个数据唯一确定了一个“源”。处于安全方面的考虑，默认情况下浏览器不允许通过Ajax请求不同“源”下的资源。在服务端经过特殊配置后允许不同源的客户端请求，这称为“跨域”。但是既然是Web API，就是要公开出来给其他人用，势必需要支持跨域，否则公开没有任何意义。 其实有一种方式可以绕过浏览器同源策略（需要客户端和服务器端做一定的支持），就是JSONP，但实际上JSONP称不上是什么特别优秀的实践，很多时候是处于无奈才使用。所以如果不是特别需要，建议不要使用JSONP。 目前Web API 主流的跨域方案是跨域资源共享Cross-Origin Resource Sharing，如果需要允许某个域具有访问我方服务器，可以在请求头中带上： 1Access-Control-Allow-Origin: http://www.example.com 如果要允许任何域访问，可以用”*”指定： 1Access-Control-Allow-Origin: * 还有一种方式是服务器端可以在域名的根目录下，放置crossdomain.xml文件： 123456&lt;?xml version=\"1.0\"?&gt;&lt;cross-domain-policy&gt; &lt;allow-access-from domain=\"www.example.com\" /&gt; &lt;allow-access-from domain=\"*.foo.com\" /&gt; &lt;allow-access-from domain=\"110.56.67.189\" /&gt;&lt;/cross-domain-policy&gt; 如果要允许任意跨域，同样可以用”*”： 1234&lt;?xml version=\"1.0\"?&gt;&lt;cross-domain-policy&gt; &lt;allow-access-from domain=\"*\" /&gt;&lt;/cross-domain-policy&gt; 参考资料 OAuth 2.0介绍 常用的Web API HTTP状态码 http-caching HTTP caching JSONP CORS","categories":[{"name":"Web","slug":"Web","permalink":"https://nullcc.github.io/categories/Web/"}],"tags":[{"name":"API","slug":"API","permalink":"https://nullcc.github.io/tags/API/"}]},{"title":"使用nginx实现简单的负载均衡和高可用","slug":"使用nginx实现简单的负载均衡和高可用","date":"2018-11-26T16:00:00.000Z","updated":"2022-04-15T03:41:13.028Z","comments":true,"path":"2018/11/27/使用nginx实现简单的负载均衡和高可用/","link":"","permalink":"https://nullcc.github.io/2018/11/27/使用nginx实现简单的负载均衡和高可用/","excerpt":"本文将介绍如何用nginx实现后端服务集群的高可用的最简单方案。","text":"本文将介绍如何用nginx实现后端服务集群的高可用的最简单方案。 系统情况描述： 现在有两台部署在不同机器上的后端服务实例，想要将其组成一个服务集群统一对外提供服务，并且在其中一个实例失效的情况下，集群对外还能正常提供服务。最简单的做法是用一个nginx将这两个实例组合起来，并提供一定程度上的健康检查，如果发现某个实例不可用，就暂时不将请求转发给它，直到该实例通过健康检查为止。 先来看nginx的配置文件nginx.conf： 123456789101112131415161718192021222324252627282930313233343536373839404142434445user nginx;worker_processes 1;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; #include /etc/nginx/conf.d/*.conf; upstream backends &#123; # 健康检查失败一次即认为实例失效，并在接下来的5s内不将请求转发到该失败实例上，其中backend1和backend2为后端服务器的地址 server backend1 max_fails=1 fail_timeout=5s; server backend2 max_fails=1 fail_timeout=5s; &#125; server &#123; listen 80; location / &#123; proxy_pass http://backends; &#125; &#125;&#125; 在docker中运行nginx，并使用指定的配置文件： 1234567docker run \\ -d \\ --net=host \\ -p 80:80 \\ --name nginx \\ -v ~/docker/nginx/nginx.conf:/etc/nginx/nginx.conf:ro \\ nginx 这样配置后，这台nginx的宿主机即作为反向代理，且能在一定程度上保证后端实例高可用。当然，这里存在一个nginx失败的单点问题，一旦这个nginx实例挂了，整个服务就挂了。这可以通过为这台nginx实例的机器配置keepalived，利用IP漂移来保证nginx本身的高可用。","categories":[{"name":"web后端","slug":"web后端","permalink":"https://nullcc.github.io/categories/web后端/"}],"tags":[{"name":"高可用","slug":"高可用","permalink":"https://nullcc.github.io/tags/高可用/"}]},{"title":"深入理解JavaScript的Prototype","slug":"深入理解JavaScript的Prototype","date":"2018-11-11T16:00:00.000Z","updated":"2022-04-15T03:41:13.035Z","comments":true,"path":"2018/11/12/深入理解JavaScript的Prototype/","link":"","permalink":"https://nullcc.github.io/2018/11/12/深入理解JavaScript的Prototype/","excerpt":"本文将展示常常令人迷惑和误解的JavaScript的Prototype到底是什么。","text":"本文将展示常常令人迷惑和误解的JavaScript的Prototype到底是什么。 面向对象编程中的类在大部分面向对象编程语言中都可以看到“类”的身影，说到“类”，进而就会谈到“继承”、“封装”和“多态”。类是一种蓝图，描述了该类的实例应该具有的数据和行为，我们会调用类的构造函数（构造函数属于类）来实例化一个对象出来。这个实例化出来的对象拥有类所描述的特性和行为。子类继承自父类就相当于将父类复制一份到子类中，子类和父类是相对独立的，在子类中调用或者覆盖父类方法并不会对父类造成影响。因此，类的继承本质就是复制。多态建立在复制这个事实基础上，表面看上去多态是由于子类实例引用了父类方法，实质上多态并不表示子类和父类有关联，而只能说明子类得到了父类的一份副本。 也就是说，在传统面向对象的设计理念中，类体系的核心是复制。 JavaScript中的prototypeprototype的基本行为下面的代码定义了一个对象obj，它本身拥有一个属性foo，使用obj.foo可以获取这个属性的值： 123// demo1.jsconst obj = &#123; foo: 1 &#125;;console.log(obj.foo); // 1 \b\b这和prototype有什么关系呢？简单来说，在JavaScript中的某个对象上引用属性时，首先会查看这个对象本身是否拥有这个属性，如果有就返回这个属性的值。如果没有，就需要查看该对象的prototype链了。以下代码将obj的prototype关联到另一个对象上： 123456789// demo2.jsconst obj1 = &#123; foo: 1 &#125;;const obj = Object.create(obj1);console.log(obj.foo); // 1console.log(Object.getPrototypeOf(obj) === obj1); // trueconsole.log(obj1.isPrototypeOf(obj)); // trueconsole.log(Object.getPrototypeOf(obj1) === Object.prototype); // trueconsole.log(Object.prototype); // &#123;&#125;console.log(Object.getPrototypeOf(Object.prototype)); // null Object.create会创建一个对象，并将这个对象的prototype关联到指定的对象。Object.getPrototypeOf用来获取一个对象的prototype。isPrototypeOf可以判断一个对象是否存在于另一个对象的prototype链上。 一般来说，普通对象的prototype链最终将指向Object.prototype，而Object.prototype的prototype是null。如下图所示： 上面的代码中，obj本身并没有foo这个属性，所以调用obj.foo时会沿着obj的prototype链查找foo属性，最终在obj1上找到了。 1234567891011121314151617181920212223242526272829// demo3.jsconst obj2 = &#123; bar: 2 &#125;;Object.defineProperty(obj2, 'baz', &#123; value: 3, enumerable: false,&#125;)const obj1 = Object.create(obj2);obj1.foo = 1;const obj = Object.create(obj1);obj.a = 0;console.log(obj); // &#123; a: 0 &#125;console.log(obj1); // &#123; foo: 1 &#125;console.log(obj2); // &#123; bar: 2 &#125;console.log(obj.foo); // 1console.log(Object.getPrototypeOf(obj) === obj1); // trueconsole.log(Object.getPrototypeOf(obj1) === obj2); // trueconsole.log(Object.getPrototypeOf(obj2) === Object.prototype); // trueconsole.log(Object.prototype); // &#123;&#125;console.log(Object.getPrototypeOf(Object.prototype)); // nullconsole.log(obj2.isPrototypeOf(obj1)); // trueconsole.log(obj1.isPrototypeOf(obj)); // trueconsole.log(obj2.isPrototypeOf(obj)); // truefor (const key in obj) &#123; console.log(key);&#125; 运行上述代码，for循环部分会打印出： afoobar 可以发现遍历一个对象时会将其本身的属性(a)和它prototype链上的所有可枚举属性(foo和bar)都遍历出来。 如果只想遍历对象本身的属性，需要进行hasOwnProperty的判断： 12345678910111213141516// demo4.jsconst obj2 = &#123; bar: 2 &#125;;Object.defineProperty(obj2, 'baz', &#123; value: 3, enumerable: false,&#125;)const obj1 = Object.create(obj2);obj1.foo = 1;const obj = Object.create(obj1);obj.a = 0;for (const key in obj) &#123; if (obj.hasOwnProperty(key)) &#123; console.log(key) &#125;&#125; 这段代码的将打印出： a JavaScript的“类”和“构造函数”JavaScript中有new关键字，于是人们顺理成章地将它当做调用“构造函数”的标志：至少我们在Java/C++中是这么做的。 123456789101112// demo5.jsfunction Foo() &#123; console.log(1);&#125;const foo = new Foo();console.log(foo); // Foo &#123;&#125;function Bar(a) &#123; this.a = a;&#125;const bar = new Bar(1);console.log(bar); // Bar &#123; a: 1 &#125; 可以发现，对函数使用new会返回一个对象，即使这个函数本身没有返回任何值（此时返回的是{}）。函数内部的this.x = y的赋值语句会使最终返回的对象具有相应的属性。正因为这样，人们认为Foo是一个类。再看下面的代码： 123456789// demo6.jsfunction Foo(name) &#123; this.name = name;&#125;const foo = new Foo('a');console.log(foo); // Foo &#123; name: 'a' &#125;console.log(Foo.prototype.constructor === Foo); // trueconsole.log(foo.constructor === Foo); // true 我们发现由new Foo()创建出来的foo有一个constructor属性，且foo.constructor指向Foo，所以人们更加笃定foo由Foo“构造”，foo是“Foo类”的一个实例。其实Foo和普调的函数并没有区别，只是JavaScript会让所有带有new的函数调用构造一个对象并返回它。 再来看看人们怎么处心积虑地在JavaScript模拟类的行为： 123456789101112131415// demo7.jsfunction Foo(name) &#123; this.name = name;&#125;Foo.prototype.getName = function() &#123; return this.name;&#125;const a = new Foo('a');const b = new Foo('b');console.log(Object.getPrototypeOf(a) === Foo.prototype); // trueconsole.log(a.getName()); // aconsole.log(a.getName === b.getName); // trueconsole.log(a.getName === Foo.prototype.getName); // true 解释一下上面这段代码，由Foo“构造”出来的对象a有一个name属性，且a的prototype指向Foo.prototype。我们在Foo.prototype上添加一个方法getName，于是再a上执行getName()时，我们成功地通过prototype链找到Foo.prototype.getName，并调用它。接着我们又“构造”了一个对象b，然后我们发现a.getName、b.getName和Foo.prototype.getName指向的是同一个对象。 这就有点意思了，在传统的类理论中，子类会复制父类的信息，所以子类和父类的同名方法在内存中必然是两个完全不同的对象，我们知道JavaScript中的===是比较对象同一性的。上面的代码意味着不管“构造”出多少个“Foo类”的实例，所有实例的方法都指向Foo.prototype中的方法。这显然和传统面向对象的类理论相违背。 更神奇的是下面这段代码： 123456789101112// demo8.jsfunction Foo(name) &#123; this.name = name;&#125;console.log(Foo.prototype.constructor === Foo); // trueFoo.prototype = &#123;&#125;;console.log(Foo.prototype.constructor === Foo); // falseconsole.log(Foo.prototype.constructor === Object); // trueconst a = new Foo('a');console.log(a.constructor === Foo); // falseconsole.log(a.constructor === Object); // true 上面的代码中改变了Foo.prototype，这导致了后面a.constructor不再指向Foo。也就是说，Foo.prototype的constructor属性默认情况下指向该函数自身，但如果我们在创建新对象后改变了Foo.prototype的指向，那么新对象的constructor属性并不会保持原来的指向（因为是引用）。因此，你无法通过a.constructor来确切地知晓是谁“构造”了a。 proto和prototype先来看一段代码： 1234567891011121314151617// demo9.jsfunction Foo(name) &#123; this.name = name;&#125;Foo.prototype.getName = function() &#123; return this.name;&#125;;const foo1 = new Foo('a');console.log(Object.getPrototypeOf(foo1) === foo1.__proto__) // true, 对象的原型可以用Object.getPrototypeOf或者__proto__属性获得console.log(Object.getPrototypeOf(Foo) === Foo.__proto__); // true, 函数也是对象，因此也可以用Object.getPrototypeOf或者__proto__属性获得其原型console.log(foo1.prototype); // undefined, 只有函数对象才有prototype属性console.log(Object.getPrototypeOf(foo1) === Foo.prototype); // true, 由函数“构造”出来的对象的原型默认指向该函数的prototype属性console.log(foo1.constructor === Foo) // true, 由函数“构造”出来的对象的constructor属性默认指向函数本身console.log(Foo.prototype.constructor === Foo) // true, 函数的prototype的constructor属性默认指向函数本身 这些错综复杂的关系可以用一张图（稍微清晰一点地）表示： instanceof看下面这段代码： 123456// demo10.jsfunction Foo() &#123;&#125;const foo = new Foo();console.log(foo instanceof Foo); // true 人们往往希望使用instanceof来判断一个对象是否是某个“类”的实例，从字面意思看来这是很直白的。但instanceof回答的问题是，在foo的prototype链中是否有一个对象指向Foo.prototype。通过上面的图我们知道，通过调用new Foo()得到foo，因此foo的原型是Foo.prototype。所以这里结果是true。但是instanceof只能用于对象和函数之间，不能用于对象与对象之间。举个例子，如果是下面这样的代码，用instanceof是不行的： 123456// demo11.jsconst obj1 = &#123; a: 1 &#125;;const obj = Object.create(obj1);console.log(obj instanceof obj1); // TypeError: Right-hand side of 'instanceof' is not callable 我们用Object.create创建了一个对象，并将该对象prototype指向obj1，如果要判断一个对象的prototype是否是另一个对象，需要使用isPrototypeOf： 1234567// demo12.jsfunction Foo() &#123;&#125;const foo = new Foo();console.log(Foo.prototype.isPrototypeOf(foo)); // trueconst obj1 = &#123; a: 1 &#125;;const obj = Object.create(obj1);console.log(obj1.isPrototypeOf(obj)); // true 仔细想想可以发现，实际上根本不存在我们以为的“x是y的实例”这种关系，也就是传统意义上的instanceof，对象间只有引用关系，如果要表示某个对象在另一个对象的prototype链上（不论是普通对象还是函数），最好使用isPrototypeOf。 JavaScript对象间关系的本质——对象关联通过上面的一些例子，我们发现JavaScript中根本不存在所谓的“类继承”机制。对象间是引用、关联的关系。理解了这个事实，很多JavaScript的“神奇”行为也很好解释了，很多人之所以会对JavaScript的“类继承”机制一头雾水，其实完全是因为以错误的方式去尝试理解它。现在再来思考Object.create带来了什么： 1234567891011// demo13.jsconst obj1 = &#123; a: 1, foo: () =&gt; &#123; return 2; &#125;&#125;;const obj = Object.create(obj1);console.log(obj.a); // 1console.log(obj.foo()); // 2 obj的prototype是obj1，执行obj.a或者obj.foo()其实是在使用obj的prototype上的属性和方法，这其实是一种委托，而委托本质上是因为对象关联。为了避免属性屏蔽或者冲突，建议在对象上显式地使用委托： 12345678910111213// demo14.jsconst obj1 = &#123; a: 1, foo: () =&gt; &#123; return 2; &#125;&#125;;const obj = Object.create(obj1);obj.doFoo = function() &#123; return this.foo();&#125;;console.log(obj.a); // 1console.log(obj.doFoo()); // 2 显式地使用委托也很简单，在对象上新建一个方法，在方法内部使用this来调用委托方法。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"js","slug":"js","permalink":"https://nullcc.github.io/tags/js/"}]},{"title":"深入理解Node.js异步编程","slug":"深入理解Node.js异步编程","date":"2018-11-03T16:00:00.000Z","updated":"2022-04-15T03:41:13.035Z","comments":true,"path":"2018/11/04/深入理解Node.js异步编程/","link":"","permalink":"https://nullcc.github.io/2018/11/04/深入理解Node.js异步编程/","excerpt":"本文将深入解析node.js的异步世界（本文比较长，请准备好瓜子和可乐）。","text":"本文将深入解析node.js的异步世界（本文比较长，请准备好瓜子和可乐）。 准备工作为了保证文章内所列代码能够正确运行，建议安装babel： 1npm install babel-cli -g 写在阅读之前开发者需要注意区分JavaScript和JavaScript运行时（宿主环境）这两个概念。严格来说，JavaScript单纯指这门编程语言，没有其他附加的含义。对于宿主环境，如果是Web前端开发，默认是浏览器，如果是Node.js，则指的是node.js运行时。不同的宿主环境有很大区别，比如浏览器和node.js的事件循环机制就有所区别。另外像console这个对象（没错，就是你经常用的console.log的那个console）也是由宿主环境提供的，它并不是JavaScript的一部分。 需要特别说明的是，本文的事件循环部分主要探讨的宿主环境是node.js，异步编程部分中的绝大多数内容都适用于目前常见的宿主环境，如浏览器、Node.js等。要注意的是虽然不同宿主环境有很多相似的地方，但是我们还是要注意区分他们的不同点。 并发模型和事件循环由于JavaScript是单线程运行的，因此它天生是异步的。试想如果一个单线程的程序是同步执行的，一旦有调用阻塞线程，线程就挂起了。对应到现实中的会发现，浏览器因为一个HTTP请求而无法响应用户操作。在使用JavaScript时（不论在哪个宿主环境），都要牢记它是单线程运行的，这个概念非常重要。 大部分使用node.js的人都被它的“异步非阻塞”特性所吸引，一些I/O密集型的应用在使用异步非阻塞的实现后，性能可以有很大的提升，而且应用所占用的资源还比原来采用同步方式编程的低得多。在语言级别，由于是单线程运行，所以完全不存在线程间同步这种麻烦事。 Node.js的并发模型基于事件循环(Event Loop)。\b下面是一个最简单的事件循环模型： 123while (queue.waitForMessage()) &#123; queue.processNextMessage();&#125; 这是一个无限while循环，当事件队列中有未处理的消息时，就取出一个消息来处理，否则就一直等待直到有队列中有消息。 为了解释Node.js的事件循环，这里直接引用我翻译的Node.js官方文档中对其事件循环的描述(译)深入理解Node.js的事件循环、定时器和process.nextTick()。 JavaScript异步编程的几种常见模式 回调函数 Promise Generation Function async/await Event 回调函数(callback)回调函数是最基本的一种异步调用模式，回调函数会在异步操作完成之后被调用。下面试一个简单的Node.js中异步读取文件的例子： 1234567891011// readFileCallback.jsconst fs = require('fs');fs.readFile('a.txt', (err, data) =&gt; &#123; if (err) &#123; throw err; &#125; console.log(data.toString());&#125;);console.log('foo'); 运行结果如下： 123$ babel-node readFile.jsfoofile a content foo被先打印出来，接着等文件读取完毕，打印出文件内容file a content，可以看到读取文件这个操作并不会阻塞当前进程。因为Node.js运行时直接从fs.readFile中返回，继续往下运行。 再看一个定时器的例子： 1234567// timerCallback.jsconst fn = () =&gt; &#123; console.log(1);&#125;;setTimeout(fn, 3000);console.log(2); 运行这段代码会发现运行后控制台立即打印出2，接着在大约3000毫秒后，控制台打印出1。这个例子再次体现了Node.js的异步特性。 我们再来看看在同步模式中写代码的场景。假设用户想要读取一个文件，由于读取文件（内部是一个系统调用，需要陷入内核）是一个耗时操作（文件比较大或者使用机械硬盘的时候的尤其耗时），因此在同步模式下，这个读取操作会阻塞当前进程（假设目前没有使用多线程），当前进程将被挂起。当前进程的其他代码在该读取操作完成之前无法被执行，如果这个文件的读取需要耗费1秒，则当前进程就要被阻塞1秒，也就是说宝贵的CPU资源在程序运行的时候要被白白浪费1秒。不要小看这1秒，1秒的CPU资源在程序在运行的时候是非常宝贵的。 如果我们想要使用回调函数的方式按顺序读取两个文件，再打印出它们的内容就要嵌套使用回调函数了： 123456789// nestReadFileCallback.jsconst fs = require('fs');fs.readFile('a.txt', 'utf8', (err, data) =&gt; &#123; console.log(\"a file content: \" + data); fs.readFile('b.txt', 'utf8', (err, data) =&gt; &#123; console.log(\"b file content: \" + data); &#125;);&#125;); 结果如下： 123$ babel-node nestCallback.jsa file content: file a contentb file content: file b content 这里为了达到异步串行执行的目的，我们使用了嵌套回调。代码开始有点不清爽了，想象一下如果多个异步调用需要按一定顺序串行执行，例如后一次异步调用依赖前一次异步调用的数据，代码会是这个样子： 1234567891011// callback helldoSomethingAsync1((err1, data1) =&gt; &#123; doSomethingAsync2(data1, (err2, data2) =&gt; &#123; doSomethingAsync3(data2, (err3, data3) =&gt; &#123; doSomethingAsync4(data3, (err4, data4) =&gt; &#123; doSomethingAsync5(data4, (err5, data5) =&gt; &#123; &#125;); &#125;); &#125;); &#125;);&#125;); 如果业务逻辑比较复杂，维护这种代码简直是噩梦，开发者把这种代码叫做callback hell（回调地狱）。那怎么办呢？我们可以使用Promise。 PromiseES 6中原生提供了Promise对象，Promise对象代表某个未来才会知道结果的事件(一般是一个异步操作)，换句话说，一个Pomise就是一个代表了异步操作最终完成或者失败的对象。Promise本质上是一个绑定了回调的对象，而不是像callback异步编程那样直接将回调传入函数内部。 Promise对外提供了统一的API，可供进一步处理。Promise的最终状态有两种：fulfilled和rejected，fulfilled表示Promise处于完成状态，rejected表示Promise处于被拒绝状态，这两种状态都是Promise的已决议状态，相反如果Promise还未被决议，它就处于未决议状态。 需要强调的一点是，Promise一经决议就无法改变其状态，这使得Promise和它的名字一样：君子一言驷马难追。 使用Promise对象可以用同步操作的流程写法来表达异步操作，避免了层层嵌套的异步回调，代码也更加清晰易懂，方便维护。用Promise重写读取文件的例子： 1234567891011121314151617181920// promiseReadSingleFile.jsconst fs = require('fs')const read = filename =&gt; &#123; return new Promise((resolve, reject) =&gt; &#123; fs.readFile(filename, 'utf8', (err, data) =&gt; &#123; if (err)&#123; reject(err); &#125; resolve(data); &#125;); &#125;);&#125; read('a.txt').then(data =&gt; &#123; console.log(data);&#125;, err =&gt; &#123; console.error(\"err: \" + err);&#125;); 如果有多个异步操作需要串行执行，且后一个操作需要拿到前一个操作的结果，我们可以在Promise上使用链式调用(Promise chain)，下面是顺序读取两个文件的例子： 1234567891011121314151617181920212223242526// promiseReadMultiFiles.jsconst fs = require('fs')const read = filename =&gt; &#123; return new Promise((resolve, reject) =&gt; &#123; fs.readFile(filename, 'utf8', (err, data) =&gt; &#123; if (err)&#123; reject(err); &#125; resolve(data); &#125;); &#125;);&#125;read('a.txt').then(data =&gt; &#123; console.log(data); return read('b.txt'); // 注意这里：在then中返回一个Promise&#125;, err =&gt; &#123; console.error(\"err: \" + err);&#125;).then(data =&gt; &#123; console.log(data);&#125;, err =&gt; &#123; console.error(\"err: \" + err);&#125;); 现在可以大致总结一下用Promise写串行异步程序的基本模式： 1234567891011121314func1().then(result1 =&gt; &#123; return func2(result1);&#125;).then(result2 =&gt; &#123; return func3(result2);&#125;).then(result3 =&gt; &#123; return func4(result3);&#125;).catch(err =&gt; &#123; // handle error&#125;) then里的参数是可选的，这里的.catch(errCallback)其实是then(null, errCallback)的缩写形式。需要注意的是，如果想要在then的fulfilled中获取上一个Promise中的结果，上一个Promise中必要显式返回结果。 catch之后还可以继续链式调用： 123456789101112131415// catch1.jsnew Promise((resolve, reject) =&gt; &#123; console.log('Initial'); resolve();&#125;).then(() =&gt; &#123; throw new Error('Something failed'); console.log('Do something'); // never reach here!&#125;).catch(() =&gt; &#123; console.error('Catch error');&#125;).then(() =&gt; &#123; console.log('Do this whatever happened before');&#125;); 运行结果如下： 1234$ babel-node catch1.jsInitialCatch errorDo this whatever happened before 一个Promise链式调用在遇到错误时会立即停止，此时如果在该出错的then之后有catch（不管这个catch是否紧跟在出错then之后），这个catch里的errCallback都会被调用，出错then和catch中间的所有then都会被忽略： 123456789101112131415161718// catch2.jsnew Promise((resolve, reject) =&gt; &#123; console.log('Initial'); resolve();&#125;).then(() =&gt; &#123; throw new Error('Something failed'); console.log('Do this'); // never reach here!&#125;).then(() =&gt; &#123; console.log('Skip this'); // never reach here!&#125;).catch(() =&gt; &#123; console.error('Catch error');&#125;).then(() =&gt; &#123; console.log('Final');&#125;) 运行结果如下： 1234$ babel-node catch2.jsInitialCatch errorFinal 在实际编程中，如果我们将一系列异步操作使用Promise链串行执行，意味着这一串操作是一个整体。一旦整体操作中的某个步骤出错，都不应该继续执行下去了。此时我们可以把catch放在Promise链的最后： 123456789101112131415161718// catch3.jsnew Promise((resolve, reject) =&gt; &#123; console.log('Initial'); resolve();&#125;).then(() =&gt; &#123; console.log('Do something 1');&#125;).then(() =&gt; &#123; throw new Error('Do something 2 failed'); console.log('Do something 2'); // never reach here!&#125;).then(() =&gt; &#123; console.log('Do something 3'); // never reach here!&#125;).catch((err) =&gt; &#123; console.error(`Catch error: $&#123;err&#125;`);&#125;) 运行结果如下： 1234$ babel-node catche.jsInitialDo something 1Catch error: Error: Do something 2 failed 这么做的好处显而易见，这符合软件工程中的Fail Fast。 小练习将setTimeout函数Promise化。 解析： setTimeout是一个旧式的异步API，它接受一个回调和一个时间参数。在ES 6以后写异步代码，强烈不建议直接调用旧式的异步API，应该把这些API都包装成Promise，并且永远不要在业务代码中直接调用这些旧式异步API。为什么不建议这么做？一个很重要的原因对异常的捕获会有问题： 12345678910// setTimeoutError.jsconst fn = () =&gt; &#123; throw new Error('This is an error!');&#125;;try &#123; setTimeout(fn, 1000);&#125; catch (err) &#123; console.error(err); // never reach here!&#125; 这里try/catch块无法捕获到fn中的异常。 参考代码： 12345678910111213// timerPromise.jsconst delay = time =&gt; &#123; return new Promise((resolve, reject) =&gt; &#123; setTimeout(resolve, time); &#125;);&#125;;delay(5000).then(() =&gt; &#123; console.log('here');&#125;)console.log('hello'); 运行这段代码，hello会被立即打印，here会在大约5000毫秒后被打印： 123$ babel-node timerPromise.jshellohere 回到刚才说到的异常捕获问题，将setTimeout包装成Promise后，我们就可以捕获到异常了： 1234567891011121314// timerPromiseCatch.jsconst delay = time =&gt; &#123; return new Promise((resolve, reject) =&gt; &#123; setTimeout(resolve, time); &#125;);&#125;;delay(5000).then(() =&gt; &#123; throw new Error('This is an error!');&#125;).catch(err =&gt; &#123; console.error(err); // Error: This is an error!&#125;) Promise APIPromise.resolve() 和 Promise.reject()使用Promise.resolve()可以立即得到一个已经resolve的Promise，这里有两种情况，如果入参本身就是一个Promise，则Promise.resolve()原样返回这个Promise，如果入参是一个立即值（比如一个整型），那么Promise.resolve()会将这个立即值包装成Promise然后返回： 123456789101112// promiseResolve.jsconst p1 = Promise.resolve(100);console.log(p1); // Promise &#123; 100 &#125;const p2 = new Promise((resolve, reject) =&gt; &#123; resolve(200);&#125;);console.log(p2); // Promise &#123; 200 &#125;const p3 = Promise.resolve(p2);console.log(p3); // Promise &#123; 200 &#125;console.log(p2 === p3); // true 使用Promise.reject()则是可以立即得到一个已经reject的Promise，其使用方式和Promise.resolve()类似。 Promise.all()Promise.all()接受一个Promise的数组，而且会并行地处理数组中的所有Promise： 12345678910111213141516171819202122// promiseAll.jsconst fs = require('fs')const read = filename =&gt; &#123; return new Promise((resolve, reject) =&gt; &#123; fs.readFile(filename, 'utf8', (err, data) =&gt; &#123; if (err)&#123; reject(err); &#125; resolve(data); &#125;); &#125;);&#125;;const p1 = read('a.txt');const p2 = read('b.txt');const results = Promise.all([p1, p2]);results.then(data =&gt; &#123; console.log(data); // [ 'file a content', 'file b content' ]&#125;) Promise.all()会返回一个promise，这个promise会收到一个完成消息，这是一个由所有传入的promise的完成消息组成的数组，该数组中元素的顺序与传入时的元素顺序一致，与每个promise的完成时间无关。从Promise.all()返回的这个promise只有在所有的成员promise完成后才会完成。如果这些成员promise中有一个被拒绝的话，Promise.all()返回的promise就会立即被拒绝，并丢弃所有其他promise的全部结果。 看一个例子，如果其中某个promise决议后为拒绝状态： 123456789101112131415161718192021222324252627// promiseAllWithReject.jsconst fs = require('fs')const read = filename =&gt; &#123; return new Promise((resolve, reject) =&gt; &#123; fs.readFile(filename, 'utf8', (err, data) =&gt; &#123; if (err)&#123; reject(err); &#125; resolve(data); &#125;); &#125;);&#125;;const p1 = read('a.txt');const p2 = read('b.txt');const p3 = new Promise((resolve, reject) =&gt; &#123; reject(new Error('This is an error!'));&#125;);const results = Promise.all([p1, p2, p3]);results.then(data =&gt; &#123; console.log(data); // never reach here!&#125;, err =&gt; &#123; console.error(err); // Error: This is an error!&#125;); 请记住为每个promise都关联一个拒绝处理函数。 刚才提到只有Promise.all()中的所有成员promise都已完成，其返回的promise的状态返回是已完成。也就是说，Promise.all()调用的完成时间取决于最慢完成的那个promise。一个简单的例子： 123456789101112131415161718// promiseAllTime.jsconst delay = time =&gt; &#123; return new Promise((resolve, reject) =&gt; &#123; setTimeout(resolve, time); &#125;);&#125;;const p1 = delay(1000);const p2 = delay(5000);const start = new Date().getTime();const p = Promise.all([p1, p2]);p.then(data =&gt; &#123; const end = new Date().getTime(); console.log(`Time consuming: $&#123;end - start&#125;ms`);&#125;); 运行结果： 12$ babel-node promiseAllTime.jsTime consuming: 5002ms 简而言之，Promise.all()会协调所有promise的运行。 Promise.race()Promise.race()接收一个promise数组，这些promise之间是竞争关系，哪个先完成就返回哪个： 12345678910111213141516171819202122232425262728// promiseRace.jsconst delay = time =&gt; &#123; return new Promise((resolve, reject) =&gt; &#123; setTimeout(resolve, time); &#125;);&#125;;const p1 = new Promise((resolve, reject) =&gt; &#123; delay(1000).then(data =&gt; &#123; return resolve(100); &#125;)&#125;);const p2 = new Promise((resolve, reject) =&gt; &#123; delay(5000).then(data =&gt; &#123; return resolve(200); &#125;)&#125;);const start = new Date().getTime();const p = Promise.race([p1, p2]);p.then(data =&gt; &#123; console.log(data); // 100 const end = new Date().getTime(); console.log(`Time consuming: $&#123;end - start&#125;ms`);&#125;); 这里p1和p2各延迟了1000ms和5000ms，分别返回100和200，使用Promise.race()只会得到先完成的p1的值，而p2会被丢弃。 Promise.race()的一种典型用法就是为一个可能耗时较长的异步操作设置一个超时，如果我们希望针对某个异步操作设置一个超时时间，如果超时了，就拒绝这个异步操作的状态，可以这么处理： 12345678910111213141516171819202122232425262728293031323334353637383940414243// promiseRaceTimeout.jsconst delay = time =&gt; &#123; return new Promise((resolve, reject) =&gt; &#123; setTimeout(resolve, time); &#125;);&#125;;const timeout = time =&gt; &#123; return new Promise((resolve, reject) =&gt; &#123; const err = new Error('Time out!'); setTimeout(() =&gt; &#123; reject(err); &#125;, time); &#125;);&#125;;const p1 = new Promise((resolve, reject) =&gt; &#123; delay(1000).then(data =&gt; &#123; return resolve(100); &#125;)&#125;);const p2 = new Promise((resolve, reject) =&gt; &#123; delay(5000).then(data =&gt; &#123; return resolve(200); &#125;)&#125;);const p = Promise.race([p1, timeout(3000)]);p.then(data =&gt; &#123; console.log(data); // 100&#125;, err =&gt; &#123; console.error(err);&#125;);const p_ = Promise.race([p2, timeout(3000)]);p_.then(data =&gt; &#123; console.log(data);&#125;, err =&gt; &#123; console.error(err); // Error: Time out!&#125;); 这里p1需要1000ms才能完成，p2需要5000ms，超时定时器统一设置成了3000ms，因此Promise.race([p1, timeout(3000)])会得到已经完成的p1的值（100），Promise.race([p2, timeout(3000)])会得到一个超时的结果，在then的reject中可以拿到这个异常。当然，如果在超时定时器超时之前已经有promise被拒绝的话，Promise.race()会直接变成拒绝状态。 Promise API还有其他几个变体： Promise.none() 和Promise.all()相反，要求所有promise都要被拒绝，然后将拒绝转化成完成值。 Promise.any() 会忽略拒绝，只要有一个promise完成，整体的状态即为完成。 Promise.first() 只要第一个promise完成，它就会忽略后续promise的任何完成和拒绝。 Promise.last() 类似于Promise.first()，但条件变为只有最后一个promise完成胜出。 对这个四个Promise API有兴趣的同学可以自己做做实验，这里不再深入讲解。 then()和catch()刚才已经提到过，使用then()和catch()可以形成Promise调用链，这里快速总结一下它们的使用方法： p.then(fulfilled); p.then(fulfilled, rejected); p.catch(rejected); // 等价于 p.then(null, rejected); 包装旧式异步API可能项目中有一些遗留代码还在使用旧式异步API，如果我们要将这部分代码Promise化，最好是有比较好用的工具，下面的polyfill可以帮助你Promise化旧式异步API： 12345678910111213141516// promiseWrapper.jsconst promiseWrapper = fn =&gt; &#123; return function () &#123; const args = [].slice.call(arguments); // convert arguments to a real array return new Promise((resolve, reject) =&gt; &#123; const cb = (err, data) =&gt; &#123; if (err) &#123; reject(err); &#125; else &#123; resolve(data); &#125; &#125;; fn.apply(null, args.concat(cb)); &#125;); &#125;;&#125;; 测试一下： 1234567891011// promiseWrapper.jsconst fs = require('fs');const read = promiseWrapper(fs.readFile);read('a.txt', 'utf8').then(data =&gt; &#123; console.log(data);&#125;, err =&gt; &#123; console.error(err);&#125;) read是经过Promise化的fs.readFile，调用read会返回一个Promise，一切和我们想象的一致。不过这样用有一个前提条件，原来的旧式异步API必须是error-first的，好消息是大多数Node.js核心API都是error-first的。 Promise的局限性 不可取消。 不可打断。 一经决议就不可变。 迭代器(Iterator)和生成器(Generator)ES 6中引入了生成器函数(Generator Function)。生成器函数用function *定义。它和普通函数相比有一些有意思的特性。 用一个简单的例子来展示生成器函数的工作方式： 123456789101112131415// generator.jsfunction *generator() &#123; console.log('hello'); const x = 10 * (yield 'world'); return x;&#125;;const it = generator();let res = it.next();console.log(res); // &#123; value: 'world', done: false &#125;console.log('pause here');res = it.next(4);console.log(res); // &#123; value: 40, done: true &#125; 运行结果： 12345$ babel-node generator.jshello&#123; value: 'world', done: false &#125;pause here&#123; value: 40, done: true &#125; 上面代码段定义了一个生成器函数，这里重要的是它的执行流程： 调用一个生成器函数（就像调用普通函数那样）并不会立即开始执行这个生成器内部的代码，而是返回一个它的迭代器。因此generator();实际上返回了一个迭代器。 接着let res = it.next();这行代码使生成器函数开始执行，打印hello。当遇到yield时，生成器会暂停，交出控制权。这里打印res会发现其内容为{ value: &#39;world&#39;, done: false }，value是生成器内部的yield出的值，如果yield后面没有东西，这个value就是undefined，done为false表示生成器还未执行完毕。 console.log(&#39;pause here&#39;);这行代码是我们在生成器暂停期间插入的一段执行逻辑。刚才提到，在生成器暂停期间会交出控制权，因此控制权又回到外部。 语句res = it.next(4);将使生成器继续运行，直到遇到下一个yield，而且这次传入了4，通过next()传入的值会使得yield获取这个值，所以在生成器内部x的值就是40（10*4）。再次观察res为{ value: 40, done: true }，由于生成最终返回x，所value就是40，done也变为true了，说明生成器执行完毕。 通过解析这段代码我们可以发现几个很有意思的事情： 生成器内部可以通过yield主动交出控制权，使控制权回到调用方。 yield后面可以有值，有值得yield会将这个值返回出来。 可以通过next()将值传入生成器中，该值将作为对应yield的值。 调用next()后，会获得一个结果，这个结果包含两个值，value表示当前yield的执行结果（或者return的结果）done表示生成器执行状态的信息：true/false分别表示执行完毕和还未执行完毕。 生成器通过yeild和next使得外部和生成器内部的通信称为可能。 看到这里可能有人要问了，这有什么用呢？和Promise相比有什么好处？请慢慢往下看。 还有一种场景，假设我们要获得一个无限的自然数序列，从小到大一次取出一个来用。由于自然数是无限的，我们不可能一次性用一个数组将它们都生成出来（时间上不允许，空间上也不允许），其实也没有必要。我们只需要在需要获取一个自然数的时候生成出一个就好了。这时使用生成器再合适不过： 1234567891011121314151617181920// numberGenerator.jsfunction *numberGenerator() &#123; let num = 0; while (true) &#123; yield num++; &#125;&#125;;const it = numberGenerator();let res = it.next();console.log(res); // &#123; value: 0, done: false &#125;res = it.next();console.log(res); // &#123; value: 1, done: false &#125;res = it.next();console.log(res); // &#123; value: 2, done: false &#125;res = it.next();console.log(res); // &#123; value: 3, done: false &#125; 由于生成器里面是一个无限while循环，所以done一直是false。 使用生成器函数需要注意一点，在获得生成器函数的迭代器后，第一次调用其next()方法时不需要传参数（尽管你可以这么做）。因为此时还没遇到yield，传了也没意义。 生成器函数的错误处理可以直接在生成器函数中使用try/catch捕获异常： 1234567891011121314// generatorCatchError.jsfunction *generator() &#123; try &#123; const x = (yield 'world')(); return x; &#125; catch (err) &#123; console.error(err); // TypeError: (intermediate value) is not a function &#125;&#125;;const it = generator();it.next();const res = it.next('bar');console.log(res); // &#123; value: undefined, done: true &#125; 由于上面的代码段中有异常，被catch捕获，没有显式调用return语句，所以默认返回值是undefined。 想停止一个生成器函数只需要调用其迭代器的return方法： 1234567891011121314151617181920212223// generatorStop.jsfunction *numberGenerator() &#123; let num = 0; while (true) &#123; yield num++; &#125;&#125;;const it = numberGenerator();let res = it.next();console.log(res); // &#123; value: 0, done: false &#125;res = it.next();console.log(res); // &#123; value: 1, done: false &#125;res = it.next();console.log(res); // &#123; value: 2, done: false &#125;res = it.next();console.log(res); // &#123; value: 3, done: false &#125;res = it.return(); // stop generatorconsole.log(res); // &#123; value: undefined, done: true &#125; 回想Promise部分介绍的链式Promise，虽然避免了嵌套回调问题，但是一连串.then()也让人挺烦的，如果能真正像写同步代码那样写串行异步代码那该多好。实际上使用生成器函数已经可以做到这点。但是为了更好地理解后面的内容，这里还有几个准备工作要做。我们知道在生成器函数中yield一个值的时候，外部可以通过next()拿到这个值，刚才的代码中yield后面都是立即值，如果把这个值换成一个异步函数会怎样？ 很自然地，我们会想让代码变成这样： 12345678910111213// generatorReadFileBadExample.jsconst fs = require('fs');function *generator ()&#123; var file1 = yield fs.readFile('a.txt', 'utf8'); console.log(file1); // undefined&#125;;const it = generator();let res = it.next();console.log(res); // &#123; value: undefined, done: false &#125;res = it.next();console.log(res); // &#123; value: undefined, done: true &#125; 但很可惜的是，这样做并不奏效，为什么？ 小练习思考一下为什么上面这段代码不能工作？ 解析： 其实仔细思考yield的行为就会发现，fs.readFile是一个旧式的异步API，调用它会立即返回undefined，如果没有传入一个回调函数给它，我们无法获得任何信息。那么问题来了，如果还要在生成器函数里调用fs.readFile时传入回调函数，那不是又回到解放前了吗，我们可不想再直接去面对赤裸裸地回调函数。也就是说，将fs.readFile直接在生成器内部执行是不可能的了，那么只能将fs.readFile的执行放到生成器函数外部，换句话说，我们要将fs.readFile连同它的参数通过yield传递到外部去执行，我们需要包装一下fs.readFile。将一个函数和一堆参数绑定后塞入另一个新的函数里，叫函数的柯里化(currying)，换一种更通俗易懂的讲法：我们把一堆参数固定到一个函数上。 由于在JavaScript中函数是一等对象，所以借助高阶函数的抽象功能，可以写一个帮助方法来对任意在最后一个参数上为回调函数的异步API进行柯里化： 1234567891011121314151617// thunkify.jsconst fs = require('fs');const thunkify = fn =&gt; &#123; return function() &#123; const args = [].slice.call(arguments); return (cb) =&gt; &#123; fn.apply(null, args.concat(cb)); &#125;; &#125;;&#125;;const readFile = thunkify(fs.readFile);readFile('a.txt', 'utf8')((err, data) =&gt; &#123; console.log(data); // file a content&#125;); 我们来尝试一下将柯里化后的旧式异步API和生成器函数结合使用： 1234567891011121314151617181920212223242526272829303132// generatorReadFile1.jsconst fs = require('fs');const thunkify = fn =&gt; &#123; return function() &#123; const args = [].slice.call(arguments); return (cb) =&gt; &#123; fn.apply(null, args.concat(cb)); &#125;; &#125;;&#125;;const readFile = thunkify(fs.readFile);function *generator ()&#123; const file1 = yield readFile('a.txt', 'utf8'); console.log(file1); // undefined const file2 = yield readFile('b.txt', 'utf8'); console.log(file2); // undefined&#125;;const it = generator();let res = it.next();res.value((err, data) =&gt; &#123; console.log(data); // file a content&#125;);res = it.next();res.value((err, data) =&gt; &#123; console.log(data); // file b content&#125;);res = it.next();console.log(res); // &#123; value: undefined, done: true &#125; 第一次调用next()时，我们从其value中得到了柯里化后的fs.readFile，我们叫它readFile。readFile接受一个回调函数，因此只要传入回调我们就能获得异步调用的结果。很好，我们的第一步目的达到了。但是仔细一看，还是有问题：在生成器函数中我们打印file1和file2结果都是undefined，生成器函数在交出控制权后，控制权转移到外部，异步调用也在外部完成，异步调用的结果也在外面。没关系，我们可以通过next(value)将这个异步调用结果带回给生成器函数内部： 123456789101112131415161718192021222324252627282930// generatorReadFile2.jsconst fs = require('fs');const thunkify = fn =&gt; &#123; return function() &#123; const args = [].slice.call(arguments); return (cb) =&gt; &#123; fn.apply(null, args.concat(cb)); &#125;; &#125;;&#125;;const readFile = thunkify(fs.readFile);function *generator() &#123; const file1 = yield readFile('a.txt', 'utf8'); console.log('got ' + file1); // got file a content const file2 = yield readFile('b.txt', 'utf8'); console.log('got ' + file2); // got file b content&#125;;const it = generator();let res = it.next();res.value((err, data) =&gt; &#123; res = it.next(data); res.value((err, data) =&gt; &#123; res = it.next(data); console.log(res); // &#123; value: undefined, done: true &#125; &#125;);&#125;); 非常好，我们成功地将异步调用的结果又传回给生成器函数，问题到这一步应该说已经基本解决了。说基本解决是因为调用方式还没有自动化，还需要手动一步步调用res.value(cb)，再次发挥JavaScript高阶函数的强大威力，写一个自动执行生成器函数的工具吧： 123456789101112131415161718192021222324252627282930313233343536// generatorAutoRunner.jsconst fs = require('fs');const thunkify = fn =&gt; &#123; return function() &#123; const args = [].slice.call(arguments); return (cb) =&gt; &#123; fn.apply(null, args.concat(cb)); &#125;; &#125;;&#125;;const readFile = thunkify(fs.readFile);function *generator() &#123; const file1 = yield readFile('a.txt', 'utf8'); console.log('got ' + file1); // got file a content const file2 = yield readFile('b.txt', 'utf8'); console.log('got ' + file2); // got file b content&#125;;const run = g =&gt; &#123; const it = g(); function next(err, data) &#123; const res = it.next(data); if (res.done) &#123; return res.value; &#125; res.value(next); &#125; next();&#125;;run(generator); 自动运行生成器函数的原理很简单，在生成器函数的迭代器上执行next时，返回的是一个柯里化后的异步函数，我们需要调用这个异步函数，同时传入一个参数，这个参数是一个回调函数，它是自动执行的关键，该回调函数内部在获取到结果值的时候，需要调用next方法将这个结果值带回给生成器函数内部，如此循环下去直到结束。 yield *语句普通的yield语句后面跟一个异步操作，yield 语句后面可以跟另一个可迭代对象，在实际使用中yield 后面一般要跟另一个Generator函数： 123456789101112131415161718192021222324252627282930313233343536373839404142var fs = require('fs');const thunkify = fn =&gt; &#123; return function() &#123; const args = [].slice.call(arguments); return (cb) =&gt; &#123; fn.apply(null, args.concat(cb)); &#125;; &#125;;&#125;;const readFile = thunkify(fs.readFile); function *generator() &#123; const f1 = yield readFile('a.txt', 'utf8'); console.log(f1); // file a content const f_ = yield *anotherGenerator(); //此处插入了另外一个异步流程 console.log(f_); // file c content var f3 = yield readFile('b.txt', 'utf8'); console.log(f3); // file b content&#125;; const anotherGenerator = function* ()&#123; const f = yield readFile('c.txt', 'utf8'); return f;&#125; function run(g) &#123; const it = g(); function next(err, data) &#123; const result = it.next(data); if (result.done) return; result.value(next); &#125; next();&#125; run(generator); //自动执行 在使用生成器函数作为异步控制流的时期，业界比较流行的自动执行的解决方案是co库： 123456789101112131415161718192021// co1.jsconst fs = require('fs');const co = require('co');const thunkify = fn =&gt; &#123; return function() &#123; const args = [].slice.call(arguments); return (cb) =&gt; &#123; fn.apply(null, args.concat(cb)); &#125;; &#125;;&#125;;const readFile = thunkify(fs.readFile);co(function *generator() &#123; const file1 = yield readFile('a.txt', 'utf8'); console.log('got ' + file1); // got file a content const file2 = yield readFile('b.txt', 'utf8'); console.log('got ' + file2); // got file b content&#125;); 还可以直接利用co库并发地执行一系列操作： 1234567891011121314151617181920// co2.jsconst fs = require('fs');const co = require('co');const thunkify = fn =&gt; &#123; return function() &#123; const args = [].slice.call(arguments); return (cb) =&gt; &#123; fn.apply(null, args.concat(cb)); &#125;; &#125;;&#125;;const readFile = thunkify(fs.readFile);co(function *generator() &#123; const files = ['a.txt', 'b.txt']; const res = yield files.map(file =&gt; readFile(file, 'utf8')); console.log(res); // [ 'file a content', 'file b content' ]&#125;); 还可以用yield并发地执行一个可迭代对象中的异步操作： 12345678910111213141516171819202122// coWithArray.jsconst fs = require('fs');const co = require('co');const thunkify = fn =&gt; &#123; return function() &#123; const args = [].slice.call(arguments); return (cb) =&gt; &#123; fn.apply(null, args.concat(cb)); &#125;; &#125;;&#125;;const readFile = thunkify(fs.readFile);co(function *generator() &#123; const files = ['a.txt', 'b.txt']; const results = yield* files.map(file =&gt; &#123; return readFile(file, 'utf8'); &#125;); console.log(results); // [ 'file a content', 'file b content' ]&#125;); async/await直到ES 7中出现async/await之前，业界普遍都是采用co库的方案。 async和await是ES 7中的新语法，新到连ES 6都不支持，但是可以通过Babel一类的预编译器处理成ES 5的代码。目前比较一致的看法是async和await是js对异步的终极解决方案。要注意的一个点是，await只能用在async函数中，但async函数中未必一定要有await。 立即尝试看看： 1234567891011121314151617181920212223242526272829// async.jsconst promiseWrapper = fn =&gt; &#123; return function () &#123; const args = [].slice.call(arguments); // convert arguments to a real array return new Promise((resolve, reject) =&gt; &#123; const cb = (err, data) =&gt; &#123; if (err) &#123; reject(err); &#125; else &#123; resolve(data); &#125; &#125;; fn.apply(null, args.concat(cb)); &#125;); &#125;;&#125;;const fs = require('fs');const readFile = promiseWrapper(fs.readFile); const asyncReadFile = async function () &#123; const f1 = await readFile('a.txt', 'utf8'); const f2 = await readFile('b.txt', 'utf8'); console.log(f1); // file a content console.log(f2); // file b content&#125;; asyncReadFile(); 如果不加await调用async函数，该异步函数将像旧式异步函数那样直接返回，也就是说，后面的代码不会等待该异步函数执行完毕，看一个例子： 12345678910111213141516// async-without-await.jsconst delay = time =&gt; &#123; return new Promise((resolve, reject) =&gt; &#123; setTimeout(resolve, time); &#125;);&#125;; const demo = async function () &#123; const start = new Date().getTime(); delay(5000); console.log(`$&#123;new Date().getTime() - start&#125;ms --- a`); await delay(5000); console.log(`$&#123;new Date().getTime() - start&#125;ms --- b`);&#125;; demo(); 打印结果： 120ms --- a5006ms --- b 使用事件进行异步编程除了回调函数、Promise、Generator、async/await这些异步方案以外，还有一种常见的异步方案：事件。在Node.js中使用事件编程十分简单，下面是一个示例： 123456789101112131415// event.jsvar events = require('events');var eventEmitter = new events.EventEmitter();eventEmitter.on('news', payload =&gt; &#123; console.log(payload.data);&#125;);eventEmitter.on('logout', payload =&gt; &#123; console.log(`User logout: $&#123;payload.data&#125;`);&#125;);eventEmitter.emit('news', &#123; data: 'Hello world!'&#125;);eventEmitter.emit('logout', &#123; data: 'Foo'&#125;); 事件的一大特定是它的解耦能力，事件相比方法调用的耦合度要低一些。在一些业务场景下，模块之间可以通过事件来解耦。 常用的异步编程库asyncbluebirdco 更多信息(译)深入理解Node.js的事件循环、定时器和process.nextTick()Fail Fasterror-firstPromise/A+规范柯里化(currying)","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"js","slug":"js","permalink":"https://nullcc.github.io/tags/js/"},{"name":"node","slug":"node","permalink":"https://nullcc.github.io/tags/node/"}]},{"title":"AOP in JavaScript and TypeScript","slug":"AOP_in_JavaScript_and_TypeScript","date":"2018-10-27T16:00:00.000Z","updated":"2022-04-15T03:41:13.013Z","comments":true,"path":"2018/10/28/AOP_in_JavaScript_and_TypeScript/","link":"","permalink":"https://nullcc.github.io/2018/10/28/AOP_in_JavaScript_and_TypeScript/","excerpt":"Write something about aop in JavaScript and TypeScript.","text":"Write something about aop in JavaScript and TypeScript. AOP OverviewAspect Oriented Programming (AOP), Chinese meaning is “面向切面编程”. We can separate parts of business logic with AOP to reduce coupling of them. Let’s image a very common situation, beside execute necessary automated operations, we also need to do something like logging, save screenshot when we use selenium-webdriver to do web automated testing. It is obvious that these operations is not strongly related with business logic, but we sure need them. Now the situation is that we need these functions but we no hope to include these code explicitly in modeling stage. So we want a new way to reslove it. For example, we want to recode time consuming and take a screenshot after every step in web automated testing. The simplest way is to put the code which record time consuming and take a screenshot in every step. But disadvantages of this approach is if we have many step, things will become uncontrollable. It’s impossible to maintain thousands of steps which there are lots of similar code in every step. AOP makes it possible to resolve this problem elegantly. AOP vs OOPWe are familiar with Object Oriented Programming (OOP). When we get a requirements, firstly we analyze the requirements and extract some domain models. Every domain model has its own attributes and methods. People using encapsulation, composition, inheritance, polymorphism and design patterns to building software and practice the thinking of OOP. If you have experiences about building software with OOP you will find that OOP is to model static things. In other words, OOP is for nouns. For example, we have a Employee class with attributes name, age, title and department, with methods work, takeABreak and loginAdminSystem. Attributes describe characteristics of objects, and methods are the operations objects can execute. Base on these, we can write some OO code: 1234567891011121314151617181920212223242526272829class Employee &#123; private name: string; private age: number; private title: string; private department: string; constructor(name: string, age: number, title: string, department: string) &#123; this.name = name; this.age = age; this.title = title; this.department = department; &#125; public work() &#123; // code for working... &#125; public takeABreak() &#123; // code for taking a break... &#125; public loginAdminSystem() &#123; // code for logining admin system, it's a sensitive operation &#125;&#125;const employee = new Employee('Bob', 35, 'Software Development Engineer', 'Devlopment');employee.work();employee.takeABreak(); Above code is strong related with Employee class which form the business logic. There is no doubt that, OOP is very suitable for describing objects. But sometime we may want some more “dynamic” things, such as we hope to logging while user is executing a sensitive operation. If we choose OOP implementation, we must modify the code of the sensitive operation loginAdminSystem to add logging code to it. Like this: 123456...public loginAdminSystem() &#123; // added: code for logging some information // code for logining admin system&#125;... It’s work of course, but no elegant. Actually it againsts OCP (open closed principle). Logging are not strongly correlated with the sensitive operation above. We had better do not to modify business logic to add logging feature. But how to reslove it? We can try AOP. Simplely, we can expose two sections in specific operation: one before it and another after it, then weave in other functions dynamically in runtime. That is to say AOP is for verbs. Our code will become more elegant and extendable with the cooperation of OOP and AOP. A simple example: function wrapping. Assume we have a function op, we want to logging something before and after it: 1234567891011let op = () =&gt; &#123; console.log(&apos;executing op...&apos;);&#125;;let oriOp = op;op = () =&gt; &#123; console.log(&apos;before op...&apos;); oriOp(); console.log(&apos;after op...&apos;);&#125; This time we wrap the function instead of modifying it. AOP code in project is more complex than code above. Basically we need some meta programming technique to support AOP. But the basic principle is similar with code above. It is worth mentioning that AOP is a programming concept, but not own by a specific programming language. Most of programming languages can be written in AOP way. Solution 1 - Simple Method HooksSolution 1 use hooks (before/after action) to wrap original method to new method, we put the auxiliary functions in hooks.See base driver and method hook driver. Issues: It’s difficult to handle relationship between before action and after action. For example, if we want to record time consuming of an action, the before action and after action will be: 123456789101112// before actionconst recordStartTime = async () =&gt; &#123; const start = new Date().getTime(); return start;&#125;;// after actionconst recordEndTime = async start =&gt; &#123; const end = new Date().getTime(); const consume = end - start; console.log(`time consume: $&#123;consume&#125;ms`);&#125;; and registerHooksForMethods: 123456789101112131415161718public registerHooksForMethods( methods: string[], beforeAction: Function, afterAction: Function ) &#123; const self = this; methods.forEach(method =&gt; &#123; const originalMethod = self[method]; // original method reference if (originalMethod) &#123; self[method] = async (...args) =&gt; &#123; // wrap original method const beforeActionRes = await beforeAction(); const methodRes = await originalMethod.call(self, ...args); await afterAction(beforeActionRes, methodRes); return methodRes; &#125;; &#125; &#125;); &#125; As you can see above, in registerHooksForMethods method, we have to get the return value of before action and pass it to after action which implementation is ugly and inflexible. So, we give up this solution even it may work. Solution 2 - Static Onion ModelLet’s look at an interesting model first: middleware onion model in Koa: See base driver and static onion driver. Static onion model is much better than method hook. It use onion model to reslove issues in method hook solution. We use a decorator to decorate methods: 123456789101112131415161718192021// decoratorexport const webDriverMethod = () =&gt; &#123; return (target, methodName: string, descriptor: PropertyDescriptor) =&gt; &#123; const desc = &#123; value: \"webDriverMethod\", writable: false &#125;; Object.defineProperty(target[methodName], \"__type__\", desc); &#125;;&#125;;// in BaseWebDriver class, a web driver method@webDriverMethod()public async findElement( by: By, ec: Function = until.elementLocated, timeout: number = 3000) &#123; await this.webDriver.wait(ec(by), timeout); return this.webDriver.findElement(by);&#125; Call use method to add a middleware: 123456789101112131415161718192021222324252627282930public use(middleware) &#123; const webDriverMethods = this.getWebDriverMethods(); const self = this; for (const method of webDriverMethods) &#123; const originalMethod = this[method]; if (originalMethod) &#123; this[method] = async (...args) =&gt; &#123; let result; const ctx = &#123; methodName: method, args &#125;; await middleware(ctx, async () =&gt; &#123; result = await originalMethod.call(self, ...args); &#125;); return result; &#125;; // check this: we must decorate new method every time when adding a middleware this.decorate(this[method]); &#125; &#125;&#125;private decorate(method) &#123; const desc = &#123; value: \"webDriverMethod\", writable: false &#125;; Object.defineProperty(method, \"__type__\", desc);&#125; But there is a little disadvantage: We must decorate new method every time when adding a middleware. In order to avoid this, we can wrap the method in runtime dynamically. Let’s move on to Solution 3. Solution 3 - Dynamic Onion ModelSee base driver and dynamic onion driver. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768export class DynamicOnionWebDriver extends BaseWebDriver &#123; protected webDriver: WebDriver; private middlewares = []; constructor(webDriver) &#123; super(webDriver); const methods = this.getWebDriverMethods(); const self = this; for (const method of methods) &#123; const desc = &#123; enumerable: true, configurable: true, get() &#123; if (methods.includes(method) &amp;&amp; this.compose) &#123; const ctx = &#123; // put some information in ctx if necessary methodName: method, &#125; const originFn = async (...args) =&gt; &#123; return this.methodMap[method].call(self, ...args); &#125;; const fn = this.compose(); return fn.bind(null, ctx, originFn.bind(self)); &#125; return this.methodMap[method].bind(this); &#125;, set(value) &#123; this[method] = value; &#125; &#125;; Object.defineProperty(this, method, desc); &#125; &#125; public use(middleware) &#123; if (typeof middleware !== \"function\") &#123; throw new TypeError(\"Middleware must be a function!\"); &#125; this.middlewares.push(middleware); &#125; private compose() &#123; const middlewares = this.middlewares; const self = this; return async (ctx, next, ...args) =&gt; &#123; let res; const dispatch = async i =&gt; &#123; let fn = middlewares[i]; if (i === middlewares.length) &#123; fn = next; &#125; if (!fn) &#123; return Promise.resolve(); &#125; try &#123; if (i === middlewares.length) &#123; res = await Promise.resolve(fn.call(self, ...args)); return res; &#125; return Promise.resolve(fn(ctx, dispatch.bind(null, i + 1))); &#125; catch (err) &#123; return Promise.reject(err); &#125; &#125;; await dispatch(0); return res; &#125;; &#125;&#125; Dynamic onion model is much complex than solution 1 and 2. We use Object.defineProperty to define our getter for every method which is taged by webDriverMethod decorator. The compose method is the key to organize all middlewares and the original method, method getter will call compose method when we want to get a method and finally return a wrapped method. Dynamic onion model is a little difficult to understand but it is worthy to take your time to learn it. BTW: method hook, static onion model and dynamic onion model these names is invented by myself, if you find a better way to describe them, please tell me. Example Repots-aop-example Run tests1npm test More Informations 什么是面向切面编程AOP Koa Web Framework Object.defineProperty()","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"js","slug":"js","permalink":"https://nullcc.github.io/tags/js/"},{"name":"aop","slug":"aop","permalink":"https://nullcc.github.io/tags/aop/"}]},{"title":"(译)深入理解Node.js的事件循环、定时器和process.nextTick","slug":"[译]深入理解Node.js的事件循环、定时器和process.nextTick","date":"2018-10-10T16:00:00.000Z","updated":"2022-04-15T03:41:13.023Z","comments":true,"path":"2018/10/11/[译]深入理解Node.js的事件循环、定时器和process.nextTick/","link":"","permalink":"https://nullcc.github.io/2018/10/11/[译]深入理解Node.js的事件循环、定时器和process.nextTick/","excerpt":"本文翻译自The Node.js Event Loop, Timers, and process.nextTick()。","text":"本文翻译自The Node.js Event Loop, Timers, and process.nextTick()。 事件循环是什么？事件循环通过尽可能地将操作交给内核处理来允许Node.js执行非阻塞I/O操作 —— 尽管JavaScript是单线程的。 由于大多数现在内核都是多线程的，它们可以在后台执行多个操作。当其中一个操作执行完毕时，内核会通知Node.js，以便可以将相应的回调函数加入到轮询队列中等待最终被执行。稍后我们将在本主题中详细解释这一细节。 事件循环详解当Node.js将在启动时初始化事件循环，处理输入脚本（或者进入REPL，不过这不在本文讨论范围内），在脚本中可能会异步调用API，调度定时器或者调用process.nextTick()，然后开始处理事件循环。 下图展示了事件循环的操作顺序的简化概述： 123456789101112131415161718 ┌───────────────────────────┐┌─&gt;│ timers ││ └─────────────┬─────────────┘│ ┌─────────────┴─────────────┐│ │ pending callbacks ││ └─────────────┬─────────────┘│ ┌─────────────┴─────────────┐│ │ idle, prepare ││ └─────────────┬─────────────┘ ┌───────────────┐│ ┌─────────────┴─────────────┐ │ incoming: ││ │ poll │&lt;─────┤ connections, ││ └─────────────┬─────────────┘ │ data, etc. ││ ┌─────────────┴─────────────┐ └───────────────┘│ │ check ││ └─────────────┬─────────────┘│ ┌─────────────┴─────────────┐└──┤ close callbacks │ └───────────────────────────┘ 注意：每个方框都被称为事件循环中的一个“阶段”。 每个阶段都拥有一个FIFO队列来存放将要被执行的回调函数。虽然每个阶段都有自己独特的地方，但一般情况下，当事件循环进入一个给定的阶段时，它将执行该阶段的任何特定操作，然后从该阶段维护的回调函数FIFO队列中取回调函数来执行，直到队列为空或者达到回调函数执行的最大次数为止。当队列为空或者达到了回调函数的最大执行次数，事件循环将进入下一个阶段，一直这样重复下去。 由于这些操作中的任何一个都有可能再调度更多的操作，且新事件的处理在轮询阶段需要在内核中排队，轮询事件可以在处理轮询事件时排队。因此，长时间运行的回调可以使轮询阶段运行得比计时器的阈值长得多。关于这部分内容可以查阅定时器和轮询章节了解更多细节。 注意：事件循环在Windows和Unix/Linux实现上有一个微小的差异，但这在这里并不重要。本文将讲解上面展示的最重要的七到八个步骤。 阶段概述 定时器阶段：该阶段将执行所有被setTimeout()和setInterval()调度的回调。 未解决的回调阶段：该阶段将执行那些被延迟到下一个事件循环迭代的回调。 空闲和准备阶段：该阶段只在内部被使用到。 轮询阶段：检索新的I/O事件；执行和I/O相关的回调（即除了关闭回调、被定时器调度的回调和被setImmediate()调度的回调以外的几乎所有回调）；Node.js将在适当的时候阻塞在此。 检查阶段：该阶段将执行被setImmediate()调度的回调。 关闭回调阶段：该阶段将执行一些关闭回调，比如socket.on(‘close’, …)中指定的回调。 在事件循环的每次运行的之间，Node.js会检查它是否在等待任何异步I/O或者定时器，如果没有，则彻底关闭。 事件循环中每个阶段的细节定时器一个定时器指定了执行所提供回调函数的时间阈值，而不是执行回调函数的确切时间。定时器回调会在所特定的时间过后尽可能早地被调度到。然而，操作系统的调度机制或者其他正在运行的回调可能会使这个行为被延迟。 注意：从技术上说，轮询阶段控制了定时器回调什么时候会被执行。 比如，假设你设置了一个回调函数在100毫秒后被调度执行，然后你的脚本执行了一个耗时95毫秒的异步的读文件操作： 12345678910111213141516171819202122232425const fs = require('fs');function someAsyncOperation(callback) &#123; // Assume this takes 95ms to complete fs.readFile('/path/to/file', callback);&#125;const timeoutScheduled = Date.now();setTimeout(() =&gt; &#123; const delay = Date.now() - timeoutScheduled; console.log(`$&#123;delay&#125;ms have passed since I was scheduled`);&#125;, 100);// do someAsyncOperation which takes 95 ms to completesomeAsyncOperation(() =&gt; &#123; const startCallback = Date.now(); // do something that will take 10ms... while (Date.now() - startCallback &lt; 10) &#123; // do nothing &#125;&#125;); 当事件循环进入轮询阶段时，队列是空的（因为fs.readFile()还未完成），因此事件循环将等待最快超时的那个定时器。当事件循环等待了95毫秒后，fs.readFile()读取文件完毕且将需要耗时10毫秒的回调函数添加到轮询队列中等待被执行。当回调函数执行完成，队列中已经没有其他回调需要被执行了，因此事件循环将看到距离当前时间最近的那个定时器的超时，然后回到定时器阶段以指定定时器回调。在这个示例中，你将看到定时器被调度和其回调被执行的间隔将是105毫秒。 注意：为了防止轮询阶段将事件循环饿死，libuv（实现Node.js事件循环和其他所有异步行为的C语言库）还有一个硬性的最大轮询时间限制（依赖于具体操作系统）。 未解决的回调阶段该阶段执行一些和操作系统有关的回调，比如TCP错误。例如如果一个TCP socket在尝试建立连接时收到ECONNREFUSED错误，一些*nix操作系统会等待报告这个错误。这将会在未解决的回调阶段排队等待被执行。 轮询阶段轮询阶段有两个主要的功能： 计算事件循环应该被阻塞多长事件并且对I/O执行轮询。 处理轮询队列中的事件。 当事件循环进入轮询阶段且此时没有定时器被调度时，将发生以下两件事中的其中一件： 如果轮询队列非空，事件循环将遍历轮询队列中的回调函数并同步地执行它们直到队列被耗尽，或者到达系统指定的硬性时间限制，结束轮询阶段。 如果队列为空，将发生以下两件事中的其中一件： 如果程序被setImmediate()调度，事件循环将结束轮询阶段，直接进入检查阶段以执行这些调度程序。 如果程序没有被setImmediate()调度，事件循环将等待有回调函数被加入到轮询队列中，然后直接执行它们。 一旦轮询队列为空，事件循环将检查哪些定时器超时了。如果有一个或多个定时器就绪，事件循环将会回到定时器阶段来执行那些定时器回调。 检查阶段该阶段允许用户在轮询阶段后立即执行回调。如果轮询阶段处于空闲状态且在程序中调用了setImmediate()，事件循环可以直接进入检查阶段而不是在轮询阶段中等待。 setImmediate()实际上是一个运行在事件循环的特殊阶段的定时器。它使用了一个libuv API以在轮询阶段完成后执行回调。 通常，当执行代码时，事件循环最终将到达轮询阶段，在该阶段它将等待传入的连接，请求等。然而，如果一个回调被setImmediate()调度且轮询阶段当前处于空闲状态时，轮询阶段将直接结束，立即进入检查阶段而不是继续等待和轮询事件。 关闭回调阶段如果一个socket或者句柄被突然关闭（比如使用socket.destroy()），’close’事件将在这个阶段被发射出去。否则它将通过process.nextTick()被发射。 setImmediate() vs setTimeout()setImmediate和setTimeout()类似，但它们的行为方式取决于它们什么时候被调用。 setImmediate()被设计成在当前轮询阶段结束时执行一段程序。 setTimeout()则是在到达一个超时时间后执行一段程序。 这两个方法哪个先被调用依赖于它们被调用的上下文。如果它们两个都在主模块中被调用，调用的时机将会被进程的性能所约束（会受到同主机上其他应用程序的冲击）。 比如，如果我们在I/O循环之外（例如主模块）运行下列脚本，则这两个方法的定时器被执行的先后顺序是非确定性的，因为执行时机会受到进程性能的影响： 12345678// timeout_vs_immediate.jssetTimeout(() =&gt; &#123; console.log('timeout');&#125;, 0);setImmediate(() =&gt; &#123; console.log('immediate');&#125;); 运行： 1234567$ node timeout_vs_immediate.jstimeoutimmediate$ node timeout_vs_immediate.jsimmediatetimeout 然而，如果你将它们放在I/O循环中运行，则setImmediate()的回调将总是被先执行： 1234567891011// timeout_vs_immediate.jsconst fs = require('fs');fs.readFile(__filename, () =&gt; &#123; setTimeout(() =&gt; &#123; console.log('timeout'); &#125;, 0); setImmediate(() =&gt; &#123; console.log('immediate'); &#125;);&#125;); 运行： 1234567$ node timeout_vs_immediate.jsimmediatetimeout$ node timeout_vs_immediate.jsimmediatetimeout 使用setImmediate()相比于setTimeout()的主要优势是，如果将其放入I/O循环中调度，setImmediate()的回调将总是在任何定时器之前被执行，而与有多少定时器无关。 process.nextTick()理解process.nextTick()你可能已经注意到尽管process.nextTick()是异步API的一部分，但上面的图表中并没有出现process.nextTick()。这是因为从技术上来说process.nextTick()并不是事件循环的一部分。相反地，nextTickQueue将会在当前操作完成之后立即被处理，而不管当前处于事件循环的哪个阶段。 回顾一下上面的图表，任何时刻你在一个给定的阶段调用process.nextTick()，则所有被传入process.nextTick()的回调将在事件循环继续往下执行前被执行。这可能会导致一些很糟的情形，因为它允许用户递归调用process.nextTick()来饿死I/O进程，这会导致事件循环永远无法到达轮询阶段。 为什么允许使用process.nextTick()？为什么process.nextTick()这样的API会被允许出现在Node.js中呢？一部分原因是因为设计理念，Node.js中的API应该总是异步的，即使是那些不需要异步的地方。下面的代码片段展示了一个例子： 1234function apiCall(arg, callback) &#123; if (typeof arg !== 'string') return process.nextTick(callback, new TypeError('argument should be string'));&#125; 上面的代码检查参数，如果检查不通过，它将一个错误对象传给回调。Node.js API最近进行了更新以允许向process.nextTick()中传递参数来作为回调函数的参数，而不必写嵌套函数。 我们所做的就是将一个错误传递给用户，但这只允许在用户代码被执行完毕后执行。使用process.nextTick()我们可以保证apiCall()的回调总是在用户代码被执行后，且在事件循环继续工作前被执行。为了达到这一点，JS调用栈被允许展开，然后立即执行所提供的回调，该回调允许用户对process.nextTick()进行递归调用，而不会达到RangeError：即V8调用栈的最大值。 这种设计理念会导致一些潜在的问题，观察下面的代码片段： 123456789101112let bar;// this has an asynchronous signature, but calls callback synchronouslyfunction someAsyncApiCall(callback) &#123; callback(); &#125;// the callback is called before `someAsyncApiCall` completes.someAsyncApiCall(() =&gt; &#123; // since someAsyncApiCall has completed, bar hasn't been assigned any value console.log('bar', bar); // undefined&#125;);bar = 1; 用户定义函数someAsyncApiCall()有一个异步签名，但实际上它是同步执行的。当它被调用时，提供给someAsyncApiCall()的回调函数会在与执行someAsyncApiCall()本身的同一个事件循环阶段被执行，因为someAsyncApiCall()实际上并未执行任何异步操作。结果就是，即使回调函数尝试引用变量bar，但此时在作用域中并没有改变量。因为程序还没运行到对bar赋值的部分。 通过将回调放到process.nextTick()中，程序依然可以执行完毕，且所有的变量、函数等都在执行回调之前被初始化。它还具有不会被事件循环打断的优点。这对于那些需要在事件循环继续往下执行之前报告一个错误的用户非常实用。以下是将上面的例子改用process.nextTick()的代码： 1234567891011let bar;function someAsyncApiCall(callback) &#123; process.nextTick(callback);&#125;someAsyncApiCall(() =&gt; &#123; console.log('bar', bar); // 1&#125;);bar = 1; 这里还有另一个现实中的例子： 123const server = net.createServer(() =&gt; &#123;&#125;).listen(8080);server.on('listening', () =&gt; &#123;&#125;); 当只传入一个端口号时，端口号被立即绑定。因此，可以立即调用’listening’回调。这里的问题是，.on(&#39;listening&#39;)回调将不会被设置。 为了绕过这个问题，’listening’事件被放入nextTick()的一个队列中，以允许程序运行至结束。这允许用户设置任何它们想要的事件处理程序。 process.nextTick() vs setImmediate()对于用户来说，这两个名字很相似，但它们的名字让人感到困惑。 process.nextTick()中的回调在事件循环的当前阶段中被立即执行。 setImmediate()中的回调在事件循环的下一次迭代或’tick’中被执行。 本质上，它们两个的名字应该互相调换一下。process.nextTick()的执行时机比setImmediate()要更及时，但这属于历史问题，现在已经不可改变。实施这项改变将导致很多npm包无法使用。每天都有很多新模块被加入，这意味着每等待一天，就会有更多潜在的破坏发生。虽然他们的名字相互混淆，但将它们调换名字这种事是不会发生的。 我们建议开发者在所有地方使用setImmediate()，因为它更容易理解（并且它可以使代码的兼容性更好，比如和浏览器环境的JS）。 为什么使用process.nextTick()？有两个主要原因： 允许用户处理错误，清理任何不再需要的资源，或者在事件循环继续执行之前重试请求。 有时确实需要展开调用栈，并在事件循环继续执行之前执行回调。 看一个简单的例子： 12345const server = net.createServer();server.on('connection', (conn) =&gt; &#123; &#125;);server.listen(8080);server.on('listening', () =&gt; &#123; &#125;); listen()在事件循环的开始被执行，但监听的回调却被放在setImmediate()中。除非传入主机名，否则端口绑定将立即发生。事件循环继续进行，将到达轮询阶段，这意味着连接成功事件有机会被处理。 另一个例子是执行函数构造函数，即，继承自EventEmitter且在构造函数中发射一个事件： 12345678910111213const EventEmitter = require('events');const util = require('util');function MyEmitter() &#123; EventEmitter.call(this); this.emit('event');&#125;util.inherits(MyEmitter, EventEmitter);const myEmitter = new MyEmitter();myEmitter.on('event', () =&gt; &#123; console.log('an event occurred!');&#125;); 你无法在构造函数中立即发射一个事件，因为此时程序还未运行到将回调赋值给事件的的那段代码。因此，在构造函数内部，你可以使用process.nextTick()设置一个回调以在构造函数执行完毕后发射事件，下面的代码满足我们的预期： 1234567891011121314151617const EventEmitter = require('events');const util = require('util');function MyEmitter() &#123; EventEmitter.call(this); // use nextTick to emit the event once a handler is assigned process.nextTick(() =&gt; &#123; this.emit('event'); &#125;);&#125;util.inherits(MyEmitter, EventEmitter);const myEmitter = new MyEmitter();myEmitter.on('event', () =&gt; &#123; console.log('an event occurred!');&#125;);","categories":[{"name":"文档翻译","slug":"文档翻译","permalink":"https://nullcc.github.io/categories/文档翻译/"}],"tags":[{"name":"node","slug":"node","permalink":"https://nullcc.github.io/tags/node/"}]},{"title":"git常用命令","slug":"git常用命令","date":"2018-06-12T16:00:00.000Z","updated":"2022-04-15T03:41:13.024Z","comments":true,"path":"2018/06/13/git常用命令/","link":"","permalink":"https://nullcc.github.io/2018/06/13/git常用命令/","excerpt":"收集git常用命令。","text":"收集git常用命令。 git loggit reset –hard [commit_id] 撤销到某次提交","categories":[{"name":"git","slug":"git","permalink":"https://nullcc.github.io/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://nullcc.github.io/tags/git/"}]},{"title":"广度优先搜索(BFS)和深度优先搜索(DFS)","slug":"广度优先搜索(BFS)和深度优先搜索(DFS)","date":"2018-06-06T16:00:00.000Z","updated":"2022-04-15T03:41:13.034Z","comments":true,"path":"2018/06/07/广度优先搜索(BFS)和深度优先搜索(DFS)/","link":"","permalink":"https://nullcc.github.io/2018/06/07/广度优先搜索(BFS)和深度优先搜索(DFS)/","excerpt":"本文将解析广度优先搜索(BFS)和深度优先搜索(DFS)。","text":"本文将解析广度优先搜索(BFS)和深度优先搜索(DFS)。 广度优先搜索(BFS)BFS即Breadth First Search，是最简便的一种图的搜索算法。BFS是一种盲目搜索算法，目的是以一种指定的顺序系统地展开并检查图中的所有节点，以寻找某种结果。换句话说，BFS并不考虑结果的位置，而是彻底搜索整张图以寻找结果。 我们通过遍历一颗二叉树来感受一下BFS是什么样的。假设一颗二叉树如下： 使用BFS对它进行遍历，遍历的顺序如下图： 上图很形象地说明了BFS的特点，还可以发现一个很有趣的事情是实际上对一颗二叉树做BFS就是以层级来遍历它：从上到下，从左到右地遍历一颗二叉树。我们可以利用队列的先进先出特性实现BFS，代码如下： 1234567891011# bfs.pydef bfs(root, fn): q = list() q.append(root) while len(q) &gt; 0: node = q.pop(0) fn(node) for child in node.get_children(): q.append(child) 测试代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# test_bfs.pyimport unittestfrom bfs.bfs import bfsclass Node: def __init__(self, val): self.val = val self.left = None self.right = None def get_children(self): children = [] if self.left is not None: children.append(self.left) if self.right is not None: children.append(self.right) return childrenclass BFSTest(unittest.TestCase): def test_bfs_using_binary_tree(self): root = Node('root') node_1 = Node('node_1') node_2 = Node('node_2') node_3 = Node('node_3') node_4 = Node('node_4') node_5 = Node('node_5') node_6 = Node('node_6') node_7 = Node('node_7') node_8 = Node('node_8') # root # / \\ # 1 2 # / \\ / \\ # 3 4 5 6 # / \\ # 7 8 root.left = node_1 root.right = node_2 node_1.left = node_3 node_1.right = node_4 node_2.left = node_5 node_2.right = node_6 node_3.left = node_7 node_3.right = node_8 output = [] def fn(node): output.append(node.val) bfs(root, fn) self.assertEqual(['root', 'node_1', 'node_2', 'node_3', 'node_4', 'node_5', 'node_6', 'node_7', 'node_8'], output) 深度优先搜索(DFS)DFS即Depth First Search，和BFS同属于基础的图论算法。其基本思想是对一张图的每一个可能路径深入到不能再深入为止，且路径上的每个节点只访问一次。DFS同样也不考虑结果的位置，而是彻底搜索整张图以寻找结果。 还是以之前那颗二叉树为例，看一下DFS是如何做的： 以下是DFS的递归和非递归解法，其中非递归解法利用了栈的后进先出特性： 123456789101112131415161718192021222324252627282930313233343536373839404142434445from collections import defaultdictclass Stack: def __init__(self): self._stack = [] def push(self, element): self._stack.insert(0, element) def pop(self): return self._stack.pop(0) def get_top(self): return self._stack[0] def is_empty(self) -&gt; int: return len(self._stack) == 0 def size(self) -&gt; int: return len(self._stack)def dfs_recursion(root, fn): visited = defaultdict(bool) visited[root] = True fn(root) for child in root.get_children(): if not visited.get(child): dfs_recursion(child, fn)def dfs_stack(root, fn): stack = Stack() stack.push(root) visited = defaultdict(bool) visited[root] = True fn(root) while not stack.is_empty(): node = stack.get_top() for child in node.get_children(): if not visited.get(child): fn(child) visited[child] = True stack.push(child) else: stack.pop() 测试代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import unittestfrom dfs.dfs import dfs_recursionclass Node: def __init__(self, val): self.val = val self.left = None self.right = None def get_children(self): children = [] if self.left is not None: children.append(self.left) if self.right is not None: children.append(self.right) return childrenclass DFSTest(unittest.TestCase): def test_dfs_recursion_using_binary_tree(self): root = Node('root') node_1 = Node('node_1') node_2 = Node('node_2') node_3 = Node('node_3') node_4 = Node('node_4') node_5 = Node('node_5') node_6 = Node('node_6') node_7 = Node('node_7') node_8 = Node('node_8') # root # / \\ # 1 2 # / \\ / \\ # 3 4 5 6 # / \\ # 7 8 root.left = node_1 root.right = node_2 node_1.left = node_3 node_1.right = node_4 node_2.left = node_5 node_2.right = node_6 node_3.left = node_7 node_3.right = node_8 output = [] def fn(node): output.append(node.val) dfs_recursion(root, fn) self.assertEqual(['root', 'node_1', 'node_3', 'node_7', 'node_8', 'node_4', 'node_2', 'node_5', 'node_6'], output) def test_dfs_stack_using_binary_tree(self): root = Node('root') node_1 = Node('node_1') node_2 = Node('node_2') node_3 = Node('node_3') node_4 = Node('node_4') node_5 = Node('node_5') node_6 = Node('node_6') node_7 = Node('node_7') node_8 = Node('node_8') # root # / \\ # 1 2 # / \\ / \\ # 3 4 5 6 # / \\ # 7 8 root.left = node_1 root.right = node_2 node_1.left = node_3 node_1.right = node_4 node_2.left = node_5 node_2.right = node_6 node_3.left = node_7 node_3.right = node_8 output = [] def fn(node): output.append(node.val) dfs_recursion(root, fn) self.assertEqual(['root', 'node_1', 'node_3', 'node_7', 'node_8', 'node_4', 'node_2', 'node_5', 'node_6'], output)","categories":[{"name":"算法","slug":"算法","permalink":"https://nullcc.github.io/categories/算法/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://nullcc.github.io/tags/算法/"}]},{"title":"常用正则表达式收集","slug":"常用正则表达式收集","date":"2018-05-02T16:00:00.000Z","updated":"2022-04-15T03:41:13.033Z","comments":true,"path":"2018/05/03/常用正则表达式收集/","link":"","permalink":"https://nullcc.github.io/2018/05/03/常用正则表达式收集/","excerpt":"本文对常用的正则表达式进行收集和整理。","text":"本文对常用的正则表达式进行收集和整理。 匹配IP地址IPv4地址1^(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$ 匹配hostname1^(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])\\.)*([A-Za-z0-9]|[A-Za-z0-9][A-Za-z0-9\\-]*[A-Za-z0-9])$ 匹配Email地址1^(([^&lt;&gt;()[\\]\\\\.,;:\\s@\\&quot;]+(\\.[^&lt;&gt;()[\\]\\\\.,;:\\s@\\&quot;]+)*)|(\\&quot;.+\\&quot;))@((\\[[0-9]&#123;1,3&#125;\\.[0-9]&#123;1,3&#125;\\.[0-9]&#123;1,3&#125;\\.[0-9]&#123;1,3&#125;\\])|(([a-zA-Z\\-0-9]+\\.)+[a-zA-Z]&#123;2,&#125;))$ 匹配日期和时间12小时制hh:mm1^(1[0-2]|0?[1-9]):([0-5]?[0-9])$ 12小时制hh:mm:ss1^(1[0-2]|0?[1-9]):([0-5]?[0-9]):([0-5]?[0-9])$ 24小时制hh:mm1^(2[0-3]|[01]?[0-9]):([0-5]?[0-9])$ 24小时制hh:mm:ss1^(2[0-3]|[01]?[0-9]):([0-5]?[0-9]):([0-5]?[0-9])$ ISO 8601日期和时间格式1^(?&lt;year&gt;-?(?:[1-9][0-9]*)?[0-9]&#123;4&#125;)-(?&lt;month&gt;1[0-2]|0[1-9])-(?&lt;day&gt;3[0-1]|0[1-9]|[1-2][0-9])T(?&lt;hour&gt;2[0-3]|[0-1][0-9]):(?&lt;minute&gt;[0-5][0-9]):(?&lt;second&gt;[0-5][0-9])(?&lt;ms&gt;\\.[0-9]+)?(?&lt;timezone&gt;Z|[+-](?:2[0-3]|[0-1][0-9]):[0-5][0-9])?$","categories":[{"name":"正则表达式","slug":"正则表达式","permalink":"https://nullcc.github.io/categories/正则表达式/"}],"tags":[{"name":"正则表达式","slug":"正则表达式","permalink":"https://nullcc.github.io/tags/正则表达式/"}]},{"title":"Dockerfile指令解析","slug":"Dockerfile指令解析","date":"2018-04-12T16:00:00.000Z","updated":"2022-04-15T03:41:13.014Z","comments":true,"path":"2018/04/13/Dockerfile指令解析/","link":"","permalink":"https://nullcc.github.io/2018/04/13/Dockerfile指令解析/","excerpt":"本文主要记录Dockerfile指令的使用方式","text":"本文主要记录Dockerfile指令的使用方式 Dockerfile指令RUN 运行命令COPY 复制文件ADD 复制文件并解压CMD 容器启动命令ENRTYPOINT 容器启动入口点ENV 设置环境变量ARG 构建参数VOLUME 指定匿名卷EXPOSE 声明暴露端口WORKDIR 指定工作目录USER 指定当前用户HEALTHCHECK 健康检查ONBUILD 以当前镜像为基础镜像构建下一级镜像时执行具体指令 RUN 运行命令指令格式： 1RUN &lt;命令&gt; RUN指令很好理解，就是在镜像内执行命令，如果有多个命令需要执行，可以用&amp;&amp;相连，这样可以防止每个命令用一个RUN导致镜像层数过多。Union FS有最大层数限制，比如 AUFS，曾经是最大不得超过42层，现在是不得超过127层。 COPY 复制文件指令格式： 12COPY &lt;源路径&gt;... &lt;目标路径&gt;COPY [\"&lt;源路径1&gt;\",... \"&lt;目标路径&gt;\"] COPY用于将构建上下文中的&lt;源路径&gt;的文/目录复制到镜像内的&lt;目标路径&gt;，源路径可以有多个，&lt;目标路径&gt;可以是容器内的绝对路径，也可以是相对于工作目录（用WORKDIR指令指定的）的相对路径。目标路径无需事先创建，如果目标路径不存在会自动创建。 需要注意的是，使用COPY指令复制文件，源文件的各种属性如读写执行权限、文件创建时间等都会保留，等于是原封不动地将文件复制过去。 ADD 复制文件并解压指令格式： 12ADD &lt;源路径&gt;... &lt;目标路径&gt;ADD [\"&lt;源路径1&gt;\",... \"&lt;目标路径&gt;\"] ADD指令和COPY指令类似，都可以将文件复制到镜像内，但如果源文件是一个压缩文件时，复制到镜像后会自动解压。在实践中，如果只是希望复制文件，就使用COPY指令，如果希望解复制并压缩一个压缩文件到镜像，就使用ADD。 CMD 容器启动命令12CMD &lt;命令&gt;CMD [\"可执行文件\", \"参数1\", \"参数2\"...] CMD指令用于指定默认的容器主进程的启动命令。 ENRTYPOINT 容器启动入口点指令格式： 12ENRTYPOINT &lt;命令&gt;ENRTYPOINT [\"可执行文件\", \"参数1\", \"参数2\"...] 当指定ENRTYPOINT后，CMD的内容将作为参数传递给ENRTYPOINT。执行时，变为： &lt;ENTRYPOINT&gt; &quot;&lt;CMD&gt;&quot; ENV 设置环境变量指令格式： 12ENV &lt;key&gt; &lt;value&gt;ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;... ENV用来设置环境变量，在设置某个环境变量后，之后的指令可以引用这个环境变量。 ARG 构建参数指令格式： 1ARG &lt;key&gt;[=&lt;value&gt;] ARG和ENV类似，都是指定环境变量，但ARG是指定构建时的环境变量，在容器运行时是不存在这些环境变量的。 VOLUME 指定匿名卷指令格式： 12VOLUME [\"&lt;路径1&gt;\", \"&lt;路径2&gt;\"...]VOLUME &lt;路径&gt; 我们要保持容器的无状态，因此不应该向容器的存储层写入任何需要持久化的数据。为此可以事先指定某些目录挂载为匿名卷，然后再启动容器时通过-v参数将宿主机的命名卷挂载到容器的匿名卷上。之后容器将数据写入这个匿名卷实际就是将数据写入宿主机的这个命名卷上。 EXPOSE 声明暴露端口指令格式： 1EXPOSE &lt;端口1&gt; [&lt;端口2&gt;...] EXPOSE指令只是声明容器运行时暴露的端口，它并不会去真正暴露端口，这仅仅是一个声明。这可以帮助镜像使用者了解该镜像端口暴露的情况，在运行容器时可以使用-p参数指定宿主机到容器的端口映射。 WORKDIR 指定工作目录指令格式： 1WORKDIR &lt;工作目录路径&gt; WORKDIR指令用来指定工作目录，指定后各层的工作目录就被更改为指定的目录。如果该目录还不存在，会自动建立该目录。 USER 指定当前用户指令格式： 1USER &lt;用户名&gt; USER指令可以改变以后的层执行命令时使用的用户，当然这个用户必须是事先建立好的，否则无法进行用户指定。 HEALTHCHECK 健康检查指令格式： 12HEALTHCHECK [选项] CMD &lt;命令&gt;：设置检查容器健康状况的命令HEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令 HEALTHCHECK支持下列选项： –interval=&lt;间隔&gt;：两次健康检查的间隔，默认为30秒；–timeout=&lt;时长&gt;：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认30秒；–retries=&lt;次数&gt;：当连续失败指定次数后，则将容器状态视为 unhealthy，默认3次。 ONBUILD 以当前镜像为基础镜像构建下一级镜像时执行具体指令指令格式： 1ONBUILD &lt;其它指令&gt; ONBUILD指令后面指定的指令将在以当前镜像为基础构建下一级镜像时才会去执行。","categories":[{"name":"docker","slug":"docker","permalink":"https://nullcc.github.io/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://nullcc.github.io/tags/docker/"}]},{"title":"Linux下好用的命令收集","slug":"Linux下好用的命令收集","date":"2018-04-12T16:00:00.000Z","updated":"2022-04-15T03:41:13.014Z","comments":true,"path":"2018/04/13/Linux下好用的命令收集/","link":"","permalink":"https://nullcc.github.io/2018/04/13/Linux下好用的命令收集/","excerpt":"本文主要记录一些Linux下好用的命令，主要是个人在日常使用中经常使用的，此文会不断更新","text":"本文主要记录一些Linux下好用的命令，主要是个人在日常使用中经常使用的，此文会不断更新 系统信息arch 机器的处理器架构uname -m 机器的处理器架构uname -r 内核版本cat /proc/cpuinfo 显示cpu info的信息cat /proc/version 显示内核版本的详细信息date 显示系统日期cal 显示日历cal [year] 显示具体年份的日历 系统启动、重启和关机shutdown -h now 关闭系统init 0 关闭系统telinit 0 关闭系统shutdown -h hours:minutes &amp; 按预定时间关闭系统shutdown -c 取消按预定时间关闭系统shutdown -r now 重启reboot 重启logout 注销 文件和目录cd .. 返回上一级目录cd - 返回上次所在的目录pwd 查看当前所在目录ls -l 显示文件和目录的详细信息ls -a 显示所有文件和目录(包括隐藏文件)mkdir [dir] 创建一个目录mkdir [dir1] [dir2] 创建多个目录mkdir -p [path to dir] 创建一个目录树rm -f [file] 删除一个文件rm -rf [dir] 删除一个目录mv [file1] [file2] 重命名文件file1为file2cp [file1] [file2] 复制文件cp -a [dir1] [dir2] 复制目录cp -a [dir] . 复制一个目录到当前工作目录ln -s [file] [link] 创建一个指向文件或目录的软链接ln [file] [link] 创建一个指向文件或目录的物理链接(硬链接)scp /opt/soft/nginx-0.5.38.tar.gz root@10.10.10.10:/opt/soft/scptest 上传本地文件到远程机器指定目录scp -r /opt/soft/mongodb root@10.10.10.10:/opt/soft/scptest 上传本地目录到远程机器指定目录scp root@10.10.10.10:/opt/soft/nginx-0.5.38.tar.gz /opt/soft/ 从远程机器复制文件到本地scp -r root@10.10.10.10:/opt/soft/mongodb /opt/soft/ 从远程机器复制目录到本地 文件查找find / -name [file] 从/开始查找指定的文件find / -user [user] 从/开始查找属于指定用户的文件或目录find / -name *.zip 从/开始查找以.zip结尾的文件which cd 显示一个二进制文件或可执行文件的完整路径whereis cd 显示一个二进制文件、源码和man的位置grep -rnw [/path/to/folder] -e ‘patten’ 在指定目录下递归查找匹配模式 磁盘空间df -h 显示已挂载的分区列表du -sh * 查看当前目录下文件/文件夹的大小 用户和组useradd [user] 创建一个新用户passwd [user] 修改指定用户的密码(只有root用户可以运行)groupadd [group] 创建一个新的用户组groupdel [group] 删除指定用户组groupmod -n [new group name] [old group name] 重命名一个用户组useradd -G [group] [user] 创建一个新用户并把他加入指定用户组id [user] 显示用户的用户ID(uid)和组ID(gid) 查看文件内容cat [file] 从第一个字节开始正向查看文件内容tac [file] 从最后一个字节开始反向查看文件内容more [file] 查看长文件的内容(space显示下一屏，Enter显示下一行)less [file] 和more很相似，但less允许用户向前后向后浏览文件(PageUp向上翻页，PageDown向下翻页)head -2 [file] 查看一个文件的前两行tail -2 [file] 查看一个文件的最后两行tail -f [file] 实时查看被追加到指定文件的内容(常用来实时打印日志) 进程ps -ef 以全格式显示所有进程ps -ef | grep java 显示所有java进程ps aux 和ps -ef类似，只不过ps aux是BSD风格，ps -ef是System V风格，且aux会截断command列，而-ef不会，一般推荐使用ps -efkill -9 [pid] 强制终止一个进程pstree 树形显示进程lsof -i:[port] 查看占用指定端口的进程 网络ifconfig eth0 显示一个以太网卡的配置ifup eth0 启用eth0网络设备ifdown eth0 禁用eth0网络设备nslookup [host] 查询host的IP地址 I/O重定向/dev/null 空设备文件0 stdin标准输入1 stdout标准输出2 stderr标准错误 > 标准输出重定向，覆盖原文件内容>&gt; 标准输出重定向，不覆盖原文件内容，追加写入2&gt; 错误输出重定向，覆盖原文件内容2&gt;&gt; 错误输出重定向，不覆盖原文件内容，追加写入2&gt;&amp;1 将标准错误重定向到标准输出 misc!! 显示上一次执行的命令cat /etc/shells 查看存在的shellecho $SHELL 查看正在使用的shellchsh -s /bin/${shell_type} 改变当前用户默认使用的shell类型","categories":[{"name":"Linux","slug":"Linux","permalink":"https://nullcc.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://nullcc.github.io/tags/Linux/"}]},{"title":"docker常用命令","slug":"docker常用命令","date":"2018-04-10T16:00:00.000Z","updated":"2022-04-15T03:41:13.023Z","comments":true,"path":"2018/04/11/docker常用命令/","link":"","permalink":"https://nullcc.github.io/2018/04/11/docker常用命令/","excerpt":"本文主要记录docker常用命令","text":"本文主要记录docker常用命令 查看docker基本信息docker version 当前docker版本的详细信息 docker -v 当前docker版本的简略（只显示版本号和发布号） docker info \bdocker当前状态 容器\bdocker ps\b 查看当前正在运行容器的状态 \bdocker ps -a\b 查看所有容器的状态 \bdocker rm [container]\b 删除指定容器 docker diff [container] 查看容器存储层的改动 docker logs [container] 查看容器的日志 docker logs -f [container] 持续输出容器的日志 docker volume prune 清理无主的数据卷 镜像\bdocker search [image]\b 搜索镜像 \bdocker pull [image]\b pull镜像到本地 \bdocker push [image]\b push镜像到hub \bdocker images\b或docker image ls 列出所有本地顶层镜像 \bdocker images -a\b或docker image ls -a 列出所有本地镜像（包括顶层镜像和中间层镜像） \bdocker images [image]:[image tag]或\b\bdocker image ls [image]:[image tag] 列出本地所有指定的镜像名（tag为可选） \bdocker rmi [image]\b或docker image rm [image] 删除指定镜像（注意：删除某个镜像前必须将依赖该镜像的所有容器先删除，否则会报错） docker rmi $(docker image ls -q [image]) 删除指定的所有镜像 docker history [image] 查看镜像的历史记录 构建d\bocker build -t [user]/[repo]:[version] .\b 使用当前目录下的Dockerfile（默认，也可以用-f参数指定一个Dockerfile）构建一个名为[user]/[repo]:[version]的镜像，也可以不填用户名：docker build -t [repo]:[version] . d\bocker build -t [git repo url] [user]/[repo]:[version] 从git repo url构建 运行\bdocker start [container]\b 启动指定容器 \bdocker stop [container]\b 停止指定容器 \bdocker restart [container]\b 重启指定容器 \bdocker pause [container]\b 暂停容器中所有的进程 \bdocker unpause [container]\b 恢复容器中所有的进程 docker container prune 清理所有处于终止状态的容器 \bdocker attach [container] 进入某个容器（使用exit退出后容器也会跟着停止运行） \bdocker exec -ti [container] /bin/bash\b 启动一个伪终端以交互式的方式进入某个容器（使用exit退出后容器不停止运行，当需要进入容器时推荐使用exec） \bdocker run -it [image] /bin/bash\b 运行镜像并运行在交互模式（-it是两个选项，-i表示运行在交互模式，-t表示终端），在容器bash中使用exit会退出并关闭容器，如果想退出交互模式但不关闭容器，可以使用ctrl+p ctrl+q，如果想在退出容器时删除容器以节省空间的话，可以带上–rm参数 \bdocker commit -m &quot;[commit message]]&quot; -a &quot;[user name]&quot; [container id] [user name]/[image]:[image tag]\b 进入一个容器后做修改，可以进行commit来构建一个新的镜像以保存这些修改，之后如果有需要可以在新镜像的基础上继续构建。-m表示本次提交的附带信息，-a表示提交的用户 \bdocker run -d -p [host port]:[container port] -v [host dir]:[container dir] --name [container name] [image]:[image tag]\b -d表示以deamon模式运行容器，-p表示将宿主机端口(host port)映射到容器端口(container port)，注意-p可以多次使用来映射多个端口，-v表示将宿主机的目录(host dir)映射到容器目录(container dir)，之所以需要映射目录是因为容器一旦退出，容器层面上的存储就会消失，容器的存储层是无状态的，它本身不能用来持久化任何数据，所以需要将需要持久化的数据映射到宿主机上，–name表示容器名称，注意后面的镜像名(image name)和镜像标签(image tag)都要有 docker-composedocker-compose --version 查看docker-compose版本docker-compose up 运行compose项目docker-compose up --no-deps -d [service name] 重新创建服务并后台停止旧服务，启动新服务，且并不会影响到其所依赖的服务 centos 7启动、停止和重启dockersudo systemctl start docker 启动docker sudo systemctl stop docker 停止docker sudo systemctl restart docker 重启docker 添加源修改/etc/docker/daemon.json： { &quot;registry-mirrors&quot;: [&quot;http://hub-mirror.c.163.com&quot;] } 之后重启docker即可。","categories":[{"name":"docker","slug":"docker","permalink":"https://nullcc.github.io/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://nullcc.github.io/tags/docker/"}]},{"title":"CentOS7搭建FTP Server","slug":"CentOS搭建FTP Server","date":"2018-03-14T16:00:00.000Z","updated":"2022-04-15T03:41:13.014Z","comments":true,"path":"2018/03/15/CentOS搭建FTP Server/","link":"","permalink":"https://nullcc.github.io/2018/03/15/CentOS搭建FTP Server/","excerpt":"本文主要记录CentOS下FTP Server的安装和配置流程。","text":"本文主要记录CentOS下FTP Server的安装和配置流程。 安装vsftpdyum install -y vsftpd 启动vsftpdservice vsftpd start 运行下面的命令： netstat -nltp | grep 21 我们可以看到vsftpd监听在21端口了： 此时直接访问ftp://ip（ip要换成你服务器的ip）就可以看到FTP的目录了： 创建ftp用户创建一个用户： useradd ftpuser 对其设置密码： passwd ftpuser 限制该用户只能通过FTP访问服务器，而不能登录： usermod -s /sbin/nologin ftpuser ftp配置vsftpd的配置目录为/etc/vsftpd。目录中文件含义如下： vsftpd.conf 为主要配置文件 ftpusers 配置禁止访问 FTP 服务器的用户列表 user_list 配置用户访问控制 创建一个欢迎文件： echo &quot;Welcome to use FTP service.&quot; &gt; /var/ftp/welcome.txt 设置目录权限： chmod a-w /var/ftp &amp;&amp; chmod 777 -R /var/ftp/pub 设置该用户的主目录： usermod -d /var/ftp ftpuser 还需要关闭SELinux： setenforce 0 之后就可以通过FTP上传软件上传文件并访问了。","categories":[{"name":"环境配置","slug":"环境配置","permalink":"https://nullcc.github.io/categories/环境配置/"}],"tags":[{"name":"环境配置","slug":"环境配置","permalink":"https://nullcc.github.io/tags/环境配置/"}]},{"title":"短链接服务原理","slug":"短链接服务原理","date":"2018-03-13T16:00:00.000Z","updated":"2022-04-15T03:41:13.037Z","comments":true,"path":"2018/03/14/短链接服务原理/","link":"","permalink":"https://nullcc.github.io/2018/03/14/短链接服务原理/","excerpt":"本文介绍短链接服务的实现原理。","text":"本文介绍短链接服务的实现原理。 很多人都用过短链接服务，简单来说就是把一个很长的URL转换成一个很短的URL。比如我们使用百度的短链接服务http://www.dwz.cn/，对https://nodejs.org/en/docs/guides/getting-started-guide/这个URL进行短链接处理，会得到http://www.dwz.cn/7A3VlG，可以看到URL缩短了很多。 需求 给定一个原始URL，将其转换为一个短链接（啊废话！），并且需要非常大短链接地址池（近乎无限）。 原始URL和其短链接要能双向转换。 短链接永久有效。 短链接服务要能够承受较大的并发量。 服务的可扩展性。 设计原始URL-&gt;短链接首先是将原始URL转换为短链接，先考虑一下短链接长什么样。假设我们有一个短链接域名t.me，这是我乱编的一个域名。很容易发现可以使用[0-9a-zA-Z]一共62个字符来构建短链接，也就是一个62进制的计数方式。比如短链接可以可以是这样样子：t.me/xyz、t.me/p6H，诸如此类。 可能有同学会说这里的转换可以用hash，计算原始URL的hash值，然后映射到一个范围内，比如[0, 2^64-1]，然后将映射值转换成62进制的形式就得到对应的短链接了。这种方式乍一看很合理，但实际上这种做法忽略了一个事实，hash存在冲突问题，当然我们可以用一些手段去解决hash冲突，但是这无疑增加了系统复杂性。 其实有更简单方便的方式来做这个转换，可以利用自然数递增这个天然特性。例如，设计一个发号策略，对第1个原始URL发号0，第2个原始URL发号1，第100个原始URL发号100，以此类推。由于自然数是无限的，因此我们能够发的号也是无限的。发号后，将这个自然数转换成62进制就得到我们的短链接了。比如第0个原始URL对应的短链接就是t.me/0，第1个原始URL对应t.me/1，第10个原始URL对应t.me/a，第36个原始URL对应t.me/A，第100个原始URL对应t.me/1C，以此类推。 另外，还需要在DB中存储原始URL和短链接的对应关系，例如使用MySQL。 原始URL和短链接的双向转换从原始URL到短链接的转换机制已经设计好了，对于从短链接到原始URL的转换也很简单，只需要将原始URL和短链接的关系存储起来即可。比如可以在为原始URL创建短链接时，将对应关系写入K-V存储，比如Redis或者memcached，将短链接做key，原始URL做value。之后当用户使用短链接访问时，我们立刻可以查询到其对应的原始URL，然后将用户302重定向到这个原始URL。 短链接永久有效短链接永久有效主要是用户体验问题，用户肯定不希望一个短链接只有很短的有效期。 在原始URL-&gt;短链接部分，我们最后将原始URL和短链接的对应关系持久化存储了，这可以保证我们一定能通过一个存在的短链接找到它对用的原始链接。 但是每次根据短链接查找原始URL都要查一次DB效率太低，尤其是并发量大的时候。刚才在原始URL和短链接的双向转换部分将短链接和原始URL的关系写入K-V存储，这个查询可以在内存中完成，效率很高。但是单实例的K-V存储的数据量是有限的，就算使用集群，相对于近乎无尽的原始URL，储存量也还是有限。所以在不影响用户使用的情况下K-V存储的原始URL和短链接对应关系需要一个淘汰机制。我们可以对一个原始URL和短地址的key-value设置过期时间，比如1小时。当key-value还未过期时，每次用户访问短地址都会将该key-value的过期时间再次更新为1小时。 如果某个key-value被淘汰之后又有用户使用该短链接查询，此时我们可以再去DB中查找对应关系，将用户重定向，同时将这个对应关系写一份缓存到K-V存储。 应对大并发量之前提到发号器，可以使用Redis来做，设置一个key原始值为0，每过来一个原始URL就对其发放一个值，然后发号器的值自增1。\b这里就有一个单点问题，如果这个节点挂了，服务整体也就挂了，一个发号器发号大并发量扛不住。 可以设计N个发号器，比如100个。这100个发号器的初始值从0-99，每次发号后自增值为N。例如，0号发号器的初始值为0，每次自增100，发号序列就是0，100，200，300…，1号发号器的初始值为1，每次自增100，发号序列就是1，101，201，301…，99号发号器初始值为99，每次自增100，发号序列就是99，199，299，399…。 这样各个发号器都在一个固定的数值序列上发号，如果后期想继续扩展也简单，多设置发号器就好了。 可扩展性如果整个系统的并发量和数据量都很大，或者可以预见未来增长很大。为了高可扩展性，K-V存储和持久化都需要使用集群做数据分片。然后使用一些路由策略将数据分布到每台具体实例上。具体的数据分片方法这里就先不描述了，数据分片这个话题本身都可以单独成文。 整体架构设计 流程图","categories":[{"name":"web后端","slug":"web后端","permalink":"https://nullcc.github.io/categories/web后端/"}],"tags":[{"name":"web后端","slug":"web后端","permalink":"https://nullcc.github.io/tags/web后端/"}]},{"title":"CentOS7下常用软件安装","slug":"CentOS7下常用软件安装","date":"2018-03-12T16:00:00.000Z","updated":"2022-04-15T03:41:13.014Z","comments":true,"path":"2018/03/13/CentOS7下常用软件安装/","link":"","permalink":"https://nullcc.github.io/2018/03/13/CentOS7下常用软件安装/","excerpt":"本文主要记录CentOS7下常用软件安装的流程。","text":"本文主要记录CentOS7下常用软件安装的流程。 安装基础编译工具：yum install gcc gcc-c++ 安装Git：yum install -y git 安装Redis：wget http://download.redis.io/releases/redis-4.0.8.tar.gz # 版本号可根据需要改变 tar xzf redis-4.0.8.tar.gz cd redis-4.0.8 make make install 上述命令执行后，会在/usr/local/bin目录下生成下面几个可执行文件，它们的作用分别是： redis-server：Redis服务器端启动程序 redis-cli：Redis客户端操作工具 redis-benchmark：Redis性能测试工具 redis-check-aof：AOF文件修复工具 redis-check-dump：检查导出工具 接着我们把Redis的配置文件拷贝到/etc下，/etc目录在UNIX-like系统中一般用来放各种配置文件。执行： cp redis.conf /etc/ 之后修改配置文件将Redis设置为守护进程启动： vim /etc/redis.conf 修改如下： daemonize yes 然后使用刚才修改过的配置文件启动Redis： redis-server /etc/redis.conf 最后检查Redis运行情况： ps -ef | grep redis 看到类似输出表示Redis启动没毛病： root 26935 1 0 11:02 ? 00:00:00 redis-server 127.0.0.1:6379 如果想将Redis添加到开机启动项，将其添加到rc.local文件即可： echo &quot;/usr/local/bin/redis-server /etc/redis.conf&quot; &gt;&gt;/etc/rc.local 安装MySQL 5.7下载MySQL yum源安装包： wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm 安装MySQL yum源： yum localinstall mysql57-community-release-el7-8.noarch.rpm 安装： yum install mysql-community-server 启动MySQL： systemctl start mysqld 加入开机启动： systemctl enable mysqld systemctl daemon-reload MySQL安装后，在/var/log/mysqld.log为root用户生成了一个默认密码，我们需要修改root密码： grep &apos;temporary password&apos; /var/log/mysqld.log # 获取root默认密码 mysql -uroot -p # 进入MySQL ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;your_root_password&apos;; # 设置root用户的密码 具体可以参见这里 安装Docker：sudo yum update sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum list docker-ce --showduplicates | sort -r sudo yum install docker-ce 启动docker并加入开机启动项： sudo systemctl start docker sudo systemctl enable docker 验证docker安装成功： docker version 在docker下安装nginx：我们使用官方镜像来安装： docker pull nginx 启动nginx： docker run -p 80:80 --name mynginx -v /opt/docker_nginx/conf/nginx.conf:/etc/nginx/nginx.conf -v /var/www/hexo:/opt/nginx/www -v /opt/docker_nginx/log:/opt/nginx/log -d nginx 解释一下参数： -p 80:80：容器的80端口映射到本机的80端口（左边是本机，右边是容器） –name mynginx：容器名称，可以自定义 -v /opt/docker_nginx/conf/nginx.conf:/etc/nginx/nginx.conf：映射宿主机和容器的目录 -v /var/www/hexo:/opt/nginx/www：表示将本机的/var/www/hexo目录映射到容器的/opt/nginx/www，注意这里其实是把一个静态文件目录映射到容器内，这个静态文件目录可以自己定义 -v $PWD/log:/opt/nginx/log：表示将本机当前目录下的/log目录映射到容器的/opt/nginx/log，作为存放日志的地方 安装nvm：根据nvm在GitHub页面的README.md来安装： curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.8/install.sh | bash 然后编辑你所使用的shell的profile，加入： export NVM_DIR=&quot;$HOME/.nvm&quot; [ -s &quot;$NVM_DIR/nvm.sh&quot; ] &amp;&amp; \\. &quot;$NVM_DIR/nvm.sh&quot; # This loads nvm 之后source一下profile，假设使用bash，运行： source ~/.bashrc 验证安装： nvm --version 安装JDK列出JDK1.8的列表： yum list java-1.8* 从列表中选择java-1.8.0-openjdk.x86_64这项，安装JDK1.8： yum install java-1.8.0-openjdk.x86_64 验证JDK安装： java -version 安装Jenkins下载依赖： sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo 导入密钥： sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key 安装： yum install jenkins 查看Jenkins安装路径： rpm -ql jenkins 输出： /etc/init.d/jenkins /etc/logrotate.d/jenkins /etc/sysconfig/jenkins /usr/lib/jenkins # Jenkins安装目录，war包在此 /usr/lib/jenkins/jenkins.war # war包在此 /usr/sbin/rcjenkins /var/cache/jenkins /var/lib/jenkins # 默认的JENKINS_HOME /var/log/jenkins # Jenkins日志文件存放处 Jenkins的配置文件存放在/etc/sysconfig/jenkins，我们通过可以修改JENKINS_PORT配置项来修改端口号，默认是8080。s 最后以守护模式启动Jenkins，也可以在命令中指定端口号： nohup java -jar /usr/lib/jenkins/jenkins.war --httpPort=8080 &amp; 之后在web上做一些配置即可。 安装RabbitMQ由于RabbitMQ是Erlang写的，因此我们需要先安装Erlang，这里使用源码安装。在此之前需要先安装一波构建依赖： sudo yum install -y which wget perl openssl-devel make automake autoconf ncurses-devel gcc 接着安装Erlang： curl -O http://erlang.org/download/otp_src_20.2.tar.gz tar zxvf otp_src_20.2.tar.gz cd otp_src_20.2 ./otp_build autoconf ./configure &amp;&amp; make &amp;&amp; sudo make install 安装完后，一般还需要将Erlang安装的bin目录加入环境变量，在~/.bashrc添加如下语句后后保存： export PATH=&quot;/usr/local/lib/erlang/bin:$PATH&quot; source ~/.bashrc 验证安装： erl 看到进入Erlang Shell说明安装成功。 比较新版本的RabbitMQ从源码编译需要make 4.x版本，可以执行以下命令安装make 4.2： wget http://ftp.gnu.org/gnu/make/make-4.2.tar.gz tar -zxvf make-4.2.tar.gz cd make-4.2 ./configure make &amp;&amp; make install cd /usr/bin mv make make_bak ln -s /usr/local/bin/make ./make 之后就检测make 4.2安装情况： make -v 这个命令应该要显示make版本为4.2。 然后安装RabbitMQ： wget http://www.rabbitmq.com/releases/rabbitmq-server/v3.6.15/rabbitmq-server-3.6.15.tar.xz xz -d rabbitmq-server-3.6.15.tar.xz tar xvf rabbitmq-server-3.6.15.tar cd rabbitmq-server-3.6.15 make make install TARGET_DIR=/opt/rabbitmq SBIN_DIR=/opt/rabbitmq/sbin MAN_DIR=/opt/rabbitmq/man DOC_INSTALL_DIR=/opt/rabbitmq/doc 安装后设置环境变量： vim /etc/profile export PATH=$PATH:/usr/local/lib/erlang/lib/rabbitmq_server-3.6.15/sbin/ source /etc/profile mkdir /etc/rabbitmq cp ./deps/rabbit/docs/rabbitmq.config.example /etc/rabbitmq/rabbitmq.config 以守护进程方式启动RabbitMQ： rabbitmq-server -detached 然后启用插件： rabbitmq-plugins enable rabbitmq_management rabbitmq-plugins enable rabbitmq_mqtt 修改iptable： vim /etc/sysconfig/iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 15672 -j ACCEPT service iptables restart 设置RabbitMQ开机自启动，执行： vim /lib/systemd/system/rabbitmq.service 内容如下： [Unit] Description=Start RabbitMQ at startup. After=multi-user.target [Service] Type=simple ExecStart=/usr/local/lib/erlang/lib/rabbitmq_server-3.6.15/sbin/rabbitmq-server PrivateTmp=true [Install] WantedBy=multi-user.target ###################################################################### # 参考，以下是yum安装的服务 [Unit] Description=RabbitMQ broker After=syslog.target network.target [Service] Type=notify User=rabbitmq Group=rabbitmq WorkingDirectory=/var/lib/rabbitmq ExecStart=/usr/lib/rabbitmq/bin/rabbitmq-server ExecStop=/usr/lib/rabbitmq/bin/rabbitmqctl stop [Install] WantedBy=multi-user.target ##################################################################### # 重载服务，启动服务 systemctl daemon-reload systemctl start rabbitmq.service","categories":[{"name":"环境配置","slug":"环境配置","permalink":"https://nullcc.github.io/categories/环境配置/"}],"tags":[{"name":"环境配置","slug":"环境配置","permalink":"https://nullcc.github.io/tags/环境配置/"}]},{"title":"Linux目录结构详解","slug":"Linux目录结构详解","date":"2018-03-12T16:00:00.000Z","updated":"2022-04-15T03:41:13.014Z","comments":true,"path":"2018/03/13/Linux目录结构详解/","link":"","permalink":"https://nullcc.github.io/2018/03/13/Linux目录结构详解/","excerpt":"本文主要记录一下Linux目录结构的含义。","text":"本文主要记录一下Linux目录结构的含义。 在Windows中，我们会看到磁盘驱动器的标识，比如C:\\，D:\\，文件和目录名都跟在驱动器名后面。Linux和Windows完全不同，在Linux中你可以在根目录下运行ls，观察它的目录结构： / - 根目录顾名思义，根目录就是所有目录所在的目录，它是Linux中所有目录的根，从某种概念上讲，它有点类似于Windows\b中的磁盘驱动器符号。之后将会看到，所有其他目录都是以/开头的。 /bin - 存放重要的用户二进制文件目录bin是binary的缩写，/bin目录存放了非常重要的用户二进制文件，其实就是一些程序。需要特别说明的是，/bin中存放的都是在单用户维护模式下还能被操作的命令，这些命令可以被root和普通用户使用。在/bin中我们可以看到例如chmod、du这些我们经常使用的Linux命令程序。 /boot - 存放系统启动时需要文件的目录/boot中存放的主要是系统启动时需要用到的文件，比如EFI、GRUB以及Linux内核。 /dev - 设备文件目录Linux将所有东西都看成文件，设备也不例外，不论是实体的硬件设备还是虚拟设备。实体硬件比如第一块被检测到的硬盘会被挂载到/dev/sda，第二块会被挂载到/dev/sdb，以此类推。虚拟设备比如null、random、stderr、stdin和stdout。 /etc - 配置文件目录etc的含义是Et cetera，表示一些相关的其他东西，/etc从UNIX早期开始就被用来存放配置文件，一直沿用至今。需要注意的是，/etc中存放的是系统配置文件，特定用户的配置文件放在每个用户的/home目录下。 /home - 用户目录/home目录中存放的是每个用户的用户目录，比如有一个用户叫tom，/home/tom就是他的用户目录，每个用户的用户目录中存放的用户数据和用户配置文件，比如每个用户的.bashrc文件、.ssh目录等。普通用户只能访问自己的用户目录而不能访问别人的，root用户则没有此限制。 /lib - 重要的共享库目录/lib目录中存放的是/bin和/sbin中那些重要的二进制文件（程序）需要的共享库。另外/usr/bin中二进制文件（程序）需要的共享库都存放在/usr/lib。 /lost+found - 恢复文件目录每个Linux文件系统都有/lost+found目录，当文件系统崩溃时，在下次启动时将会进行文件系统自检，任何在崩溃过程中损坏的文件都将被放入/lost+found，你可以尝试找到并恢复这些文件。 /media - 可删除媒介目录当你将一些可删除媒介挂（比如CD，U盘，光驱等）载到计算机上时，系统会在/media下自动创建子目录，之后就可以通过访问这个子目录来访问媒介。比如你在电脑上插入CD，在/media目录下就会自动生成一个子目录，你可以通过访问这个子目录来访问CD的内容。 /mnt - 临时挂载点目录mnt是mount的缩写，各种设备挂载到系统后，会在/mnt目录下生成相应设备的目录，比如挂载光驱、文件系统、CD等。 /opt - 可选包目录opt是Optional的意思，系统中安装的可选软件包被存放在/opt目录中。 /proc - 内核和进程文件目录/proc是一个虚拟目录，并不包含任何标准文件。当系统启动后，会对当前内核和硬件信息进行检测，并将这些信息放在这个目录中（其实都在内存中）。你可以使用cat /proc/meminfo来查看关于内存的一些信息，或者使用cat /proc/version来查看Linux内核信息。 /root - root用户的home目录和一般用户的home目录放在/home/{用户名}不同，root用户的home目录专门被放在/root。 /run - 应用程序状态目录/run目录用来存放应用程序在运行期间需要的一些短暂存在的文件，比如socket和pid文件，这些文件对这些应用程序的运行很重要，因此不能把他们放在/tmp目录下，因为/tmp目录下的文件有可能会被系统清空。 /sbin - 系统管理员用的二进制文件目录/sbin目录和/bin目录有些类似，都是存放二进制文件。比较特殊的是，/sbin目录存放的都是系统管理的命令程序，一般只有root用户能使用。 /srv - 服务数据目录/srv目录主要用来存放一些系统提供的网络服务的数据，如果你在机器上运行一个HTTP服务器来对外提供静态文件时，/srv/http目录将存放这些静态文件，如果你运行一个FTP服务器对外提供文件，则/srv/ftp目录将存放这些文件。 /tmp - 临时文件目录应用程序会将一些临时文件存放在/tmp目录中，一般来说当系统重启后，/tmp下的文件会被自动清空，也会被一些监控程序清空。 /usr - 用户二进制文件和只读文件目录/usr目录下存放的是用户使用的程序，可以很明显地发现，Linux将普通用户和系统使用的程序分开存放，比如上面提到过，系统使用的一些重要程序存放在/bin，而那些不重要的程序就被放在/usr/bin。不太重要的系统管理程序存放在/usr/sbin而不是/sbin。usr也被称为UNIX Software Resource，这是比较早期的说法。下面是/usr目录下常见的几个子目录的含义： /usr/bin 存放用户命令 /usr/include 存放编程语言的头文件和包含文件 /usr/lib 存放各个应用程序的库函数和目标文件 /usr/local 存放系统管理员下载的软件的安装目录 /usr/sbin 存放不常用的系统命令 /usr/src 存放源码 /var - 变动数据目录/var一般用来存放经常变动的数据，比如日志文件和缓存文件。下面是/var目录下常见的几个子目录的含义： /var/cache 存放应用程序运行产生的临时文件 /var/lib 存放程序在执行过程中需要使用到的数据文件，每个软件在此目录下都有自己独自的目录 /var/lock 存放程序的锁状态 /var/log 存放各种日志文件 /var/mail 存放个人电子邮箱 /var/run 有些程序在运行后，会将他们的pid、socket文件放置到这个目录中","categories":[{"name":"Linux","slug":"Linux","permalink":"https://nullcc.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://nullcc.github.io/tags/Linux/"}]},{"title":"关于TDD和BDD的一点浅见","slug":"关于TDD和BDD的一点浅见","date":"2018-03-05T16:00:00.000Z","updated":"2022-04-15T03:41:13.029Z","comments":true,"path":"2018/03/06/关于TDD和BDD的一点浅见/","link":"","permalink":"https://nullcc.github.io/2018/03/06/关于TDD和BDD的一点浅见/","excerpt":"TDD(Test-Driven Development)和BDD(Behavior-Driven Development)在敏捷开发中经常被提到，虽然看上去好像有点类似，但实际上它们两者的含义、适用人群、使用方式和在软件开发中的作用的区别相当大。","text":"TDD(Test-Driven Development)和BDD(Behavior-Driven Development)在敏捷开发中经常被提到，虽然看上去好像有点类似，但实际上它们两者的含义、适用人群、使用方式和在软件开发中的作用的区别相当大。 TDD(Test-Driven Development)——测试驱动开发TDD是敏捷开发中的一项核心开发实践，也是一种方法论。其主要思想是在正式编写需求功能的代码之前，先编写单元测试代码，再编写需求功能代码满足这些单元测试代码。 TDD的测试粒度很细，可以说是最底层的一种测试了，开发人员针对一个类、一个函数去编写单元测试。在实际项目中，仅仅是一个函数的单元测试可能就要写多个测试函数：针对\b边界值、极大值、极小值、正常值、空值等的测试。因此这也导致了开发人员需要编写大量测试代码，如果想要较好地覆盖需要测试函数和类，这些单元测试代码的量可能会远大于实际功能代码的量。 TDD中主要是开发人员写单元测试，基本不太需要测试人员参与。TDD的好处非常明显，第一个好处是由于每个类和函数都有相应的单元测试，因此在任何一个时间节点，功能代码的质量都不会太差，只有很少量的bug，软件的可交付性好。还有一个好处是，代码的可维护性较高，开发人员可以时常重构和优化代码，而不必担心无意中引入问题，开发人员可以在每次重构完一个函数时运行单元测试，单元测试会告诉我们这个重构是否引入了错误。TDD结合持续集成(CI)在实际项目开发中可以有非常好的效果，开发人员在任何时刻都可以启动一次自动化测试，CI系统稍后会告诉你结果。TDD保证了在函数和类级别的可测试性和可维护性。 实际项目中，由于交付时间的限制，不太可能所有函数和类都去用详尽的单元测试区覆盖，所以实践TDD的最基本要求应该是对那些重要的东西编写尽可能详细的单元测试，把我们的精力花费在最核心的部分。 BDD(Behavior-Driven Development)——行为驱动开发从层次上来说，BDD是基于TDD的，或者说在自动化测试中，TDD所在的位置比较底层，是基础，而BDD大概在中间的位置。 BDD核心的是，开发人员、QA、非技术人员和用户都参与到项目的开发中，彼此协作。BDD强调从用户的需求出发，最终的系统和用户的需求一致。BDD验证代码是否真正符合用户需求，因此BDD是从一个较高的视角来对验证系统是否和用户需求相符。 BDD倡导使用简单明了的自然语言描述系统行为，保证系统功能和设计是相符的。","categories":[{"name":"敏捷开发","slug":"敏捷开发","permalink":"https://nullcc.github.io/categories/敏捷开发/"}],"tags":[{"name":"敏捷开发","slug":"敏捷开发","permalink":"https://nullcc.github.io/tags/敏捷开发/"},{"name":"TDD","slug":"TDD","permalink":"https://nullcc.github.io/tags/TDD/"},{"name":"BDD","slug":"BDD","permalink":"https://nullcc.github.io/tags/BDD/"}]},{"title":"(译)分片：如何将数据存储在不同的Redis实例中","slug":"(译)分片：如何将数据存储在不同的Redis实例中","date":"2018-03-02T16:00:00.000Z","updated":"2022-04-15T03:41:13.012Z","comments":true,"path":"2018/03/03/(译)分片：如何将数据存储在不同的Redis实例中/","link":"","permalink":"https://nullcc.github.io/2018/03/03/(译)分片：如何将数据存储在不同的Redis实例中/","excerpt":"本文翻译自Partitioning: how to split data among multiple Redis instances。","text":"本文翻译自Partitioning: how to split data among multiple Redis instances。 分片就是将你的数据切分成多个部分存放在多个Redis实例中的过程，因此每个实例中将只有你的所有键的一个子集。本文档的第一部分将向你介绍分片的概念，第二部分将向你展示Redis的分片方案。 为什么分片很有用？Redis中的分片主要有两个目标： 它使用多台计算机的内存综合来构建更大型的数据库。如果不分片，你将只能使用单个计算机所能支持的内存量。它允许将计算能力扩展至多核以及多台计算机，并将网络带宽扩展到多台计算机和网络适配器上。 分片的基础概念这里有一些不同的分片标准。假设我们有4个Redis实例：R0、R1、R2、R3，且代表用户的键的名称类似user:1、user:2，以此类推，我们有几个不同的方法来选择一个实例并将一个给定的键存储在这个实例中。换句话说，即有不同的方式将一个给定的键映射到一个给定的Redis服务器上。 最简单的一种方式是范围分片，将一定范围的对象映射到指定的Redis实例上，例如，我们可以让ID范围在0-10000内的用户映射到R0，ID范围在10001-20000内的用户映射到R1，以此类推。 该系统在实际工作中可行，然而，它的缺点是需要一个将对象范围映射到实例的表。对于每一类对象，我们都需要管理这样一个表，因此，范围分片在Redis中往往是不可取的，因为它比其他的分片方式更低效。 一个范围分片的替代方案是哈希分片。这种模式对任何键都有效，它不需要键采用object_name:&lt;id&gt;的形式，而且它很简单： 使用哈希函数（比如，crc32哈希函数）将键名称转换为一个数字。例如，如果键名称为foobar，crc32(foobar)将输出类似93024922这样的数字。 对这个数字使用取模操作，将它转换成0-3之间的整数，以便这个数字可以映射到我们的其中一个Redis实例上。93024922对4取模等于2，因此键foorbar将被存储在R2上。注意：取模操作返回的是一个除法操作的余数，它在很多编程语言中被实现为%操作符。 还有很多其他方式来执行分片操作，但看了这两个例子后你应该有一些想法了。一个基于哈希分片的高级方式被称为一致性哈希，它是由几个Redis客户端和代理实现的。 分片的不同实现分片在软件上可以被分为几个不同的部分。 客户端分片意味着由客户端直接选择正确的节点进行写操作或读取一个给定的键。很多Redis客户端都实现了客户端分片。 代理辅助分片意味着我们的客户端将请求发送给一个能够支持Redis协议的代理，而不是直接将请求发送给正确的Redis实例。代理在确认后将根据已配置的分区模式将我们的请求发送给正确的Redis实例，之后会将回复回送给客户端。Redis和Memcached代理twemproxy实现了辅助代理分区。 请求路由意味着你可以将请求随机发送给一个实例，这个实例确认后将会把这个请求发送给正确的节点。Redis集群实现了一个混合形式的请求路由，这里面有来自客户端的帮助（请求不是从一个Redis实例直接发送给另一个实例，而是将客户端重定向到正确的节点）。 分片的缺点在使用分片时，Redis的有些功能表现不佳： 涉及多个键的操作通常不受支持。例如，你无法对两个存在于不同Redis实例上的集合执行交集操作（实际上这里有两种方法来完成这个操作，但都是间接的）。 无法使用涉及多个键的Redis事务。 分片的粒度是键，因此无法将类似一个非常大的有序集合这样的大键分片到多个实例上。 当使用了分片时，数据处理会更加复杂，例如你不得不处理多个RDB/AOF文件，并且为了备份你的数据就需要将多个实例和主机上的持久化文件进行聚合。 增加和减少容量都很复杂。例如，Redis集群支持在运行时增减节点来几乎透明地对数据进行平衡，但其他一些系统比如客户端分片和代理辅助分片不支持这种功能。然而，一种叫做预分片的技术在这里会有所帮助。 Redis是作为数据存储还是缓存？虽然Redis的分片不管在将Redis作为数据存储使用或者作为缓存使用在概念上都是相同的，但在将Redis作为数据存储使用时有一个重要的限制。当Redis被用来作为数据存储使用，一个给定的键必须总是被映射到相同的Redis实例上。当Redis被用来作为缓存，因一个给定节点不可用而转而使用另一个节点也不是什么大问题，我们可以改变键-实例映射关系来提高系统可用性（也就是系统对我们的查询的响应能力）。 如果对于一个给定键，首选节点不可用的话，一致性哈希实现通常能够将其切换到其他节点上。类似地如果你添加了一个新节点，一部分新键将开始被存储在这个新节点上。 这里的主要概念如下： 如果Redis被当成一个缓存使用，使用一致性哈希进行扩容和缩容是很容易的。 如果Redis被当成一个数据存储使用，使用了一个固定的键-节点映射，则节点数量是固定的，不能改变。否则，我们需要一个在节点被添加或移除时能够在节点之间进行再平衡键的系统，目前只有Redis集群能够做到 —— Redis集群在2015-04-01之后在正产环境中可用。 预分片我们发现关于分片的一个问题是，除非我们将Redis作为一个缓存使用，否则添加或删除节点可能很麻烦，使用固定的键-实例映射要简单得多。 然而，数据存储的需求可能会随着时间变化。今天我只要10个Redis节点就能搞定，但明天我可能就需要50个节点。 由于Redis非常的小和轻量级（空闲实例值占用1MB内存），一个解决该问题的简单方法是在一开始就启动大量实例。即使你只使用一台服务器，你也可以决定从一开始就采用分布式的方法进行部署，使用分片将多个Redis实例运行在一台服务器上。 而且从一开始你就可以选择一个非常大的实例数量。例如，32或64个实例对大多数用户都是一个不错的选择，这个数量可以为将来的增长预留足够的空间。 这样，d昂你的数据存储需求增加，需要更多的Redis服务器，你需要做的就是简单地将Redis实例从一台服务器移动到另一台。一旦你添加了第一台额外的服务器，你将需要把一半的Redis实例从第一台移动到第二台，以此类推。 使用Redis复制你将能够在很少或者没有停机时间的情况下为你的用户们移动数据： 在新服务器上启动新Redis实例。 通过将这些新实例设置为你旧实例的的从节点来移动数据。 停止你的客户端。 用新的服务器IP更新被移动实例的配置。 对新服务器上的从节点发送SLAVEOF NO ONE命令。 使用更新后的配置重启你的客户端。 最后关闭旧服务器上那些不再使用的Redis实例。 Redis分片的各种实现到目前为止，我们涵盖了Redis分片的理论部分，但在实践中要如何做呢？我们应该使用哪种系统？ Redis集群Redis集群是获得自动分片和高可用性的首选方式。它从2015-04-01开始就在生产环境中可用了。你可以在集群教程中获取更多有关Redis集群的信息。 一旦Redis集群可用，而且如果你使用的编程语言中有一个Redis集群兼容的客户端，那么Redis集群将变为Redis分片的事实标准。 Redis集群是请求路由和客户端分片之间的一种混合方式。 TwemproxyTwemproxy是一个由Twitter为Memcached ASCII和Redis协议开发的一个代理。它是单线程的，使用C编写而成，并且它非常快。它是一个使用Apache 2.0许可证的开源软件。 Twemproxy支持在多个Redis实例中自动分片，如果节点不可用，会将节点移出（这将改变键-实例映射，因此你应该只在将Redis作为一个缓存使用时采用这种方式）。 由于你可以启动多个代理且可以指示你的客户端连接到接收连接的第一个节点，因此使用代理不会造成单点失败问题。 基本上Twemproxy是一个介于客户端和Redis实例之间的中间层，这将可靠地为我们以最小的复杂度处理分片。 你可以从antirez的这篇blog中阅读到更多关于Twemproxy的信息。 支持一致性哈希的客户端一种Twemproxy的替代方案是，使用支持采用一致性哈希或其他类似算法进行分片的客户端。这里有多个Redis客户端支持一致性哈希，尤其是Redis-rb和Predis。 请查看Redis客户端的完整列表来确认是否有一个你的编程语言下支持的一致性哈希的成熟的客户端实现。","categories":[{"name":"文档翻译","slug":"文档翻译","permalink":"https://nullcc.github.io/categories/文档翻译/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://nullcc.github.io/tags/Redis/"}]},{"title":"深入解析Javascript异步编程","slug":"深入解析Javascript异步编程","date":"2018-03-02T16:00:00.000Z","updated":"2022-04-15T03:41:13.035Z","comments":true,"path":"2018/03/03/深入解析Javascript异步编程/","link":"","permalink":"https://nullcc.github.io/2018/03/03/深入解析Javascript异步编程/","excerpt":"这里深入探讨下Javascript的异步编程技术。","text":"这里深入探讨下Javascript的异步编程技术。 Javascript异步编程简介至少在语言级别上，Javascript是单线程的，因此异步编程对其尤为重要。 拿nodejs来说，外壳是一层js语言，这是用户操作的层面，在这个层次上它是单线程运行的，也就是说我们不能像Java、Python这类语言在语言级别使用多线程能力。取而代之的是，nodejs编程中大量使用了异步编程技术，这是为了高效使用硬件，同时也可以不造成同步阻塞。不过nodejs在底层实现其实还是用了多线程技术，只是这一层用户对用户来说是透明的，nodejs帮我们做了几乎全部的管理工作，我们不用担心锁或者其他多线程编程会遇到的问题，只管写我们的异步代码就好。 Javascript异步编程方法 ES 6以前： 回调函数 事件监听(事件发布/订阅) Promise对象 ES 6： Generator函数(协程coroutine) ES 7: async和await PS:如要运行以下例子，请安装node v0.11以上版本，在命令行下使用 node [文件名.js] 的形式来运行，有部分代码需要开启特殊选项，会在具体例子里说明。 1.回调函数回调函数在Javascript中非常常见，一般是需要在一个耗时操作之后执行某个操作时可以使用回调函数。 example 1: 12345678910//一个定时器function timer(time, callback)&#123; setTimeout(function()&#123; callback(); &#125;, time);&#125; timer(3000, function()&#123; console.log(123);&#125;) example 2: 123456789//读文件后输出文件内容var fs = require('fs');fs.readFile('./text1.txt', 'utf8', function(err, data)&#123; if (err)&#123; throw err; &#125; console.log(data);&#125;); example 3: 123456789//嵌套回调，读一个文件后输出，再读另一个文件，注意文件是有序被输出的，先text1.txt后text2.txtvar fs = require('fs');fs.readFile('./text1.txt', 'utf8', function(err, data)&#123; console.log(\"text1 file content: \" + data); fs.readFile('./text2.txt', 'utf8', function(err, data)&#123; console.log(\"text2 file content: \" + data); &#125;);&#125;); example 4: 1234567891011//callback helldoSomethingAsync1(function()&#123; doSomethingAsync2(function()&#123; doSomethingAsync3(function()&#123; doSomethingAsync4(function()&#123; doSomethingAsync5(function()&#123; &#125;); &#125;); &#125;); &#125;); &#125;); 通过观察以上4个例子，可以发现一个问题，在回调函数嵌套层数不深的情况下，代码还算容易理解和维护，一旦嵌套层数加深，就会出现“回调金字塔”的问题，就像example 4那样，如果这里面的每个回调函数中又包含了很多业务逻辑的话，整个代码块就会变得非常复杂。从逻辑正确性的角度来说，上面这几种回调函数的写法没有任何问题，但是随着业务逻辑的增加和趋于复杂，这种写法的缺点马上就会暴露出来，想要维护它们实在是太痛苦了，这就是“回调地狱(callback hell)”。 一个衡量回调层次复杂度的方法是，在example 4中，假设doSomethingAsync2要发生在doSomethingAsync1之前，我们需要忍受多少重构的痛苦。 回调函数还有一个问题就是我们在回调函数之外无法捕获到回调函数中的异常，我们以前在处理异常时一般这么做： example 5: 123456try&#123; //do something may cause exception..&#125;catch(e)&#123; //handle exception...&#125; 在同步代码中，这没有问题。现在思考一下下面代码的执行情况： example 6: 12345678910var fs = require('fs');try&#123; fs.readFile('not_exist_file', 'utf8', function(err, data)&#123; console.log(data); &#125;);&#125;catch(e)&#123; console.log(\"error caught: \" + e);&#125; 你觉得会输出什么？答案是undefined。我们尝试读取一个不存在的文件，这当然会引发异常，但是最外层的try/catch语句却无法捕获这个异常。这是异步代码的执行机制导致的。 Tips: 为什么异步代码回调函数中的异常无法被最外层的try/catch语句捕获? 异步调用一般分为两个阶段，提交请求和处理结果，这两个阶段之间有事件循环的调用，它们属于两个不同的事件循环(tick)，彼此没有关联。 异步调用一般以传入callback的方式来指定异步操作完成后要执行的动作。而异步调用本体和callback属于不同的事件循环。 try/catch语句只能捕获当次事件循环的异常，对callback无能为力。 也就是说，一旦我们在异步调用函数中扔出一个异步I/O请求，异步调用函数立即返回，此时，这个异步调用函数和这个异步I/O请求没有任何关系。 2.事件监听(事件发布/订阅)事件监听是一种非常常见的异步编程模式，它是一种典型的逻辑分离方式，对代码解耦很有用处。通常情况下，我们需要考虑哪些部分是不变的，哪些是容易变化的，把不变的部分封装在组件内部，供外部调用，需要自定义的部分暴露在外部处理。从某种意义上说，事件的设计就是组件的接口设计。 example 7: 12345678910//发布和订阅事件var events = require('events');var emitter = new events.EventEmitter(); emitter.on('event1', function(message)&#123; console.log(message);&#125;); emitter.emit('event1', \"message for you\"); 这种使用事件监听处理的异步编程方式很适合一些需要高度解耦的场景。例如在之前一个游戏服务端项目中，当人物属性变化时，需要写入到持久层。解决方案是先写一个订阅方，订阅’save’事件，在需要保存数据时让发布方对象(这里就是人物对象)上直接用emit发出一个事件名并携带相应参数，订阅方收到这个事件信息并处理。 3.Promise对象ES 6中原生提供了Promise对象，Promise对象代表了某个未来才会知道结果的事件(一般是一个异步操作)，并且这个事件对外提供了统一的API，可供进一步处理。 使用Promise对象可以用同步操作的流程写法来表达异步操作，避免了层层嵌套的异步回调，代码也更加清晰易懂，方便维护。 Promise.prototype.then()Promise.prototype.then()方法返回的是一个新的Promise对象，因此可以采用链式写法，即一个then后面再调用另一个then。如果前一个回调函数返回的是一个Promise对象，此时后一个回调函数会等待第一个Promise对象有了结果，才会进一步调用。 example 8: 123456789101112131415161718192021//ES 6原生Promise示例const fs = require('fs')const read = function (filename)&#123; const promise = new Promise(function(resolve, reject)&#123; fs.readFile(filename, 'utf8', function(err, data)&#123; if (err)&#123; reject(err); &#125; resolve(data); &#125;); &#125;); return promise;&#125; read('a.txt').then(function(data)&#123; console.log(data);&#125;, function(err)&#123; console.log(\"err: \" + err);&#125;); 12$ babel-node promise.jsfile a content 以上代码中，read函数是Promise化的，在read函数中，实例化了一个Promise对象，Promise的构造函数接受一个函数作为参数，这个函数的两个参数分别是resolve方法和reject方法。如果异步操作成功，就是用resolve方法将Promise对象的状态从“未完成”变为“完成”(即从pending变为resolved)，如果异步操作出错，则是用reject方法把Promise对象的状态从“未完成”变为“失败”(即从pending变为rejected)，read函数返回了这个Promise对象。Promise实例生成以后，可以用then方法分别指定resolve方法和reject方法的回调函数。 上面这个例子，Promise构造函数的参数是一个函数，在这个函数中我们写异步操作的代码，在异步操作的回调中，我们根据err变量来选择是执行resolve方法还是reject方法，一般来说调用resolve方法的参数是异步操作获取到的数据(如果有的话)，但还可能是另一个Promise对象，表示异步操作的结果有可能是一个值，也有可能是另一个异步操作，调用reject方法的参数是异步回调用的err参数。 调用read函数时，实际上返回的是一个Promise对象，通过在这个Promise对象上调用then方法并传入resolve方法和reject方法来指定异步操作成功和失败后的操作。 example 9: 123456789101112131415161718192021222324//原生Primose顺序嵌套回调示例var fs = require('fs')var read = function (filename)&#123; var promise = new Promise(function(resolve, reject)&#123; fs.readFile(filename, 'utf8', function(err, data)&#123; if (err)&#123; reject(err); &#125; resolve(data); &#125;) &#125;); return promise;&#125; read('./text1.txt').then(function(data)&#123; console.log(data); return read('./text2.txt');&#125;).then(function(data)&#123; console.log(data);&#125;); 在Promise的顺序嵌套回调中，第一个then方法先输出text1.txt的内容后返回read(‘./text2.txt’)，注意这里很关键，这里实际上返回了一个新的Promise实例，第二个then方法指定了异步读取text2.txt文件的回调函数。这种形似同步调用的Promise顺序嵌套回调的特点就是有一大堆的then方法，代码冗余略多。 异常处理Promise.prototype.catch()Promise.prototype.catch方法是Promise.prototype.then(null, rejection)的别名，用于指定发生错误时的回调函数。 example 9: 12345678910111213141516171819202122232425262728var fs = require('fs')var read = function (filename)&#123; var promise = new Promise(function(resolve, reject)&#123; fs.readFile(filename, 'utf8', function(err, data)&#123; if (err)&#123; reject(err); &#125; resolve(data); &#125;) &#125;); return promise;&#125; read('./text1.txt').then(function(data)&#123; console.log(data); return read('not_exist_file');&#125;).then(function(data)&#123; console.log(data);&#125;).catch(function(err)&#123; console.log(\"error caught: \" + err);&#125;).then(function(data)&#123; console.log(\"completed\");&#125;) 使用Promise对象的catch方法可以捕获异步调用链中callback的异常，Promise对象的catch方法返回的也是一个Promise对象，因此，在catch方法后还可以继续写异步调用方法。这是一个非常强大的能力。 如果在catch方法中发生了异常： example 10: 1234567891011121314151617181920212223242526272829var fs = require('fs')var read = function (filename)&#123; var promise = new Promise(function(resolve, reject)&#123; fs.readFile(filename, 'utf8', function(err, data)&#123; if (err)&#123; reject(err); &#125; resolve(data); &#125;) &#125;); return promise;&#125; read('./text1.txt').then(function(data)&#123; console.log(data); return read('not_exist_file');&#125;).then(function(data)&#123; console.log(data);&#125;).catch(function(err)&#123; console.log(\"error caught: \" + err); x+1;&#125;).then(function(data)&#123; console.log(\"completed\");&#125;) 在上述代码中，x+1会抛出一个异常，但是由于后面没有catch方法了，导致这个异常不会被捕获，而且也不会传递到外层去，也就是说这个异常就默默发生了，没有惊动任何人。 我们可以在catch方法后再加catch方法来捕获这个x+1的异常： example 11: 1234567891011121314151617181920212223242526272829303132var fs = require('fs')var read = function (filename)&#123; var promise = new Promise(function(resolve, reject)&#123; fs.readFile(filename, 'utf8', function(err, data)&#123; if (err)&#123; reject(err); &#125; resolve(data); &#125;) &#125;); return promise;&#125; read('./text1.txt').then(function(data)&#123; console.log(data); return read('not_exist_file');&#125;).then(function(data)&#123; console.log(data);&#125;).catch(function(err)&#123; console.log(\"error caught: \" + err); x+1;&#125;).catch(function(err)&#123; console.log(\"error caught: \" + err);&#125;).then(function(data)&#123; console.log(\"completed\");&#125;) Promise异步并发如果几个异步调用有关联，但它们不是顺序式的，是可以同时进行的，我们很直观地会希望它们能够并发执行(这里要注意区分“并发”和“并行”的概念，不要搞混)。 Promise.all() Promise.all方法用于将多个Promise实例，包装成一个新的Promise实例。 var p = Promise.all([p1,p2,p3]); Promise.all方法接受一个数组作为参数，p1、p2、p3都是Promise对象实例。 p的状态由p1、p2、p3决定，分两种情况： 只有p1、p2、p3的状态都变成fulfilled，p的状态才会变成fulfilled，此时p1、p2、p3的返回值组成一个数组，传递给p的回调函数。 只要p1、p2、p3之中有一个被rejected，p的状态就变成rejected，此时第一个被reject的实例的返回值，会传递给p的回调函数。 一个具体的例子： example 12: 12345678910111213141516171819202122232425var fs = require('fs')var read = function (filename)&#123; var promise = new Promise(function(resolve, reject)&#123; fs.readFile(filename, 'utf8', function(err, data)&#123; if (err)&#123; reject(err); &#125; resolve(data); &#125;) &#125;); return promise;&#125; var promises = [1, 2].map(function(fileno)&#123; return read('./text' + fileno + '.txt');&#125;); Promise.all(promises).then(function(contents)&#123; console.log(contents);&#125;).catch(function(err)&#123; console.log(\"error caught: \" + err);&#125;) 上述代码会并发执行读取text1.txt和text2.txt的操作，当两个文件都读取完毕时，输出它们的内容，contents是一个数组，每个元素对应promises数组的执行结果 (顺序完全一致)，在这里就是text1.txt和text2.txt的内容。 Promise.race()Promise.race()也是将多个Promise实例包装成一个新的Promise实例： 1var p = Promise.race([p1,p2,p3]); 上述代码中，p1、p2、p3只要有一个实例率先改变状态，p的状态就会跟着改变，那个率先改变的Promise实例的返回值，就传递给p的返回值。如果Promise.all方法和Promise.race方法的参数不是Promise实例，就会先调用下面讲到的Promise.resolve方法，将参数转为Promise实例，再进一步处理。 example 13: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960var http = require('http'); var qs = require('querystring'); var requester = function(options, postData)&#123; var promise = new Promise(function(resolve, reject)&#123; var content = \"\"; var req = http.request(options, function (res) &#123; res.setEncoding('utf8'); res.on('data', function (data) &#123; onData(data); &#125;); res.on('end', function () &#123; resolve(content); &#125;); function onData(data)&#123; content += data; &#125; &#125;); req.on('error', function(err) &#123; reject(err); &#125;); req.write(postData); req.end(); &#125;); return promise;&#125; var promises = [\"柠檬\", \"苹果\"].map(function(keyword)&#123; var options = &#123; hostname: 'localhost', port: 9200, path: '/meiqu/tips/_search', method: 'POST' &#125;; var data = &#123; 'query' : &#123; 'match' : &#123; 'summary' : keyword &#125; &#125; &#125;; data = JSON.stringify(data); return requester(options, data);&#125;); Promise.race(promises).then(function(contents) &#123; var obj = JSON.parse(contents); console.log(obj.hits.hits[0]._source.summary);&#125;).catch(function(err)&#123; console.log(err); &#125;); Promise.resolve()有时候需将现有对象转换成Promise对象，可以使用Promise.resolve()。 如果Promise.resolve方法的参数，不是具有then方法的对象（又称thenable对象），则返回一个新的Promise对象，且它的状态为fulfilled。 如果Promise.resolve方法的参数是一个Promise对象的实例，则会被原封不动地返回。 example 14: 12345var p = Promise.resolve('Hello');p.then(function (s)&#123; console.log(s)&#125;); Promise.reject()Promise.reject(reason)方法也会返回一个新的Promise实例，该实例的状态为rejected。Promise.reject方法的参数reason，会被传递给实例的回调函数。 example 15: 12345var p = Promise.reject('出错了');p.then(null, function (s)&#123; console.log(s)&#125;); 上面代码生成一个Promise对象的实例p，状态为rejected，回调函数会立即执行。 3.Generator函数Generator函数是协程在ES 6中的实现，最大特点就是可以交出函数的执行权（暂停执行）。 在node中需要开启–harmony选项来启用Generator函数。 整个Generator函数就是一个封装的异步任务，或者说是异步任务的容器。异步操作需要暂停的地方，都用yield语句注明。 协程的运行方式如下： 第一步：协程A开始运行。第二步：协程A执行到一半，暂停，执行权转移到协程B。第三步：一段时间后，协程B交还执行权。第四步：协程A恢复执行。 上面的协程A就是异步任务，因为分为两步执行。 比如一个读取文件的例子： example 16: 12345function asnycJob() &#123; // ...其他代码 var f = yield readFile(fileA); // ...其他代码&#125; asnycJob函数是一个协程，yield语句表示，执行到此处执行权就交给其他协程，也就是说，yield是两个阶段的分界线。协程遇到yield语句就暂停执行，直到执行权返回，再从暂停处继续执行。这种写法的优点是，可以把异步代码写得像同步一样。 看一个简单的Generator函数例子： example 17: 12345678910function* gen(x)&#123; var y = yield x + 2; return y;&#125; var g = gen(1);var r1 = g.next(); // &#123; value: 3, done: false &#125;console.log(r1);var r2 = g.next() // &#123; value: undefined, done: true &#125;console.log(r2); 需要注意的是Generator函数的函数名前面有一个”*”。 上述代码中，调用Generator函数，会返回一个内部指针(即遍历器)g，这是Generator函数和一般函数不同的地方，调用它不会返回结果，而是一个指针对象。调用指针g的next方法，会移动内部指针，指向第一个遇到的yield语句，上例就是执行到x+2为止。 换言之，next方法的作用是分阶段执行Generator函数。每次调用next方法，会返回一个对象，表示当前阶段的信息（value属性和done属性）。value属性是yield语句后面表达式的值，表示当前阶段的值；done属性是一个布尔值，表示Generator函数是否执行完毕，即是否还有下一个阶段。 Generator函数的数据交换和错误处理next方法返回值的value属性，是Generator函数向外输出数据；next方法还可以接受参数，这是向Generator函数体内输入数据。 example 18: 12345678910function* gen(x)&#123; var y = yield x + 2; return y;&#125; var g = gen(1);var r1 = g.next(); // &#123; value: 3, done: false &#125;console.log(r1);var r2 = g.next(2) // &#123; value: 2, done: true &#125;console.log(r2); 第一个next的value值，返回了表达式x+2的值(3)，第二个next带有参数2，这个参数传入Generator函数，作为上个阶段异步任务的返回结果，被函数体内的变量y接收，因此这一阶段的value值就是2。 Generator函数内部还可以部署错误处理代码，捕获函数体外抛出的错误。 example 19: 12345678910111213function* gen(x)&#123; try &#123; var y = yield x + 2; &#125; catch (e)&#123; console.log(e); &#125; return y;&#125; var g = gen(1);g.next();g.throw('error!'); //error! 下面是一个读取文件的真实异步操作的例子。 example 20: 1234567891011121314151617181920212223var fs = require('fs');var thunkify = require('thunkify');var readFile = thunkify(fs.readFile); var gen = function* ()&#123; var r1 = yield readFile('./text1.txt', 'utf8'); console.log(r1); var r2 = yield readFile('./text2.txt', 'utf8'); console.log(r2);&#125;; //开始执行上面的代码var g = gen(); var r1 = g.next();r1.value(function(err, data)&#123; if (err) throw err; var r2 = g.next(data); r2.value(function(err, data)&#123; if (err) throw err; g.next(data); &#125;);&#125;); 这就是一个基本的Generator函数定义和执行的流程。可以看到，虽然这里的Generator函数写的很简洁，和同步方法的写法很像，但是执行起来却很麻烦，流程管理比较繁琐。 在深入讨论Generator函数之前我们先要知道Thunk函数这个概念。 求值策略(即函数的参数到底应该何时求值) 传值调用 传名调用 Javascript是传值调用的，Thunk函数是编译器“传名调用”的实现，就是将参数放入一个临时函数中，再将这个临时函数放入函数体，这个临时函数就叫做Thunk函数。 举个栗子就好懂了： example 21: 1234567891011121314151617function f(m)&#123; return m * 2;&#125; var x = 1; f(x + 5); //等同于 var thunk = function (x) &#123; return x + 5;&#125;; function f(thunk)&#123; return thunk() * 2;&#125; Thunk函数本质上是函数柯里化(currying)，柯里化进行参数复用和惰性求值，这个是函数式编程的一些技巧，在js中，我们可以利用高阶函数实现函数柯里化。 JavaScript语言的Thunk函数 在JavaScript语言中，Thunk函数替换的不是表达式，而是多参数函数，将其替换成单参数的版本，且只接受回调函数作为参数。 example 22: 1234567891011121314var fs = require('fs');// 正常版本的readFile（多参数版本）fs.readFile(fileName, callback); // Thunk版本的readFile（单参数版本）var readFileThunk = Thunk(fileName);readFileThunk(callback); var Thunk = function (fileName)&#123; return function (callback)&#123; return fs.readFile(fileName, callback); &#125;;&#125;; 任何函数，只要参数有回调函数，就能写成Thunk函数的形式。以下是一个简单的Thunk函数转换器： example 23: 123456789var Thunk = function(fn)&#123; return function ()&#123; var args = Array.prototype.slice.call(arguments); return function (callback)&#123; args.push(callback); return fn.apply(this, args); &#125; &#125;; &#125;; 从本质上说，我们借助了Javascript高阶函数来抽象了异步执行流程。 使用上面的转换器，生成fs.readFile的Thunk函数。 example 24: 1234var readFileThunk = Thunk(fs.readFile);readFileThunk('./text1.txt', 'utf8')(function(err, data)&#123; console.log(data);&#125;); 可以使用thunkify模块来Thunk化任何带有callback的函数。 我们需要借助Thunk函数的能力来自动执行Generator函数。 下面是一个基于Thunk函数的Generator函数执行器。 example 25: 1234567891011121314//Generator函数执行器function run(fn) &#123; var gen = fn(); function next(err, data) &#123; var result = gen.next(data); if (result.done) return; result.value(next); &#125; next();&#125; run(gen); 我们马上拿这个执行器来做点事情。 example 26: 1234567891011121314151617181920212223242526var fs = require('fs');var thunkify = require('thunkify');var readFile = thunkify(fs.readFile); var gen = function* ()&#123; var f1 = yield readFile('./text1.txt', 'utf8'); console.log(f1); var f2 = yield readFile('./text2.txt', 'utf8'); console.log(f2); var f3 = yield readFile('./text3.txt', 'utf8'); console.log(f3);&#125;; function run(fn) &#123; var gen = fn(); function next(err, data) &#123; var result = gen.next(data); if (result.done) return; result.value(next); &#125; next();&#125; run(gen); //自动执行 现在异步操作代码的写法就和同步的写法一样了。实际上，Thunk函数并不是自动控制Generator函数执行的唯一方案，要自动控制Generator函数的执行过程，需要有一种机制，自动接收和交还程序的执行权，回调函数和Promise都可以做到(利用调用自身的一些特性)。 yield *语句普通的yield语句后面跟一个异步操作，yield 语句后面需要跟一个遍历器，可以理解为yield 后面要跟另一个Generator函数，讲起来比较抽象，看一个实例。 example 27: 1234567891011121314151617181920212223242526272829303132333435363738//嵌套异步操作流var fs = require('fs');var thunkify = require('thunkify');var readFile = thunkify(fs.readFile); var gen = function* ()&#123; var f1 = yield readFile('./text1.txt', 'utf8'); console.log(f1); var f_ = yield * gen1(); //此处插入了另外一个异步流程 var f2 = yield readFile('./text2.txt', 'utf8'); console.log(f2); var f3 = yield readFile('./text3.txt', 'utf8'); console.log(f3);&#125;; var gen1 = function* ()&#123; var f4 = yield readFile('./text4.txt', 'utf8'); console.log(f4); var f5 = yield readFile('./text5.txt', 'utf8'); console.log(f5);&#125; function run(fn) &#123; var gen = fn(); function next(err, data) &#123; var result = gen.next(data); if (result.done) return; result.value(next); &#125; next();&#125; run(gen); //自动执行 上面这个例子会输出： 1 4 5 2 3 也就是说，使用yield *可以在一个异步操作流程中直接插入另一个异步操作流程，我们可以据此构造可嵌套的异步操作流，更为重要的是，写这些代码完全是同步风格的。 目前业界比较流行的Generator函数自动执行的解决方案是co库，此处也只给出co的例子。顺带一提node-fibers也是一种解决方案。 顺序执行3个异步读取文件的操作，并依次输出文件内容： example 28: 1234567891011121314151617181920212223var fs = require('fs');var co = require('co');var thunkify = require('thunkify');var readFile = thunkify(fs.readFile); co(function*()&#123; var files=[ './text1.txt', './text2.txt', './text3.txt' ]; var p1 = yield readFile(files[0]); console.log(files[0] + ' -&gt;' + p1); var p2 = yield readFile(files[1]); console.log(files[1] + ' -&gt;' + p2); var p3 = yield readFile(files[2]); console.log(files[2] + ' -&gt;' + p3); return 'done';&#125;); 并发执行3个异步读取文件的操作，并存储在一个数组中输出(顺序和文件名相同)： example 29: 123456789101112131415var fs = require('fs');var co = require('co');var thunkify = require('thunkify');var readFile = thunkify(fs.readFile);co(function* () &#123; var files = ['./text1.txt', './text2.txt', './text3.txt']; var contents = yield files.map(readFileAsync); console.log(contents);&#125;); function readFileAsync(filename) &#123; return readFile(filename, 'utf8');&#125; co库和我们刚才的run函数有点类似，都是自动控制Generator函数的流程。 ES 7中的async和awaitasync和await是ES 7中的新语法，新到连ES 6都不支持，但是可以通过Babel一类的预编译器处理成ES 5的代码。目前比较一致的看法是async和await是js对异步的终极解决方案。 async函数实际上是Generator函数的语法糖(js最喜欢搞语法糖，包括ES 6中新增的“类”支持其实也是语法糖)。 配置Babel可以看：配置Babel 如果想尝个鲜，简单一点做法是执行 sudo npm install –global babel-cli readFile.js代码如下： 12345678910111213141516171819var fs = require('fs');var readFile = function (fileName)&#123; return new Promise(function (resolve, reject)&#123; fs.readFile(fileName, function(error, data)&#123; if (error) reject(error); resolve(data); &#125;); &#125;);&#125;; var asyncReadFile = async function ()&#123; var f1 = await readFile('./text1.txt'); var f2 = await readFile('./text2.txt'); console.log(f1.toString()); console.log(f2.toString());&#125;; asyncReadFile(); 接着执行：babel-node readFile.js 输出： 1 2 光是这个例子还不足以理解async/await，接下来稍微深入一点。首先要注意的一个问题是，await只能用在async函数中，但async函数中未必一定要有await。 按照上面实例的写法，asyncReadFile函数中读取了两个文件text1.txt和text2.txt，这里是顺序读取的，如果用回调的方式写，其实是这样的： 123456789101112131415161718192021var asyncReadFile = function(cb)&#123; var res = &#123;&#125;; fs.readFile('./text1.txt', function(error, data)&#123; if (error) &#123; return cb(err); &#125; res.f1 = data; fs.readFile('./text1.txt', function(error, data)&#123; if (error) &#123; return cb(err); &#125; res.f2 = data; return cb (null, res); &#125;); &#125;);&#125; asyncReadFile(function(err, data)&#123; console.log(data.f1.toString()); console.log(data.f2.toString());); 以上代码先读取text1.txt，然后读取text2.txt，最后再输出两个文件的内容。 不知不觉就写了两层嵌套，如果需要顺序执行的异步调用更多，这样的代码简直无法直视，后期改需求或者重构会很痛苦。 所以现在我们知道了，readFile.js中的这种写法的异步调用是顺序执行的。可以再来看一个更加直观的例子，写一个async函数，里面去请求几个远端的API，在API中故意写成延迟一段时间后再返回响应，这里只截取关键代码： node koa中间层： 123456// 顺序异步执行await baseService.request('test/test1'); await baseService.request('test/test2');await baseService.request('test/test3'); await ctx.render('blog/hi', &#123;&#125;); 后端API： 12345678910111213141516171819202122232425262728'use strict';var test1 = function(req, res) &#123; res.header(\"Access-Control-Allow-Origin\", \"*\"); setTimeout(function()&#123; return res.json(&#123;result: 1, message: 'msg1'&#125;); &#125;, 1000);&#125;; var test2 = function(req, res) &#123; res.header(\"Access-Control-Allow-Origin\", \"*\"); setTimeout(function()&#123; return res.json(&#123;result: 1, message: 'msg2'&#125;); &#125;, 2000);&#125;; var test3 = function(req, res) &#123; res.header(\"Access-Control-Allow-Origin\", \"*\"); setTimeout(function()&#123; return res.json(&#123;result: 1, message: 'msg3'&#125;); &#125;, 5000);&#125;; module.exports = function (router) &#123; router.get('/test1', test1); router.get('/test2', test2); router.get('/test3', test3);&#125;; 这里的设计是这样的： 浏览器 &lt;----&gt; node koa中间层 &lt;----&gt; 后端API 可以看一下node koa中间层具体返回的时间： GET /hi - 8170ms 大概是8000ms，大致等于test1,test2和test3接口响应时间之和。很显然，这里这种写法是异步顺序执行，这适用于后面的异步调用依赖于前面异步调用的情况。 那如果我们需要多个异步调用并发执行，也就是说这几个异步调用之间没有依赖关系，则需要使用Promise.all的帮助了，把node koa中间层变成这样： node koa中间层： 123456// 并发异步执行await Promise.all([ baseService.request('test/test1'), baseService.request('test/test2'), baseService.request('test/test3')]); 后端API代码不变，node koa中间层具体返回的时间： GET /hi - 5070ms 返回时间缩短为5000ms，这大致等于test1,test2和test3种响应时间最长的那个(test3接口)，那么就说明使用Promise.all可以让传入的异步调用变为并发执行的，互不干扰。 如果不想用Promise.all，还有一种写法也可以做到并发异步执行： 1234567891011// 并发异步执行var res = [];var tasks = [ baseService.request('test/test1'), baseService.request('test/test2'), baseService.request('test/test3')]; for (let task of tasks)&#123; res.push(await task);&#125;","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"js","slug":"js","permalink":"https://nullcc.github.io/tags/js/"}]},{"title":"(译)Redis键空间通知","slug":"(译)Redis键空间通知","date":"2018-03-01T16:00:00.000Z","updated":"2022-04-15T03:41:13.011Z","comments":true,"path":"2018/03/02/(译)Redis键空间通知/","link":"","permalink":"https://nullcc.github.io/2018/03/02/(译)Redis键空间通知/","excerpt":"本文翻译自Redis Keyspace Notifications。","text":"本文翻译自Redis Keyspace Notifications。 重要：键空间通知是从Redis 2.8.0起才有的功能。 功能概述键空间通知允许客户端订阅发布/订阅信道来接收以某种方式影响Redis数据集的事件。 可以接收的事件示例如下： 影响给定键的所有命令。 键接收到LPUSH操作。 键在0号数据库中过期。 事件使用普通Redis的发布/订阅层来，因此实现了发布/订阅功能的客户端可以在不修改的情况下使用这个功能。 由于Redis的发布/订阅当前是发后即忘模式的，因此如果你的应用需要可靠的事件通知，则无法满足，也就是说，如果你的发布/订阅客户端断开连接并在稍后重连，客户端断开的这段时间内的所有事件通知都会丢失。 在未来有计划实现更加可靠的事件通知，但是这可能会在更一般的层面上解决，这也可能给发布/订阅本身带来可靠性，或允许使用Lua脚本拦截发布/订阅消息来执行类似将事件加入到一个列表中这样的操作。 事件的类型键空间通知被实现成对每个影响Redis数据集的操作发送两个不同类型的事件。例如，一个在0号数据库中对名称为mykey的键执行DEL操作将触发推送两条消息，相当于执行了下面两条PUBLISH命令： PUBLISH __keyspace@0__:mykey del PUBLISH __keyevent@0__:del mykey 很容易看出一个信道可以监听所有针对键mykey的事件，另一个信道允许获取对所有键执行删除操作的信息。 第一类事件，以keyspace为前缀的信道被称为键-空间通知，第二类事件，以keyevent为前缀的信道被称为键-事件通知。 In the above example a del event was generated for the key mykey. What happens is that: 上面的例子中针对键mykey生成了一个删除事件。具体如下： 键-空间信道以消息的方式接收事件名称。 键-事件信道以消息的方式接收键名称。 仅提供我们感兴趣的事件子集的其中一种类型的通知是可能的。 配置默认情况下，键空间事件通知是被禁用的，因为这个功能会消耗一些CPU，默认打开是不明智的。可以在redis.conf文件中使用notify-keyspace-events或CONFIG SET命令启用它。 将参数设置为空字符串会禁用通知。为了启用该功能，必须使用一个非空字符串作为参数，参数由多个字符组成，每个字符都有特别的含义，如下表所示： K 键空间事件，以__keyspace@&lt;db&gt;__为前缀发布消息 E 键事件事件，以__keyevent@&lt;db&gt;__为前缀发布消息 g 通用命令（非特定类型），例如 DEL、 EXPIRE、 RENAME等 $ 字符串命令 l 列表命令 s 集合命令 h 哈希命令 z 有序集合命令 x 键过期事件（每次当一个键过期时生成的事件） e 键淘汰事件（当一个键由于内存超量导致被淘汰时生成的事件） A 参数g$lshzxe的别名，因此&quot;AKE&quot;字符串表示所有的事件 K或E至少有一个应该出现在参数字符串中，否则，无论字符串的其他部分是什么，都不会推送事件。 例如，为了只针对列表开启键-空间事件，配置参数必须设置为Kl，以此类推。 字符串KEA可以用来开启任何可能的事件。 由不同命令生成的事件不同的命令会生成不同类型的事件，如下面的列表： DEL命令会对每一个被删除的键生成一个del event。 RENAME命令会生成两个事件，对源键生成一个rename_from事件，对目标键生成一个rename_to事件。 EXPIRE命令在对一个键设置过期时间时生成一个expire event，或在每次使用正数时间对一个键设置过期时间后导致键因过期被删除时生成一个expired event（查看EXPIRE的文档来获取更多信息）。 SORT命令在使用STORE设置一个新建时会生成一个sortstore event。如果结果列表为空，并使用了STORE选项，且相同的键名已经存在，结果就是这个键被删除，因此这种情况下会生成一个del event。 SET和它的左右变种(SETEX、SETNX、GETSET)会生成set events。然而SETEX同时还会生成一个expire events。 MSET会对每个键生成一个独立的set event。 SETRANGE命令生成一个setrange event。 INCR、DECR、INCRBY、DECRBY命令都会生成incrby events。 INCRBYFLOAT命令会生成一个incrbyfloat events。 APPEND命令会生成一个append event。 LPUSH和LPUSHX命令会生成一个单一的lpush event，即使是在复杂情况下。 RPUSH和RPUSHX命令会生成一个单一的rpush event，即使是在复杂情况下。 RPOP命令会生成一个rpop event。如果列表中的最后一个元素被弹出，还会额外生成一个del event。 LPOP命令会生成一个lpop event。如果列表中的第一个元素被弹出，还会额外生成一个del event。 LINSERT命令会生成一个linsert event。 LSET命令会生成一个lset event。 LREM命令会生成一个lrem event，如果结果列表为空且键被删除将额外生成一个del event。 LTRIM命令会生成一个ltrim event，如果结果列表为空且键被删除将额外生成一个del event。 RPOPLPUSH和BRPOPLPUSH命令会生成一个rpop event和一个lpush event。在这两种情况下，顺序都是有保证的（lpush event将总是在rpop event之后被推送）。如果结果列表长度为0且键被删除将额外生成一个del event。 HSET、HSETNX和HMSET都会生成一个单一的hset event。 HINCRBY命令会生成一个hincrby event。 HINCRBYFLOAT命令会生成一个hincrbyfloat event。 HDEL命令会生成一个单一的hdel event，如果结果哈希为空且键被删除将额外生成一个del event。 SADD命令会生成一个单一的sadd event，即使是在复杂情况下。 SREM命令会生成一个单一的srem event，如果结果集合为空且键被删除将额外生成一个del event。 SMOVE命令会对源键生成一个srem event，且对目标键生成一个sadd event。 SPOP命令会生成一个spop event，如果结果集合为空且键被删除将额外生成一个del event。 SINTERSTORE、SUNIONSTORE、SDIFFSTORE会分别生成sinterstore event、sunionostore event、sdiffstore event。在集合为空且被存储的键已经存在的特殊情况下，由于key会被删除会生成一个del event。 ZINCR命令会生成一个zincr event。 ZADD命令会生成一个单一的zadd event，即使有多个元素一次性被添加。 ZREM命令会生成一个的那一的zrem event，即使有多个元素一次性被删除。当结果有序集合为空且键被删除，会生成一个额外的del event。 ZREMBYSCORE命令会生成一个单一的zrembyscore event。当结果有序集合为空且键被删除，会生成一个额外的del event。 ZREMBYRANK命令会生成一个单一的zrembyrank event。当结果有序集合为空且键被删除，会生成一个额外的del event。 ZINTERSTORE和ZUNIONSTORE会分别生成zinterstore event和zunionstore event。在有序集合为空且被存储的键已经存在的特殊情况下，由于key会被删除会生成一个del event。 每次由于键的过期时间到导致键被从数据集中删除时，会生成一个expired event。 每次由于Redis超过最大内存限制，使用maxmemory policy释放内存导致一个键从数据集中被淘汰时，会生成一个evicted event。 重要：只有在目标键真正被修改时，所有命令才会生成事件。例如一个SREM命令从一个集合中删除了一个不存在的元素，这实际上不会对键造成改变，因此不会生成任何事件。 如果对给定命令如何生成事件存有疑问，最简单的做法就是自己测试一下： $ redis-cli config set notify-keyspace-events KEA $ redis-cli --csv psubscribe &apos;__key*__:*&apos; Reading messages... (press Ctrl-C to quit) &quot;psubscribe&quot;,&quot;__key*__:*&quot;,1 此时在另一个终端中使用redis-cli发送命令给Redis服务器并观察事件的生成： &quot;pmessage&quot;,&quot;__key*__:*&quot;,&quot;__keyspace@0__:foo&quot;,&quot;set&quot; &quot;pmessage&quot;,&quot;__key*__:*&quot;,&quot;__keyevent@0__:set&quot;,&quot;foo&quot; ... Timing of expired events 过期事件计时有生存时间的键在Redis中过期有两种方式： 当键被一个命令访问且发现键已过期。 通过在后台系统中运行定时任务增量地查找过期键，这样能够收集到那些从未被访问键。 当一个键被访问且被上述两种方法之一发现已过期时会生成expired events，这就会导致一个结果，Redis服务器将无法保证在键的生存时间变为0时立即生成expired events。 如果没有命令不断地访问键，且有很多的键被设置了生存时间，则在键的生存时间下降到0和生成expired events这之间会有一个显著的延迟。 基本上来说，Redis服务器是在删除过期键时，而不是在键的生存时间理论上变为0时生成expired events。","categories":[{"name":"文档翻译","slug":"文档翻译","permalink":"https://nullcc.github.io/categories/文档翻译/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://nullcc.github.io/tags/Redis/"}]},{"title":"(译)Redis事务","slug":"(译)Redis事务","date":"2018-02-28T16:00:00.000Z","updated":"2022-04-15T03:41:13.010Z","comments":true,"path":"2018/03/01/(译)Redis事务/","link":"","permalink":"https://nullcc.github.io/2018/03/01/(译)Redis事务/","excerpt":"本文翻译自Transactions。","text":"本文翻译自Transactions。 MULTI，EXEC，DISCARD和WATCH是Redis中事务的基础。它们允许将一组命令一起执行，这里有两个重要保证： 事务中的所有命令都是串行地被序列化和解析的。永远不会发生这种情况：在一个Redis事务的执行过程中，其他客户端的请求被插入到事务中间执行这种事情。这保证了事务中的所有命令是作为一个不可分离的整体被执行的。 要么所有的命令都没有被处理，因此Redis事务也是原子性的。EXEC命令触发事务中所有命令的执行，因此，如果一个客户端在调用MULTI命令之前丢失了服务器的连接，那么将不会执行任何命令，相反，如果调用了EXEC命令，则所有操作都会被执行。当使用AOF文件时，Redis将确保使用后一个单一的write(2)系统调用将事务写到磁盘上。然而，如果Redis服务器崩溃或被系统管理员强制杀掉的话，可能只有部分操作被写入磁盘。Redis会在重启时检测到这种情况，然后以一个错误退出。使用redis-check-aof工具可能可以修复AOF文件，这个工具将会移除那些未能被完整记录下来的部分事务，以便让服务器可以再次启动。 从Redis 2.2开始，Redis提供了上述两个保证，以乐观锁的形式，非常类似于一个检查-设置(CAS)操作。这些会在本文后面说明。 用法一个Redis事务使用MULTI命令开启。这个命令总是使用OK作为响应。用户可以发送多个命令让服务器一次执行。Redis将对这些命令进行排队，而不是立刻执行它们。事务中的所有命令将在执行EXEC命令时被执行。 相反，调用DISCARD命令将刷新事务队列并退出事务。 下面的例子原子性地对键foo和bar执行自增。 &gt; MULTI OK &gt; INCR foo QUEUED &gt; INCR bar QUEUED &gt; EXEC 1) (integer) 1 2) (integer) 1 从上面的信息可以看到，EXEC命令返回一个回复的数组，数组中的各个元素是事务内部每个单独命令被执行后的回复，回复数组中元素的顺序和事务中命令的顺序一致。 当一个Redis连接处于一个MULTI请求上下文中时，所有命令的回复都是字符串QUEUED（从Redis协议的角度看着是一个状态回复）。当执行EXEC命令时，Redis会处理队列中的命令。 事务中的错误在事务中有可能遇到两种命令错误： 命令可能无法进行排队，因此在执行EXEC之前可能会有错误。例如可能命令存在语法错误（错误的参数个数，错误的命令名称等），或者可能有一些临界条件比如内存不足（如果我们使用maxmemory指令配置了系统内存上限的话）。 一个命令可能在调用EXEC之后执行失败，例如我们对一个键执行了操作的操作（像是对一个字符串调用了一个列表操作）。 客户端通常通过在执行EXEC命令之前检查命令排队的返回值来感知第一类错误：如果命令返回QUEUED就表示命令排队成功，否则Redis会返回一个错误。如果这里在对命令排队时有一个错误，大多数客户端将终止并丢弃事务。 然而从Redis 2.6.5开始，服务器将记住在命令的添加过程中出现了一个错误，并将在执行EXEC时返回一个错误来拒绝执行事务，并自动丢弃事务。 Redis 2.6.5之前的做法是执行排队成功的那部分命令，万一客户端忽略之前的错误而调用了EXEC命令。新的做法使得将事务和流水线相结合更加简单，因此可以立即发送整个事务，并在稍后一起读取所有回复。 执行EXEC命令之后发生的错误的处理方式也没什么特别的：即使事务中有一些命令执行失败，其他命令也会被正常地执行。 这在协议级别上看得更清楚。在厦门的示例中，即使语法正确，其中一个命令的执行也会失败： Trying 127.0.0.1... Connected to localhost. Escape character is &apos;^]&apos;. MULTI +OK SET a 3 abc +QUEUED LPOP a +QUEUED EXEC *2 +OK -ERR Operation against a key holding the wrong kind of value EXEC命令返回两个元素的字符串回复，一个是OK代码另一个是-ERR回复。然后由客户端用一种合理的方式将错误报告给用户。 需要注意的很重要的一点是，即使一个命令失败，队列中的其他命令也会被执行——Redis不会停止处理命令。 另一个例子，再次使用telnet协议来，展示了如何尽快报告语法错误： MULTI +OK INCR a b c -ERR wrong number of arguments for &apos;incr&apos; command 这次存在语法错误，INCR命令的参数由问题呢，这个命令不会被排队。 为何Redis不支持回滚？如果你有使用关系型数据库的背景，事实上Redis在事务中执行命令是可以失败的，但Redis也将继续执行事务中剩余的命令而不是回滚，这对你来说看上去可能会很奇怪。 然而，这种做法有它的好处： Redis命令只能在语法错误（这种问题在命令排队时无法被检测到）或对数据类型执行了错误的操作时才能失败：这意味着在实际情况下，失败的命令是编程错误的结果，以及一种在开发环境就能被检测到的错误，而不是在生产环境中。 Redis内部简化和速度快的原因是因为它不需要回滚的能力。 一个观点认为当错误发生时，回滚并不能把你从编程错误中拯救出来。例如，一个查询对一个键自增2而不是1，或者对错误的键执行了自增，此时回滚机制没有任何帮助。考虑到没有人可以把程序员从他或她自己的错误中拯救出来，而且对一个Redis命令的错误使用不太可能进入生产环境，在错误发生时我们选择了不支持回滚这种更简单和更快速的方式。 丢弃命令队列DISCARD命令可以用来终止一个事务。在这种情况下，不会执行任何命令，且连接的状态会恢复到正常状态。 &gt; SET foo 1 OK &gt; MULTI OK &gt; INCR foo QUEUED &gt; DISCARD OK &gt; GET foo &quot;1&quot; 使用检查-设置(CAS)的乐观锁定WATCH命令用来给Redis事务提供检查-设置(CAS)的行为。 在需要被监视的键上执行WATCH命令以便检测这些键的修改。如果在执行EXEC命令之前有至少一个被监视的键被修改了，则整个事务将终止，且EXEC命令会返回Null回复给客户端来通知事务失败了。 例如，想象我们需要自动对一个键执行自增1的操作（让我们假设Redis没有INCR命令）。 首次尝试可能是这样的： val = GET mykey val = val + 1 SET mykey $val 上面的代码只有在一段时间内仅有一个客户端对键进行操作时才是可靠的。如果多个客户端都在同一时间尝试对键执行自增操作就会造成竞争条件。例如，客户端A和客户端B将读取到旧的值，比如10。键的值将被两个客户端自增到11，并最后使用SET命令将键的值设置为11。所以最后的值是11而不是12. 幸好我们可以WATCH命令来很好地解决这个问题： WATCH mykey val = GET mykey val = val + 1 MULTI SET mykey $val EXEC 使用上面的代码，如果有竞争条件且另一个客户端在我们调用WATCH命令和EXEC命令之间修改了val的值，事务将会失败。 我们只需要重复这个操作，这次我们将不会遇到新的竞争条件。这种形式的锁定叫做乐观锁定，这是一种非常强大的锁定方式。在很多用例中，多客户端将访问不同的键，所以不太可能发生冲突，通常不需要重复进行操作。 WATCH命令解析那么WATCH命令到底是什么呢？它是一个能让EXEC命令以条件执行的命令：我们让Redis在没有被监控的键被修改的情况下执行事务。（但他们的值可能被相同的客户端在事务中改变却不会终止事务。更多关于这种情况的信息请看这里）否则事务根本不会被执行。（注意如果使用WATCH命令监控了一个很容易改变的键，且在你对其执行WATCH命令后键过期了，EXEC命令仍会执行。更多关于这种情况的信息请看这里） WATCH命令可以被多次调用。简单地说，所有的WATCH调用都将在调用开始观察键的改变情况时造成一些影响，直到EXEC命令被调用。你也可以在一个单一个的WATCH调用中给其传入任意数量的键。 当EXEC被调用时，所有的键都会变为未被监控的状态，而不管事务是否被终止。当一个客户端连接关闭时，所有键也会变成未被监控的状态。 还可以使用UNWATCH命令（没有参数）来刷新所有被监控的键。这在有时候我们乐观地锁定了一些键时很有用，因为我们可能需要执行一个事务来改变这些键，但是在读取了键的当前内容后，我们决定放弃我们的更改。如果发生这种情况，我们只要调用UNWATCH以便连接还可以被新的事务自由地使用。 使用WATCH命令里实现ZPOP一个能很好地说明如何使用WATCH命令来创建新的原子操作的例子是Redis没有提供ZPOP的实现，这个命令将从一个有序列表中以原子性地弹出评分最低的元素。这里有一个最简单的实现： WATCH zset element = ZRANGE zset 0 0 MULTI ZREM zset element EXEC 如果EXEC执行失败（即返回一个Null回复），主需要重复怎么操作即可。 Redis脚本和事务从定义上说一个Redis脚本是事务性的，所以你可以使用一个脚本来做任何在原来Redis事务内做的事情，并且一般来说使用脚本会更简单且更快一些。 这种重复是因为实际上脚本功能是在Redis 2.6以后被引入的，而事务功能很早以前就有了。然而，我们不可能在短期内移除事务的支持，因为它在语义上是合适的，即使不适使用Redis脚本还是可以避免竞争条件，特别是因为Redis的事务实现复杂度比较小。 然而，短期内我们不太可能看到整个用户群体都在使用Redis脚本。如果发生这种情况，我们可能会使事务成为一个过时的功能并最终移除它。","categories":[{"name":"文档翻译","slug":"文档翻译","permalink":"https://nullcc.github.io/categories/文档翻译/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://nullcc.github.io/tags/Redis/"}]},{"title":"(译)使用Redis实现分布式锁","slug":"(译)使用Redis实现分布式锁","date":"2018-02-24T16:00:00.000Z","updated":"2022-04-15T03:41:13.012Z","comments":true,"path":"2018/02/25/(译)使用Redis实现分布式锁/","link":"","permalink":"https://nullcc.github.io/2018/02/25/(译)使用Redis实现分布式锁/","excerpt":"本文翻译自Distributed locks with Redis。","text":"本文翻译自Distributed locks with Redis。 在许多环境中，分布式锁是一个非常有用的原语，不同进程必须以互斥的方式对共享资源进行操作。 这里有一些库和博客文章描述了如何使用Redis实现一个DLM（分布式锁管理器），但每个库都是用不同的方式实现，和稍微复杂一些的设计实现相比，许多实现都过于简单以至于可靠性不高。 本文为试图使用Redis实现分布式锁提供了更加规范的算法。我们提出了一个算法，叫做Redlock，我们相信它实现的DML要比普通的实现更好。我们希望社区对它进行分析，提供反馈，并将其作为分布式锁的实现或成为更复杂设计替代品的起点。 各种实现在描述算法之前，这里提供了一些可供参考的实现链接。 Redlock-rb (Ruby实现)。这里还有一个redlock-rb的分支提供了更简易的分布式特性。 Redlock-py（Python实现）。 Aioredlock（异步的Python实现）。 Redlock-php（PHP实现）。 PHPRedisMutex（进一步的PHP实现）。 cheprasov/php-redis-lock（PHP的Redis锁库）。 Redsync.go（Go实现）。 Redisson（Java实现）。 Redis::DistLock（Perl实现）。 Redlock-cpp（C++实现）。 Redlock-cs（C#/.NET实现）。 RedLock.net（C#/.NET实现）。包含异步和锁扩展的支持。 ScarletLock（包含可配置数据源的C#/.NET实现）。 node-redlock（NodeJS实现）。包含锁扩展支持。 安全性和活跃性保证我们将使用三个属性来建模我们的设计，从我们的观点来看，这是以有效的方式使用分布式锁的最低保证。 安全属性：互斥性。在任何时刻，只有一个客户端能持有锁。 活跃性属性A：不会发生死锁。即使当持有锁的客户端崩溃或出现网络割裂，最终也可以获取一个锁。 活跃性属性B：容错性。只要大多数Redis节点存活，客户端就可以获取和释放锁。 为什么基于故障转移的实现还不够？要了解需要做哪些改进，先让我们分析一下当前大多数基于Redis的分布式锁代码库的情况。 使用Redis来对一个资源加锁最简单的方式是创建一个键。一般使用Redis的键过期功能对这个键设置一个生存时间，因此最终这个键将被释放（即在上个小节中提到的第二个属性）。当客户端要释放资源时，删除这个键即可。 从表面上看这种方式工作得很好，但这里有一个问题：在我们的架构中有一个单点失败的问题。当Redis主节点崩溃会发生什么？我们可以增加一个从节点！当主节点不可用时我们就使用从节点。不幸地是，这不可行。这样做我们无法实现互斥的安全属性，因为Redis的复制是异步进行的。 这个模型中有一个很明显的竞争条件： 客户端A从主节点上获取锁。 主节点在将键复制给从节点之前崩溃。 从节点被提升为主节点。 客户端B针对同一个资源获取了客户端A正在持有的锁。这是不安全的！ 有时，在特殊情况下，比如在故障期间，多个客户端可以同时持有锁，这是非常好的。这种清下你可以使用基于复制的方案。否则我们建议你以本文档描述的方案进行实现。 单实例情况下的正确实现在试图客服上面描述的单例情况的局限之前，让我们来看看如何正确处理这种简单的情况，因为在一些可以接受时不时出现竞态条件的应用中这是一种可行的解决方案，因为对单实例在执行对资源进行锁定是我们在这里描述的分布式算法的基础。 获取锁的方式如下： SET resource_name my_random_value NX PX 30000 上述命令将只在键不存在的情况下（使用NX选项）设置键，键过期时间为30000毫秒（使用PX选项）。键的值被设置为”myrandomvalue”。这个值必须在所有客户端和所有锁请求中都是唯一的。 基本上来说，键的随机值是用来用一种安全的方式来释放锁，使用一段脚本来告诉Redis：只有在键存在且键的值是我们期望的那个值的时候删除这个键。这可以使用下面的Lua脚本来完成： if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then return redis.call(&quot;del&quot;,KEYS[1]) else return 0 end 一件很重要的事情是避免移除一个由其他客户端创建的锁。比如一个客户端可能持有锁，并长时间阻塞在某个操作上，阻塞的时间超过了锁的有效时间（即那个键的过期时间），锁过期以后，由其他客户端持有，接着这个被阻塞的客户端又再次释放锁。仅仅使用DEL来释放锁是不安全的，因为这有可能会误移除掉其他客户端持有的锁。使用上面的脚本，每个锁都使用一个随机字符串进行“签名”，以便只有创建该锁的客户端可以释放它。 这个随机字符串应该是什么样子的？我们假设它是一个由/dev/urandom创建的20字节的字符串，但你可以使用开销更低的方式来使其足够唯一以适应你的任务。例如一个安全的选择是使用/dev/urandom的输出作为RC4加密算法的种子，并从中生成伪随机流。一个更简单的解决方案是使用UNIX的微秒级精度的时间，和客户端ID进行连接，这不是绝对安全的，但是大多数环境下都能完成任务。 我们使用的键有效期时间，被称为“锁有效时间”。它同时也是锁的自动释放时间，并且也是在另一个客户端可能再次获取到锁之前，当前客户端执行操作需要的时间，这在技术上可以保证互斥性，但这仅限于从获取锁的时刻开始到给定的窗口时间结束这段时间内。 因此我们现在有了一个获取和释放锁的好方法。可以推论得到，关于一个由单一实例组成的非分布式系统，这种方式总是可用的且安全的。让我们将这个概念扩展到没有这种保证的分布式系统中。 Redlock算法在该算法的分布式版本中我们假设我们有N个Redis主节点。这些节点是完全独立的，因此我们不适用复制或者任何隐式协调系统。我们已经描述了在一个单实例中如何安全地获取和释放锁。我们想当然地认为该算法使用这种方式在单个实例上获取和释放锁。在我们的例子中设置了N=5，这是一个合理的值，因此我们需要在不同的计算机或虚拟机上运行5个Redis主节点以确保它们的崩溃不会相互关联。 为了获取锁，客户端需要执行以下操作： 以毫秒级精度获取当前时间。 客户端尝试使用相同的key和相同的随机值从所有的N个Redis实例中依次获取锁。在步骤2中，当在每个实例上设置锁时，客户端使用一个比锁自动释放时间小的超时时间以便获取这个锁。例如如果锁的自动释放时间为10秒，超时是时间可以被设置在5~50毫秒这个范围内。这防止了客户端长时间阻塞在那些已经崩溃的节点上：如果一个Redis实例不可用，我们将尽快尝试和下一个实例建立连接。 客户端通过用当前时间戳减去在步骤1中获取的时间戳计算获取锁所需的时间。仅当客户端能够从大多数Redis实例中（在本例中至少3个）获取锁时，并且获取锁花费的总时间比锁的有效时间短时，该锁才被认为是可以获取的。 如果客户端获取了锁，它的有效时间将被设置为初始有效时间减去获取锁花费的时间，即在第3步中计算出来的时间。 如果客户端由于某种原因获取锁失败（要么客户端无法从N/2+1个实例中获取锁，要么计算出的锁有效时间为负数），客户端将尝试释放在所有实例中的锁（即使是对那些它认为无法获取锁的实例）。 这个算法是异步的吗？这个算法依赖于一个假设，即进程之间没有同步时钟，然而，每个进程内的本地时钟近似以相同的速率流逝，其误差小于锁的自动释放时间。这个假设很像现实世界中的计算机：每台计算机都有一个本地时钟且我们通常可以依赖不同的计算机来实现一个小的时钟漂移。 在这一点上我们需要更更好地指定我们的互斥原则：只有当持有锁的客户端在锁的有效期（即在步骤3中计算出的锁有效时间）减去一些时间（只需几毫秒，以补偿进程间的时钟漂移）内完成它的工作时，才能保证该算法的有效性。 有关需要绑定时钟漂移的类似系统的更多信息，本文是一个有趣的参考：租约：一种高效的分布式文件缓存容错机制。 失败重试当一个客户端无法获取锁时，它应该尝试在一个随机延迟后再次获取锁以便尽量去同步多个客户端尝试在相同时间对相同资源获取锁（这引发导致脑裂状态导致没有任何一个客户端能获取到锁）。客户端越快去尝试在大部分Redis实例中获取锁，脑裂状态的窗口时间就越小（以及重试的必要性），因此在理想情况下，客户端应该尝试使用多路复用同时向N个Redis实例发送SET命令。 对于未能获取到大部分锁的客户端来说，非常重要的一点是，要尽快释放那些部分获取的锁，因此没有必要等锁过期后再一次获取锁（然而，如果发生了网络分区导致客户端再也无法和Redis实例通信，还有一个补救方案就是等待键过期）。 释放锁释放锁很简单，只需释放所有Redis实例上的锁，不管客户端是否认为它能够成功锁定实例。 安全参数这个算法安全吗？我们可以来试着理解看看在不同场景下会发生什么。 首先，让我们假设一个客户端可以在大部分Redis实例上获取锁。所有Redis实例都将包含相同生存时间的键。然而，所有键的设置时间不同，因此所有键的过期时间也不同。但是如果第一个键的过期时间为T1（即我们对第一台服务器采样得到的时间），最后一个键的过期时间为T2（即我们对最后一台服务器采样得到的时间），我们保证第一个键的过期时间至少不能小于MIN_VALIDITY=TTL-(T2-T1)-CLOCK_DRIFT。所有其他Redis实例上的键将在之后过期，因此我们保证至少在这段时间内，各个Redis实例上的键将被同时设置。 在这段时间内，大部分Redis实例上的键被设置了，另一个客户端将无法获取到这个锁，因为当N/2+1个键已经存在时，N/2+1 SET NX操作无法成功。所以如果一个锁被获取了，则不可能在同一时间再次被获取（这违反互斥性质）。 然而，我们也要确保多个客户端同时尝试过去同一个锁是无法成功的。 如果一个客户端以一个接近或大于锁最大有效期的时间锁定了大部分Redis实例（即我们在SET命令中使用的TTL），那么客户端将认为锁无效并释放所有已经获取的锁，因此我们只需要考虑客户端在小于锁有效期的时间内获取了大多数Redis实例的锁。在本例中提到的参数中，没有客户端可以在MIN_VALIDITY时间内重新获取锁。因此多个客户端同时（即在步骤2中提到的“时间”内）可以锁定N/2+1个Redis实例，除非锁定大多数Redis实例的时间都超过TTL（锁的过期时间），锁才无效。 你能提供一个和当前算法类似的算法的证明或者找到其中一个bug吗？如果有的话我们将非常感激。 活跃性参数系统的活跃性基于三个主要特征： 锁的自动释放（当键过期时）：最终键可以再次被锁定。 事实上，当未获取到锁时，或者在获取到锁且工作结束时，客户端通常会合作删除锁，使我们不必等到键过期以后才能再次获取到锁。 事实上，当一个客户端需要重新获取锁时，它会等待一段比获取大多数锁的时间大的时间，以便使造成脑裂条件的概率不可能存在。 然而，我们在网络分区上需要承受TTL的时间消耗，因此如果出现连续的网络分区，我们就要无限期地承受这个时间消耗。这在每次一个客户端获取到一个锁后且客户端释放锁之前出现网络分区时都会发生。 基本上如果有出现无限的连续网络分区，系统可能将永远不可用。 性能，崩溃恢复和fsync许多用户使用Redis作为一个锁服务器需要高性能，即获取锁和释放锁的低延迟，和尽量高的每秒可能执行的获取/释放锁的操作次数。为了满足这些需求，减少和N个Redis服务器对话延迟的的策略一定是复用（或乞丐版复用，即让套接字工作在非阻塞模式下，一次性发送所有命令，并在稍后接受所有命令回复，这里假设了客户端和各个Redis实例之间的RTT是相似的）。 然而，如果我们要建立一个崩溃-恢复的系统模型，这里还要考虑关于持久化的问题。 为了发现问题所在，让我们假设Redis没有配置持久化。一个客户端在5个Redis实例中获取了3个锁。此时在客户端已经获取到锁的实例中有一个实例重启了，此时，又有三个实例可以对这个资源进行锁定，另一个客户端就可以再次锁定它，这违反了锁的互斥性安全属性。 如果我们开启AOF持久化，会有所改善。例如，我们可以通过发送SHUTDOWN命令重启更新一台服务器。因为Redis的过期时间在语义上的实现实际上不受服务器关机的影响，这满足我们所有的要求。然而，只要是正常的关机，一切都很好。如果是停电呢？如果Redis被配置成默认的每秒执行一次fsync刷数据到磁盘上，有可能出现重启机器后部分键丢失。从理论上讲，如果我们想要保证锁在任何机器重启的情况下的安全性，我们需要设置持久化为fsync=always。反过来，这将完全破坏系统的性能，使系统的性能和传统的用于实现安全分布式锁的中央处理系统处于同一水平。. 然而，事情要比乍看起来要好。基本上，只要实例崩溃后重新启动，算法安全性就保持不变，它不再参与任何当前活跃的锁，因此当实例重启后，客户端只会从除了重新加入系统以外的Redis实例中获取锁。 为了保证这一点，我们需要来看一个例子，在一个实例崩溃后，实例不可用的时间至少比我们使用的TTL长一点，也就是说，当实例崩溃时，当前存在的锁将在这个实例重启之后变得无效且被自动释放。 使用延迟重启基本上可以实现安全性甚至不需要任何形式的Redis持久化，然而请注意，这可能导致可用性问题。例如，如果大部分实例崩溃，系统将在TTL时间内变成全局不可用（这里的全局意味着没有任何资源能够这段时间内被锁定）。 使算法更可靠：扩展锁如果客户端要执行的操作是由一些小步骤组成的，默认可以使用较小的锁有效时间，并扩展了实现锁扩展机制的算法。基本上对于客户端来说，如果在计算过程中锁的有效期快到了，此时可以在键存在的情况下通过向所有Redis实例发送一个Lua脚本来延长键的TTL，而键的值还是保持当时客户端获取锁时赋予的那个值。 客户端只应该在锁的有效期内（基本上使用的算法与获取锁时使用的算法非常类似），且可以对大多数Redis实例延长锁有效期的前提下考虑重新获取锁。 然而，这并没有在技术上改变算法，因此应该限制重新获取锁的尝试次数，否则会违反其中一条活跃性属性。 想帮忙吗？如果你在研究分布式系统，我们将非常乐意听取你的观点和分析。如果能有其他语言的参考实现就更好了。 先说声谢谢了！ 分析RedlockMartin Kleppmann analyzed Redlock here. I disagree with the analysis and posted my reply to his analysis here. Martin Kleppmann在这里分析了Redlock。我并不同意这个分析，我把我对他的分析的回复放在这里。","categories":[{"name":"文档翻译","slug":"文档翻译","permalink":"https://nullcc.github.io/categories/文档翻译/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://nullcc.github.io/tags/Redis/"}]},{"title":"(译)将Redis作为LRU缓存使用","slug":"(译)将Redis作为LRU缓存使用","date":"2018-02-23T16:00:00.000Z","updated":"2022-04-15T03:41:13.012Z","comments":true,"path":"2018/02/24/(译)将Redis作为LRU缓存使用/","link":"","permalink":"https://nullcc.github.io/2018/02/24/(译)将Redis作为LRU缓存使用/","excerpt":"本文翻译自Using Redis as an LRU cache。","text":"本文翻译自Using Redis as an LRU cache。 当把Redis当做一个缓存使用时，常见的操作是当你向缓存增加新数据时缓存会自动淘汰旧数据。这种缓存行为对于开发者来可以说是非常熟悉了，因为这是一个常见缓存系统的基本行为。 LRU实际上只支持一种淘汰数据的模式。本文涵盖了Redis中maxmemory指令的基本话题，maxmemory指令用来限制Redis的最大内存使用量，并深度涵盖了Redis中使用的近似LRU的算法。 从Redis 4.0开始，引入了一个一个新的LFU（最不经常使用）淘汰策略。这在本文档的一个独立小节中被涵盖。 maxmemory配置指令maxmemory配置指令用来配置Redis的最大内存使用量。可以在redis.conf文件中配置该指令，也可以在运行时使用CONFIG SET命令进行配置。 例如为了将最大内存使用量限制在100mb，可以在redis.conf文件中使用如下指令进行配置。 maxmemory 100mb 将maxmemory设置为0将禁用内存限制。在64位系统上默认禁用内存限制，但在32位系统上内存限制被隐形地定为3GB。 当Redis的内存使用量达到指定的限制值时，有多种可选的策略。Redis可以直接对客户端发送的命令返回错误，这会导致更多的内存使用，或者Redis可以在加入新数据时淘汰一些旧数据，以便内存使用量降低到限制值以内。 淘汰策略当Redis的内存使用量达到限制值时，将遵循由maxmemory-policy配置指令指定的几种策略。 有如下几种策略可用： noeviction：当Redis达到最大内存使用量限制时返回错误，此时客户端继续尝试执行命令会导致更多的内存使用（大多数写命令会，除了DEL和一些异常）。 allkeys-lru：使用最近最少使用算法(LRU)淘汰键来为新增的数据腾出空间。 volatile-lru：仅对那些设置了过期时间的键使用最近最少使用算法(LRU)进行淘汰。 allkeys-random：对所有键进行随机淘汰来为新增的数据腾出空间。 volatile-random：仅对那些设置了过期时间的键进行随机淘汰。 volatile-ttl：对那些设置了过期时间的键进行淘汰，尝试优先淘汰那些生存时间(TTL)较短的键来为新增的数据腾出空间。 当键的条件不匹配时，和noeviction类似，volatile-lru、volatile-random和volatile-ttl这几个策略都不会淘汰键。 根据你应用的访问模式选择正确的键淘汰策略非常重要，然而你可以在运行时重新配置键淘汰策略，并使用Redis的INFO命令监控的缓存未命中和命中数量以此来调整配置。 一般来说作为一个经验法则： 当预计你的请求时一个幂律分布时，也就是说，当你预计其中所有元素的一个子集比其他元素更经常被访问时，可以使用allkeys-lru策略。如果你不确定的话，这是一个不错的选择。 如果你在循环中对所有键进行连续扫描，或者你预计你的访问在键空间分布均匀时（所有键被访问的可能性相同），请使用allkeys-random策略。 如果你想使用设置了过期时间的对象的TTL来提示Redis如何淘汰键，可以使用volatile-ttl策略。 当你想使用一个单一Redis实例来做缓存和持久地保存键值时，volatile-lru和volatile-random策略都很有用。然而，一般来说比较好的做法是运行两个Redis实例来解决这个问题。 还需要注意的是，对一个键设置过期时间需要消耗内存，所以使用像allkeys-lru这样的策略会有比较高的内存效率，因为在内存压力下，不需要为了淘汰键而对键设置过期时间。 键淘汰过程是如何运作的？了解键淘汰的运作过程非常重要，如下： 一个客户端执行一个新命令，导致更多的数据被添加进Redis。 Redis检查内存使用量，如果大于最大内存使用量限制，遵循配置的策略对键进行淘汰。 接着继续执行新命令，以此类推。 所以我们不断地在跨域内存限制的边界，有时超过这个限制，然后又通过键淘汰使内存使用量回到限制值以内。 如果一个命令导致内存被大量占用（比如计算一个大集合的交集并将其存储到一个新键中）一段时间，那么实际的内存使用量会显著地超过内存限制值。 近似LRU算法Redis的LRU算法并不是一个准确的LRU实现。这意味着Redis无法选出最合适被淘汰的键。相反，Redis会运行一个近似的LRU算法，通过采样少量的键，并淘汰这些采样键中最符合（即距离上一次被访问的时间最长的）淘汰条件的键。 然而从Redis 3.0开始，该算法会选择一批符合被淘汰条件地键进行淘汰。这提升了算法性能，也使其更加接近一个真正的LRU算法的行为。 关于Redis的LRU算法的一个很重要的事情是你可以通过改变每次键淘汰时采样键的数量来调整算法的精度。这个参数由下面的配置指令控制： maxmemory-samples 5 Redis不使用一个真正的LRU算法实现的原因是它需要消耗更多内存。然而，对于使用Redis的应用来说近似LRU算法和真正的LRU算法几乎是等价的。下面是Redis使用的近似LRU算法和真正的LRU算法的对比图。 使用一个有给定数量的键填充的Redis服务器来进行测试并生成上面的图。键被从头到尾地访问，因此使用LRU算法将淘汰那些首先被访问的键。之后又有50%的键被加入其中，迫使一半的旧键被淘汰。 你在图中可以看到三种类型的点，形成了三个明显的带。 浅灰色带是被淘汰的对象。 灰色带是未被淘汰的对象。 绿色带是新加入的对象。 理论上我们对LRU实现的期望是，在旧键中，前一半将被淘汰。取而代之的是Redis的LRU算法将只能概率性地淘汰旧键。 如你所见，对于采样键数为5的情况Redis 3.0的表现要好于Redis 2.8，然而Redis 2.8还是会保留大部分最近访问时间较新的键。在Redis 3.0中将采样数设为10时，近似LRU的表现已经非常接近于理论LRU的表现。 注意，LRU算法只是一个预测给定键在将来被访问的可能性的模型。此外，如果你的数据访问模式接近于幂律分布，即大部分的访问都发生在键空间的某个子集内，则近似LRU算法将很好地处理这种情况。 在模拟中我们发现使用幂律访问模式时，真正的LRU算法和Redis的近似LRU算法区别极小或者根本不存在。 然而，你可以将采样键数量提高到10以及一些额外的CPU使用为代价来让近似LRU算法的行为更接近真正LRU算法，并检查这么做是否会影响你的缓存未命中率。 为了在生产环境中对不同的采样键数进行实验，可以使用CONFIG SET maxmemory-samples &lt;count&gt;命令，非常简单。 新的LFU（最不经常使用）模式从Redis 4.0开始，提供了一个全新的最不经常使用的淘汰策略。在某些情况下这种模式的表现更佳（提供了一个更好的命中/未命中率），因为Redis使用LFU将尝试追踪各个键的访问频率，因此比较少被访问的键将被淘汰，而经常被访问的键将被保留在内存中。 考虑在使用LRU算法的情况下，一个最近被访问过的键有可能实际上几乎从未被访问过，这个键不会被淘汰，所以我们有淘汰掉一个在未来有较高几率被访问到的键的风险。LFU不存在这个问题，且一般来说它对不同访问模式的适应性更好。 为了配置使用LFU模式，可以使用下列策略： volatile-lfu：使用近似LFU算法来淘汰那些设置了过期时间的键。 allkeys-lfu：使用近似LFU算法来淘汰任何键。 LFU是LRU的近似：它使用了一个概率计数器，叫做Morris计数器，以便对每个对象仅使用几个比特位来估计对象的访问频率，使用衰减的方式使计数器的值随着时间的推移而减少：在某些时候，我们不再想考虑那些频繁被访问的键，即使距离上一次被访问时间较长，从而使算法能够适应访问模式的变化。 通过采样键来选择需要淘汰的键的方式和LRU的情况类似（在本文上小节中有描述）。 然而，和LRU不同的是，LFU具有一定的可调参数：比如，如果一个被频繁访问的对象不再被访问了，它的排名下降有多快？还可以调整Morris计数器的范围，以便更好地适应特定用例的算法。 默认情况下Redis 4.0的配置为： 使计数器饱和大约需要1百万个请求。 计数器每分钟衰减一次。 这些都是经过实验测试后得出的合理值，但用户可能想要改变这些配置项以便得到最优配置。 可以在redis.conf文件中找到并调整这些参数的配置指令，简单说就是： lfu-log-factor 10 lfu-decay-time 1 衰减时间表示当被采样的对象的上次衰减时间和当前时间的差比该值大时，就对其计数器进行衰减。一个特殊值0表示：总是在扫描时对计数器进行衰减，这个值很少使用。 计数器对数因子改变了能使频率计数器饱和的命中次数，它的范围是0-255。该因子的值越大，就需要越多访问量来使计数器的值达到最高。该因子的值越小，计数器的分辨率越低，如下表所示： factor 100 hits 1000 hits 100K hits 1M hits 10M hits 0 104 255 255 255 255 1 18 49 255 255 255 10 10 18 142 255 255 100 8 11 49 143 255 所以基本上，这个因子是一个对低访问频率和高访问频率对象之间的较好折中。更多的信息可以查看redis.conf自身的文档注释。 由于LFU是一个新特性，我们将非常感谢你对在你的使用场景中将它和LRU进行对比的任何反馈。","categories":[{"name":"文档翻译","slug":"文档翻译","permalink":"https://nullcc.github.io/categories/文档翻译/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://nullcc.github.io/tags/Redis/"}]},{"title":"(译)Redis的发布和订阅","slug":"(译)Redis的发布和订阅","date":"2018-02-21T16:00:00.000Z","updated":"2022-04-15T03:41:13.011Z","comments":true,"path":"2018/02/22/(译)Redis的发布和订阅/","link":"","permalink":"https://nullcc.github.io/2018/02/22/(译)Redis的发布和订阅/","excerpt":"本文翻译自Pub/Sub。","text":"本文翻译自Pub/Sub。 SUBSCRIBE、UNSUBSCRIBE和PUBLISH实现了发布/订阅消息传递模式（这里引用维基百科），发送者（发布者）不需要编程就可以将消息发送给指定的接收者（订阅者）。相反，已发布的消息具有信道特性，发布者不需要知道有哪些订阅者（如果有的话）。订阅者关注一个或多个它感兴趣的信道，只接收它感兴趣的信道的信息，而不需要知道有哪些发布者（如果有的话）。这种对发布者和订阅者的解耦允许更大和更具动态性地网络拓扑结构， 例如为了订阅foo和bar两个信道，客户端发送一个SUBSCRIBE命令并提供信道的名称： SUBSCRIBE foo bar 其他客户端向这些信道发送的消息将会被Redis推送给所有订阅了这些信道的客户端。 一个订阅了一个或多个信道的客户端不应该发布命令，尽管它可以对其他信道执行订阅或取消订阅。订阅和取消订阅操作的回复信息都以消息的形式发送，以便客户端可以读取连贯的消息流，其中第一个元素指明了消息的类型。 在一个订阅了信道的客户端允许执行的命令有SUBSCRIBE、PSUBSCRIBE、UNSUBSCRIBE、PUNSUBSCRIBE、PING和QUIT。 请注意redis-cli在订阅模式下将不接受任何命令，此时只能用Ctrl-C退出订阅模式。 推送消息的格式一个应答消息是一个具有三个元素的数组。 第一个元素是消息的类型： subscribe：表示我们成功订阅了应答中第二个元素中指明的信道。第三个参数表示我们当前订阅的信道个数。 unsubscribe：表示我们成功退订了应答中第二个元素指明的信道。第三个参数表示我们当前订阅的信道个数。当最后一个参数是0时，我们不再订阅任何信道，此时客户端可以发布任何Redis命令了，因为我们已经退出了发布/订阅状态。 message：表示接收到一个从其他客户端使用PUBLISH命令发布的消息。第二个参数表示信道名称，第三个参数为实际的消息载荷。 数据库 &amp; 作用域发布/订阅和键空间无关。它不受任何东西的干扰，包括数据库编号。 在db 10上发布消息，可以被在db 1上的订阅者监听到。 如果你需要某种形式的作用域，可以使用环境名称（test、staging、production等等）作为信道的前缀。 协议示例SUBSCRIBE first second *3 $9 subscribe $5 first :1 *3 $9 subscribe $6 second :2 此时，另一个客户端向信道second执行一个发布操作： &gt; PUBLISH second Hello 下面是第一个客户端收到的： *3 $7 message $6 second $5 Hello 现在客户端使用不带额外参数的UNSUBSCRIBE命令退订所有已订阅的信道： UNSUBSCRIBE *3 $11 unsubscribe $6 second :1 *3 $11 unsubscribe $5 first :0 模式匹配的订阅Redis的发布/订阅实现支持模式匹配。客户端可以使用glob风格的模式匹配来订阅所有发送给名字匹配指定模式的信道。 比如： PSUBSCRIBE news.* 此时客户端将接收到所有发送到news.art.figurative、news.music.jazz等信道的的消息。所有glob风格的模式都是合法的，支持多个通配符。 PUNSUBSCRIBE news.* 这个命令将是的客户端退订匹配news.*模式的所有信道。此调用不会影响其他信道的订阅。 因模式匹配而受到的消息以不同的格式发送： 消息的类型是pmessage：表示这是由其他客户端使用PUBLISH命令发布的消息，通过模式匹配了订阅。第二个元素是被匹配模式的原始形式，第三个参数是信道名称，最后一个参数是实际的消息载荷。 与SUBSCRIBE和UNSUBSCRIBE命令一样，PSUBSCRIBE和PUNSUBSCRIBE命令使用psubscribe和punsubscribe来表示命令类型，并使用与subscribe和unsubscribe信息相同的格式。 同时匹配模式和信道名的消息订阅如果一条发布的消息匹配了多个已订阅的模式，同时匹配模式和信道名的消息订阅，客户端将多次收到同一条消息。如下面的例子所示： SUBSCRIBE foo PSUBSCRIBE f* 上例中，如果一条消息被发送到信道foo，客户端将收到两条消息：一条类型为message的消息和一条类型为pmessage的消息。 模式匹配订阅计数的含义在subscribe、unsubscribe、psubscribe和punsubscribe消息类型中，最后一个参数表示当前仍在订阅的信道数量。这个数字实际上是客户端当前仍然订阅的信道和模式的总数。所以只有当退订所有信道和模式时，这个计数值变为0，客户端才会退出发布/订阅模式。 编程示例Pieter Noordhuis提供了一个很好的例子，使用EventMachine和Redis构建一个多用户高性能的网络聊天项目。 客户端库实现的提示由于所有收到的消息都包含原始的订阅信息，所以客户端可以使用一个哈希表保存已注册的回调函数，然后将原始的订阅信息（消息类型为message时为信道名，消息类型为pmessage时为匹配的模式）传递给回调函数（可以是匿名函数、块、函数指针）。 当接收到消息时，可以进行复杂度为O(1)的查找，以便将消息传递给已注册的回调函数。","categories":[{"name":"文档翻译","slug":"文档翻译","permalink":"https://nullcc.github.io/categories/文档翻译/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://nullcc.github.io/tags/Redis/"}]},{"title":"Python性能测试基础","slug":"Python性能测试基础","date":"2018-02-21T16:00:00.000Z","updated":"2022-04-15T03:41:13.017Z","comments":true,"path":"2018/02/22/Python性能测试基础/","link":"","permalink":"https://nullcc.github.io/2018/02/22/Python性能测试基础/","excerpt":"本文中我们来看看在Python中如何做基本的性能测试。","text":"本文中我们来看看在Python中如何做基本的性能测试。 在编程领域中对于性能这个词，有很多评估的角度，比如CPU时间、内存消耗、磁盘I/O、网络带宽等，本文将从CPU时间和内存消耗两个方面来介绍如何对Python程序进行性能分析。 首先我们需要一段测试代码，我们使用判断一个给定的数字是否是素数的函数isPrime来做测试，这是一段CPU密集型的代码： 123456789import math def isPrime(n): if n &lt;= 1: return False for i in range(2, int(math.sqrt(n)) + 1): if n % i == 0: return False return True 下面的一部分测试将使用isPrime函数。 使用装饰器测量函数运行时间代码如下： 123456789101112131415161718192021from functools import wrapsimport timedef timeit(fn): @wraps(fn) def timeit_(*args, **kwargs): t1 = time.time() res = fn(*args, **kwargs) t2 = time.time() print(fn.__name__ + \" took \" + str(t2-t1) + \" seconds\") return res return timeit_@timeitdef test(): l1 = [0] * 1000000 l2 = [0] * 10000000 l3 = [0] * 100000000if __name__ == \"__main__\": res = test() 执行后输出： 1test took 0.7034409046173096 seconds 使用装饰器是比较方便的做法，不过装饰器也只能打印整个函数的执行时间，测试粒度较粗。 使用timeit模块测试一段代码的CPU时间直接看代码： 123456789101112131415161718192021222324252627#!/usr/bin/python#coding=utf-8import math import timeit def isPrime(n): if n &lt;= 1: return False for i in range(2, int(math.sqrt(n)) + 1): if n % i == 0: return False return Truedef fn(): start = 1 end = 2000000 cnt = 0 for i in range(start, end + 1, 1): res = isPrime(i) if res: cnt += 1 print(\"There are \" + str(cnt) + \" primes in the \" + str(start) + \"-\" + str(end) + \" range.\")if __name__ == \"__main__\": t = timeit.timeit(stmt=fn, number=1) print(t) 我们计算2000000以内的素数个数，在我的Mac笔记本上耗时如下： 12.233505398966372 执行期间如果你打开活动监视器会发现该Python进程占用的CPU内核使用率达到99%以上，因为isPrime函数是CPU密集型的。 timeit模块还有很多用法，具体信息可以查阅相关文档。 使用timeit模块来测量一段代码能直观地获得执行这段代码的总耗时，这是比较粗粒度的测量，我们无法得知其中每行代码的耗时占比。因此这种方式只能在前期粗略地帮我们搜集一些代码整体执行时间的数据，如果要深入分析还是要想其他办法。 使用UNIX的time命令进行简单的计时类UNIX操作系统提供了time命令来对一个程序的执行进行计时，我们把代码修改成下面这样： 1234567891011121314151617181920212223242526#!/usr/bin/python#coding=utf-8import math import timeit def isPrime(n): if n &lt;= 1: return False for i in range(2, int(math.sqrt(n)) + 1): if n % i == 0: return False return Truedef fn(): start = 1 end = 2000000 cnt = 0 for i in range(start, end + 1, 1): res = isPrime(i) if res: cnt += 1 print(\"There are \" + str(cnt) + \" primes in the \" + str(start) + \"-\" + str(end) + \" range.\")if __name__ == \"__main__\": fn() 我们在shell中执行（注意要使用/usr/bin/time来引用time命令，否则会引用到shell内建的time命令，后者对我们的性能测试意义不大）： 1/usr/bin/time -p python prime_numbers.py 获得如下输出： real 13.96 user 13.75 sys 0.09 time命令输出了三行，其中real表示总耗时，user表示CPU花费在实际任务上的时间，这其中不包括陷入内核中执行的时间，sys表示陷入内核执行的时间。 需要注意的是，上例中/usr/bin/time记录的时间包括了启动Python解释器的时间。 使用cProfile模块python -m cProfile profile.stats prime_numbers.py 下面的命令使用cProfile模块对prime_numbers.py进行分析： 1python -m cProfile -s cumulative prime_numbers.py 输出如下： 12345678910114000509 function calls (4000503 primitive calls) in 14.028 secondsOrdered by: cumulative timencalls tottime percall cumtime percall filename:lineno(function) 2/1 0.000 0.000 14.028 14.028 &#123;built-in method builtins.exec&#125; 1 0.000 0.000 14.028 14.028 test2.py:1(&lt;module&gt;) 1 0.405 0.405 14.027 14.027 test2.py:12(fn)2000000 13.398 0.000 13.622 0.000 test2.py:4(isPrime)1999999 0.224 0.000 0.224 0.000 &#123;built-in method math.sqrt&#125;... 这里只截取前面几行最重要的输出信息，输出结果中每列的含义： ncalls：函数被调用了总次数 tottime：函数执行的总时间（不包括其下的子函数） percall：函数单次执行的时间（不包括其下的子函数），即tottime/ncalls cumtime：函数执行的总时间（包括其下的子函数） percall：函数单次执行的时间（包括其下的子函数），即cumtime/ncalls filename:lineno(function)：函数的基本信息 使用line_profile对代码进行逐行分析先安装line_profile： 1pip install line_profiler 这次我们使用另一端代码，这段代码主要是创建了3个列表，列表长度分别是一百万、一千万和一亿，并且对要测试的函数line_test加上@profile装饰器： 1234567891011#!/usr/bin/python#coding=utf-8@profiledef line_test(): l1 = [0] * 1000000 l2 = [0] * 10000000 l3 = [0] * 100000000if __name__ == \"__main__\": line_test() 下面的命令使用line_profile模块对line_test.py进行分析： 1kernprof -l -v line_test.py 你可能会奇怪我们并没有手动引入profile怎么能运行，其实当执行kernprof时，kernprof会自动将profile注入到__builtins__命名空间中。 输出结果如下： 1234567891011121314Wrote profile results to line_test.py.lprofTimer unit: 1e-06 sTotal time: 0.582631 sFile: line_test.pyFunction: line_test at line 6Line # Hits Time Per Hit % Time Line Contents============================================================== 6 @profile 7 def line_test(): 8 1 3440.0 3440.0 0.6 l1 = [0] * 1000000 9 1 54167.0 54167.0 9.3 l2 = [0] * 10000000 10 1 525024.0 525024.0 90.1 l3 = [0] * 100000000 来看看上面输出结果中每列的含义： Line #：代码行号，这里的代码行号和源代码文件中的行号是完全一致的 Hits：代码行的执行次数 Time：该代码行的总执行时间 Per Hit：该代码行每次执行的时间 % Time：代码行执行时间占整个程序执行时间的比率 Line Contents：具体代码行 这些输出信息非常直观，比如你会发现第10行创建长度为一亿的列表占用了90%的时间，当你对你自己的代码做line_profile时，根据这些信息很容易发现哪部分代码执行时间比较长，可以着重优化。 使用memory_profile诊断内存使用量先安装memory_profile： 1pip install memory_profiler 这次我们使用另一端代码，这段代码主要是创建了3个列表，列表长度分别是一百万、一千万和一亿，并且对要测试的函数memory_test加上@profile装饰器： 12345678910111213#!/usr/bin/python#coding=utf-8from memory_profiler import profile@profiledef memory_test(): l1 = [0] * 1000000 l2 = [0] * 10000000 l3 = [0] * 100000000if __name__ == \"__main__\": memory_test() 下面的命令使用memory_profile模块对memory_test.py进行分析： 1python -m memory_profiler memory_test.py 输出如下： 123456789Filename: memory_test.pyLine # Mem usage Increment Line Contents================================================ 6 34.2 MiB 34.2 MiB @profile 7 def memory_test(): 8 41.8 MiB 7.6 MiB l1 = [0] * 1000000 9 118.1 MiB 76.3 MiB l2 = [0] * 10000000 10 881.0 MiB 762.9 MiB l3 = [0] * 100000000 输出结果中每列的含义： Line #：代码行号，这里的代码行号和源代码文件中的行号是完全一致的 Mem usage：当前程序使用的总内存大小 Increment：内存增长的大小，即和上一行代码相比，执行本行代码导致的内存增量 Line Contents：具体代码行 memory_profile的输出结果也很好理解，可以依据代码行来观察具体的内存增量，比如上例中第10行代码创建了长度为一亿的列表，占用了比较多的内存，其Increment是762.9 MiB。需要注意的是，MiB表示的是2^20字节，这和MB有所区别。 用heapy调查堆上的对象先安装依赖: 1pip install guppy 需要注意的是heapy不支持Python 3.x，所以我们在Python 2.7上测试。 测试代码如下： 1234567891011121314151617181920212223#!/usr/bin/python#coding=utf-8def heapy_test(): l1 = [0] * 1000000 from guppy import hpy hp = hpy() h = hp.heap() print h print l2 = [0] * 10000000 h = hp.heap() print h print l3 = [0] * 100000000 h = hp.heap() print h printif __name__ == \"__main__\": heapy_test() 输出结果如下： 1234567891011121314151617181920212223242526272829303132333435363738394041Partition of a set of 26265 objects. Total size = 11526656 bytes. Index Count % Size % Cumulative % Kind (class / dict of class) 0 178 1 8152080 71 8152080 71 list 1 11976 46 985344 9 9137424 79 str 2 5969 23 481488 4 9618912 83 tuple 3 323 1 277448 2 9896360 86 dict (no owner) 4 69 0 219768 2 10116128 88 dict of module 5 1653 6 211584 2 10327712 90 types.CodeType 6 200 1 211136 2 10538848 91 dict of type 7 1615 6 193800 2 10732648 93 function 8 200 1 177912 2 10910560 95 type 9 124 0 135328 1 11045888 96 dict of class&lt;90 more rows. Type e.g. &apos;_.more&apos; to view.&gt;Partition of a set of 26274 objects. Total size = 95415008 bytes. Index Count % Size % Cumulative % Kind (class / dict of class) 0 179 1 92038232 96 92038232 96 list 1 11978 46 985472 1 93023704 97 str 2 5968 23 481424 1 93505128 98 tuple 3 329 1 279128 0 93784256 98 dict (no owner) 4 69 0 219768 0 94004024 99 dict of module 5 1653 6 211584 0 94215608 99 types.CodeType 6 200 1 211136 0 94426744 99 dict of type 7 1614 6 193680 0 94620424 99 function 8 200 1 177912 0 94798336 99 type 9 124 0 135328 0 94933664 99 dict of class&lt;90 more rows. Type e.g. &apos;_.more&apos; to view.&gt;Partition of a set of 26275 objects. Total size = 900721480 bytes. Index Count % Size % Cumulative % Kind (class / dict of class) 0 180 1 897344672 100 897344672 100 list 1 11978 46 985472 0 898330144 100 str 2 5968 23 481424 0 898811568 100 tuple 3 329 1 279128 0 899090696 100 dict (no owner) 4 69 0 219768 0 899310464 100 dict of module 5 1653 6 211584 0 899522048 100 types.CodeType 6 200 1 211136 0 899733184 100 dict of type 7 1614 6 193680 0 899926864 100 function 8 200 1 177912 0 900104776 100 type 9 124 0 135328 0 900240104 100 dict of class&lt;90 more rows. Type e.g. &apos;_.more&apos; to view.&gt; 上面的结果是按照在堆上分配内存的大小来排序的。","categories":[{"name":"测试","slug":"测试","permalink":"https://nullcc.github.io/categories/测试/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://nullcc.github.io/tags/Python/"},{"name":"测试","slug":"测试","permalink":"https://nullcc.github.io/tags/测试/"}]},{"title":"(译)使用流水线加速Redis查询","slug":"(译)使用流水线加速Redis查询","date":"2018-02-20T16:00:00.000Z","updated":"2022-04-15T03:41:13.012Z","comments":true,"path":"2018/02/21/(译)使用流水线加速Redis查询/","link":"","permalink":"https://nullcc.github.io/2018/02/21/(译)使用流水线加速Redis查询/","excerpt":"本文翻译自Using pipelining to speedup Redis queries。","text":"本文翻译自Using pipelining to speedup Redis queries。 请求/响应协议和RTTRedis是一个使用客户端-服务器模型和所谓的请求/响应协议的TCP服务器。 这意味着一个处理一个请求通常要经过以下几个步骤： 客户端向服务器发送一个查询，并从套接字上读取从服务器返回的响应，这一般是阻塞的。 服务器处理命令并返回响应给客户端。 例如，一个四个命令的序列是这样子的： Client: INCR X Server: 1 Client: INCR X Server: 2 Client: INCR X Server: 3 Client: INCR X Server: 4 客户端和服务器使用网络连接在一起。连接可以是非常快速的（比如一个回环接口），也可以是非常慢的（Internet中中间需要很多跳的两台主机之间）。无论网络的延迟是多少，请求数据包从客户端到达服务器，和应答数据包从服务器返回客户端都需要时间。 这个时间叫做RTT（往返时间）。很容易发现当客户端需要一次执行多个请求时（比如向同一个列表中添加多个元素，或者向数据库中填充多个键），RTT对性能的影响。比如假设RTT是250ms（Internet中的非常慢速的连接），即使服务器每秒能执行100k个请求，我们一秒中最多只能处理四个请求。 如果使用回环接口，RTT会小很多（比如我的主机上ping 127.0.0.1的时间是0.044ms），但如果你要一次执行多个写入请求，延时还是太高。 幸运的是有一种方式可以改进这个问题。 Redis流水线一个请求/响应服务器可以被实现成尽管客户端还没从上一个请求中读取到响应时也可以处理新的请求。使用这种方式就可以在不等待响应的情况下x向服务器发送多个命令，最后一次性读取多有响应。 这种方式叫做流水线，是十几年来广泛使用的技术。比如很多POP3协议的实现已经支持这种特性，大大加快了从服务器上下载新邮件的速度。 Redis支从很早就开始支持流水线，所以不论你使用哪个版本，你都可以使用Redis的流水线功能。下面是一个使用netcat工具的例子： $ (printf &quot;PING\\r\\nPING\\r\\nPING\\r\\n&quot;; sleep 1) | nc localhost 6379 +PONG +PONG +PONG 这时我们不需要在每个命令上都等待RTT，取而代之是三个命令只需要一次RTT。 为了更清楚地说明，我们最初那个例子通过使用流水线会变成下面这样： Client: INCR X Client: INCR X Client: INCR X Client: INCR X Server: 1 Server: 2 Server: 3 Server: 4 特别注意：当客户端使用流水线发送命令，将迫使服务器使用内存在将响应信息排队。所以如果你需要使用流水线发从大量命令，比较好的做法是将它们按照一个合理的大小分成多个批次，比如使用流水线一次性发送10k个命令，然后读取响应，再发送另外的10k个命令，以此类推。这样每个批次的处理速度几乎是相同的，但是为了排队这10k个命令的响应需要消耗大量内存。 不仅仅是减少了RTT流水线不仅仅是为了减少了由于往返时间造成的延迟成本，它实际上提高了一个给定的Redis服务器每秒能执行的操作数。事实是，不使用流水线的情况下，从访问数据结构和产生响应的角度看，处理每个命令的代价都非常低廉，但从套接字I/O的角度看，代价却非常高。因为这涉及到read()和write()系统调用，这需要从用户态陷入内核态。这种上下文切换的代价非常高。 当使用流水线时，使用一个read()系统调用即可读取多个命令，并且使用一个write()系统调用即可发送多个应答。因此，Redis每秒执行的查询总数随着流水线的长度增加几乎呈线性增长，相比不使用流水线，最终能达到10倍的增长，如下图： 一些现实的代码样例在下面的基准测试中我们将使用支持流水线的Redis的Ruby客户端，来测试使用流水线带来的速度提升： 12345678910111213141516171819202122232425262728293031require 'rubygems'require 'redis'def bench(descr) start = Time.now yield puts \"#&#123;descr&#125; #&#123;Time.now-start&#125; seconds\"enddef without_pipelining r = Redis.new 10000.times &#123; r.ping &#125;enddef with_pipelining r = Redis.new r.pipelined &#123; 10000.times &#123; r.ping &#125; &#125;endbench(\"without pipelining\") &#123; without_pipelining&#125;bench(\"with pipelining\") &#123; with_pipelining&#125; 在Mac OS X上运行上面的简单脚本将提供给我们下面的数据，运行在回环接口上对速度的提升最小，因为回环接口的RTT已经非常小了： without pipelining 1.185238 seconds with pipelining 0.250783 seconds 如你所见，使用流水线我们将传输效率提高了5倍。 流水线 VS 脚本使用Redis脚本（Redis 2.6或更高版本支持），一些原本使用流水线的情况如果使用服务端脚本来做效率会更高。脚本的一大优点是它能够以最小的延迟读取和写入数据，使像读、计算、写这类操作非常快（流水线在这种情况下帮助不大，因为客户端在执行写入之前需要先读取应答）。 有时应用可能需要在一个流水线中发送EVAL或EVALSHA命令。Redis明确地支持SCRIPT LOAD命令来应对这种情况（它保证EVALSHA可以在没有失败风险的情况下被调用）。 附录：为什么忙循环即使是在回环接口上运行还是很慢？尽管阅读了本文的所有信息，你可能还是奇怪为什么像下面这种基准测试（伪代码中），即使是在回环接口上运行（客户端和服务器运行在同一台物理机上）还是很慢： FOR-ONE-SECOND: Redis.SET(&quot;foo&quot;,&quot;bar&quot;) END 毕竟如果Redis进程和基准测试在同一个环境下运行，这种情况下消息在内存中从一处拷贝到另一处不是不会受到任何延迟和网络的影响吗？ 原因是系统中的进程并不总是在运行，实际上是内核调度器让进程运行，因此情况是，比如基准测试被允许运行，从Redis服务器中读取响应（与最后一个被执行的命令相关的），并且发送一个新命令给服务器。现在命令被存储在回环接口缓冲区中，但是为了让服务器读取命令，内核将调度Redis服务器进程（当前正在被一个系统调用阻塞）运行，以此类推。因此，实际上由于内核调度器的工作原理，回环接口还是会有类似网络延迟的延迟存在。 基本上使用回环接口对网络服务器的性能做基准测试是最愚蠢的事情了。明智的做法是避免使用这种方式做基准测试。","categories":[{"name":"文档翻译","slug":"文档翻译","permalink":"https://nullcc.github.io/categories/文档翻译/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://nullcc.github.io/tags/Redis/"}]},{"title":"(译)Redis响应延迟问题排查","slug":"(译)Redis响应延迟问题排查","date":"2018-02-14T16:00:00.000Z","updated":"2022-04-15T03:41:13.011Z","comments":true,"path":"2018/02/15/(译)Redis响应延迟问题排查/","link":"","permalink":"https://nullcc.github.io/2018/02/15/(译)Redis响应延迟问题排查/","excerpt":"本文翻译自Redis latency problems troubleshooting。","text":"本文翻译自Redis latency problems troubleshooting。 本文将帮助你了解当你遇到了Redis延迟问题时究竟发生了什么。 在这里延迟指的是客户端从发送命令到接收命令回复这段时间的最大值。一般情况下Redis处理命令的时间非常短，基本上在微秒级别，但是这里有几种情况会导致高延迟。 我很忙，把清单给我下面的文档对于想要以低延迟运行Redis来说非常重要。然而我知道大家都很忙，所以我们先来看一个快速清单。如果你没有遵守这些步骤，请回到这里阅读整个文档。 确保服务器没有被慢查询阻塞。使用Redis的慢日志功能来检查是否有慢查询。 对于EC2的用户，确保你基于现代的EC2实例使用HVM，比如m3.medium。否则fock()操作会很慢。 必须禁用内核的Transparent huge pages特性。使用echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled来禁用它，然后重启你的Redis进程。 如果你在使用虚拟机，很可能存在一种和Redis无关的内在延迟。检查机器的最小延迟，你可以在你的运行环境中使用./redis-cli --intrinsic-latency 100。注意：你需要在服务器上运行这个命令而不是在客户端上。 打开并使用Redis的延迟监控特性来获取你机器上的人类可读的延迟事件描述。 一般来说，使用下表进行持久化和延迟/性能的权衡，顺序从最高安全性/最高延迟到最低安全性/最低延迟。 AOF + fsync always: 非常慢，只有当你确实需要时才使用该配置。 AOF + fsync every second: 一个比较均衡的选择。 AOF + fsync every second + no-appendfsync-on-rewrite选项为yes: 也是一个比较均衡的选择，但是要避免重写期间执行fsync，这可以降低磁盘压力。 AOF + fsync never: 将fsync操作交给内核，减少了对磁盘的压力和延迟。 RDB: 这里你可以配置触发生成RDB文件的条件。 以下我们花费15分钟时间来看看细节。 测量延迟如果你对处理延迟问题很有经验，可能你知道在你的应用程序中如何测量延迟，也许你的延迟问题是非常明显的，甚至是肉眼可见的。然而redis-cli可以在毫秒级别测量一个Redis服务器的延迟，只需要运行： redis-cli --latency -h `host` -p `port` 使用Redis内置的延迟监控子系统从Redis 2.8.13开始，Redis提供了延迟监控功能，能够取样检测出是哪里导致了服务器阻塞。这使得本文档所列举的问题的调试更加简单，所以我们建议尽量开启延迟监控。有关这方面更详细的说明请查阅延迟监控的文档。 虽然延迟监控的采样和报告能力可以使我们更容易地了解造成Redis延迟的原因，但还是建议你阅读本文档更广泛地了解Redis的延迟尖峰。 延迟的基线在你运行Redis的环境中有一种固有的延迟，这种延迟来自操作系统内核，如果你正在使用虚拟化，这种延迟来自于你使用的虚拟机管理程序。 虽然这个延迟无法被抹去，但这是我们学习的重要对象，因为它是基线，或者换句话说，由于内核和虚拟机管理程序的存在，你无法将Redis的延迟优化得比你系统中正在运行的进程的延迟还要低。 我们称这种延迟为内在延迟，redis-cli从Redis 2.8.7版本之后就可以测量内在延迟了。下面是一个运行在Linux 3.11.0入门级服务器上的实例。 注意：参数100表示测试执行的时间的秒数。我们运行测试的时间越久，就越有可能发现延迟尖峰。100秒通常是合适的，不过你可能希望在测试过程中不同的时间执行一些其他的操作。请注意，测试是CPU密集型的，这可能会使系统中的单个内核跑满。 $ ./redis-cli --intrinsic-latency 100 Max latency so far: 1 microseconds. Max latency so far: 16 microseconds. Max latency so far: 50 microseconds. Max latency so far: 53 microseconds. Max latency so far: 83 microseconds. Max latency so far: 115 microseconds. 注意：在这个特殊情况下，redis-cli需要在服务器端运行，而不是在客户端。这种特殊模式下，redis-cli根本不需要连接到一台Redis服务器：它只是试图测量内核不提供CPU时间给redis-cli进程本身的最大时间。 上面的例子中，系统固有延迟只有0.115毫秒（或115微秒），这是个好消息，但是请记住，系统内在延迟可能随着系统负载而随时间变化。 虚拟化环境的情况会差一些，特别是在共享虚拟环境中有高负载的其他应用在运行时。下面是一个在Linode 4096实例上运行Redis和Apache的例子： $ ./redis-cli --intrinsic-latency 100 Max latency so far: 573 microseconds. Max latency so far: 695 microseconds. Max latency so far: 919 microseconds. Max latency so far: 1606 microseconds. Max latency so far: 3191 microseconds. Max latency so far: 9243 microseconds. Max latency so far: 9671 microseconds. 这里我们测量出有9.7毫秒的内在延迟：这意味着Redis的延迟不可能比这个数字更低了。然而，在不同的虚拟化环境中，如果有高负载的其他应用程序在运行时，很容易出现更高的内在延迟。除非我们能够在系统中测量出40毫秒的内在延迟，否则显然Redis运行正常。 网络通信引起的延迟客户端使用一条TCP/IP连接或一条UNIX域连接来连接Redis。一个带宽为1 Gbit/s的网络典型的延迟为200μs，然而一个UNIX域套接字的延迟可以低至30μs。这具体依赖你的网络和系统硬件情况。高层的通信增加了更多的延迟（由于线程调度、CPU缓存、NUMA配置等等）。系统内部引起的延迟在虚拟化环境中要比在物理机上高得多。 其结果是尽管Redis处理大部分命令的时间都在亚微秒级别，但一个客户端和服务器之间的多次往返会增加网络和系统的延迟。 一个高效的客户端将会通过使用流水线来限制执行多个命令时的通信往返次数。流水线特性被服务器和绝大多数客户端所支持。批量操作命令如MSET/MGET也是为了这个目的。从Redis 2.4起，一些命令还支持所有数据类型的可变参数。 下面是一些准则： 如果经济上允许，优先选择使用物理机而不是虚拟机来承载Redis服务端。 不要随意连接/断开到服务器（尤其是web应用程序）。尽量保持连接长时间可用。 如果Redis的服务端和客户端部署在同一台机器上，请使用UNIX域套接字。 相比起流水线，尽量使用批量操作命令（MSET/MGET），或可变参数命令（如果可能的话）。 相比起发送多个单独命令，尽量使用流水线（如果可能的话）。 在不适合使用原生流水线功能的场景，Redis支持服务端Lua脚本（针对一个命令的输出是另一个命令的输入的情况）。 在Linux中，你可以通过process placement (taskset)、 cgroups、 real-time priorities (chrt)、 NUMA configuration (numactl)或使用一个低延迟内核来获得更低的延迟。请注意Redis并不适合被绑定在一个CPU内核上运行。Redis会fork出一些后台任务比如bgsave或AOF重写这些非常消耗CPU的任务。这些任务禁止和Redis的主事件循环运行在同一个CPU上。 大部分情况下，我们不需要这种类型的系统级优化。只有当你确实需要或者对它们很熟悉的情况下再去使用它们。 Redis的单线程属性Redis被设计成大部分情况下是单线程的。这意味着使用一个线程处理所有的客户端请求，其中使用了多路复用技术。这意味着Redis在一个时刻只能处理一个命令，所以所有命令都是串行执行的。这和Node.js的工作机制很类似。然而，Redis和Node.js通常都被认为是非常高性能的。这有部分原因是因为它们处理每个请求的时间都很短，但是主要原因是因为它们都被设计成不会被系统调用锁阻塞，比如从套接字中读取或写入数据。 之所以说Redis大部分情况下是单线程的，是因为从Redis 2.4版本起，为了在后台执行一些慢速的I/O操作，一般是磁盘I/O，Redis使用了其他线程来执行。但这也不能改变Redis使用单线程处理所有请求这个事实。 慢查询命令引起的响应延迟使用单线程的一个结果是，当一个请求的处理很慢时，所有其他客户端将等待该请求被处理完毕。当执行普通命令时，比如GET或SET或LPUSH时这完全不是问题，因为这几个命令的执行时间是常数（非常短）。然而，有些命令会操作多个元素，比如SORT、LREM、SUNION等。例如，计算两个大集合的交集需要花费很长的时间。 所有命令的算法复杂度都有文档记录。一个好的实践是当你使用你不熟悉的命令之前先检查该命令的算法复杂度。 如果你关注Redis的响应延迟问题，你就不应该对有多个元素的值使用慢查询命令，你应该在Redis复制节点上运行你所有的慢查询。 可以使用Redis的Slow Log功能来监控慢查询命令。 而且，你可以使用你最喜欢的进程级监控程序（top, htop, prstat等）来快速检查Redis主进程的CPU消耗。如果并发量并不是很高，很可能是因为你使用了慢查询命令。 重要提示：一个非常常见的造成Redis响应延迟的情况是在生产环境中使用KEYS命令。Redis文档中指出KEYS命令只能用于调试目的。从Redis 2.8之后，为了在键空间或大集合中增量地迭代键而引入了一些命令，请查阅SCAN, SSCAN, HSCAN and ZSCAN的文档来获取更多信息。 fork引起的响应延迟为了在后台生成RDB文件，或者当AOF持久化开启时重写AOF文件，Redis需要执行fork。fork操作（在主线程中执行）会引发响应延迟。 在大多数类UNIX系统中fork是一个开销很昂贵的操作，因为它涉及复制与进程相关联的大量对象。对于和虚拟内存相关联的页表尤其如此。 例如在一个Linux/AMD64系统上，内存被划分为一个个个4KB大小的页面。为了将逻辑地址转换成物理地址，每个进程都维护一个页表（在内部用一棵树表示），每个页面包含进程地址空间中的至少一个指针。所以一个拥有24GB内存的Redis实例需要的页表大小为24 GB / 4 kB * 8 = 48 MB。 当执行一个后台持久化任务时，该Redis实例需要执行fork，这将涉及分配和复制48MB的内存。这需要消耗时间和CPU资源，特别是在虚拟机上执行分配和初始化大内存时开销尤其昂贵。 不同系统中fork操作的耗时现代硬件在复制页表这个操作上非常快，但Xen却不是这样。Xen的问题不在于虚拟化，而在于Xen本身。一个例子是使用VMware或Virtual Box不会导致fork变慢。下面比较了不同Redis实例执行fork操作的耗时。数据来自于执行BGSAVE，并观察INFO命令输出的latest_fork_usec信息。 然而，好消息是基于EC2 HVM的实例执行fork操作的表现很好，几乎和在物理机上执行差不多，所以使用m3.medium（或高性能）的实例将会得到更好的结果。 运行于VMware的Linux对一个6.0GB的Redis实例执行fork操作耗时77ms（12.8ms/GB）。 运行于物理机（硬件未知）上的Linux对一个6.1GB的Redis实例执行fork操作耗时80ms（13.1ms/GB）。 运行于物理机（Xeon @ 2.27Ghz）对一个6.9GB的Redis实例执行fork操作耗时62ms（9ms/GB）。 运行于6sync（KVM）虚拟机的Linux对360MB的Redis实例执行fork操作耗时8.2ms（23.3ms/GB）。 运行于EC2，旧实例类型（Xen）的Linux对6.1GB的Redis实例执行fork操作耗时1460ms（239.3ms/GB）。 运行于EC2，新实例类型（Xen）的Linux对1GB的Redis实例执行fork操作耗时10ms（10ms/GB）。 运行于Linode（Xen）虚拟机的Linux对0.9GB的Redis实例执行fork操作耗时382ms（424ms/GB）。 你可以看到运行在Xen上的虚拟机会有一到两个数量级的性能损失。对于EC2的用户有个很简单的建议：使用现代的基于HVM的实例。 transparent huge pages引起的响应延迟遗憾的是如果一个Linux内核启用了transparent huge pages，Redis为了将数据持久化到磁盘时调用fork将会引起很大的响应延迟。大内存页导致了以下问题： 当调用fork时，共享大内存页的两个进程将被创建。 在一个高负载的实例上，一些事件循环就将导致访问上千个内存页，导致几乎整个进程执行写时复制。 这将导致高响应延迟和大内存的使用。 请确保使用下面的命令关闭transparent huge pages： echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled 页交换引起的响应延迟（操作系统分页）为了更高效地利用系统内存，Linux（以及很多其他的现代操作系统）能够将内存页迁移到磁盘，反之亦然。 如果内核将一个Redis的内存页从内存交换到磁盘文件，当Redis要访问该内存页中的数据时（比如访问该内存页中的一个键），内核为了将内存页从磁盘文件迁移回内存将会暂停Redis进程。这是一个涉及随机I/O的慢速操作（和访问一个已经在内存中的页面相比是非常慢的），这将导致导致Redis客户端感觉到异常的响应延迟。 内核将Redis内存页从内存交换到磁盘主要有三个原因： 系统有内存压力，比如正在运行的进程需要比当前可用物理内存更多的内存。最简单的例子就是Redis使用了比可用内存更多的内存。 Redis实例中的数据集，或数据集中的一部分几乎是闲置状态（从未被客户端访问过），此时内核将把这部分内存页交换到磁盘上。这种情况非常罕见，因为即使是一个中等速度的Redis实例也经常会访问所有内存页，迫使内核将所有内存页保留在内存中。 系统中的一些进程引发了大量读或者写这种I/O操作。因为一般文件都会被缓存，这将导致内核需要增加文件系统缓存，这会导致内存页交换。请注意这包括生成Redis RDB和/或AOF这些会生成大文件的后台线程。 幸运的是Linux提供了很不错的工具来检查这些问题，所以当由于内存页交换导致的响应延迟发生时我们应该怀疑是否是上面三个原因导致的。 首先要做的是检查有多少Redis内存页被交换到了磁盘。为了达到这个目的我们需要获得Redis实例的pid： $ redis-cli info | grep process_id process_id:5454 现在进入这个进程的文件系统目录： $ cd /proc/5454 你可以在这里找到一个名为smaps的文件，这个文件描述了Redis进程的内存布局（假设你正在使用Linux 2.6.16或更高版本的内核）。这个文件包含了进程非常详细的内存布局信息，其中有一个名为Swap字段对我们很重要。然而，这里面不仅仅只有一个swap字段，因为smaps文件还包含了Redis进程的其他内存映射（进程的内存布局比一个内存页的线性数组要复杂得多）。 由于我们对进程的所有内存交换情况感兴趣，因此首先要做的就是找出该文件中的所有Swap字段： $ cat smaps | grep &apos;Swap:&apos; Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 12 kB Swap: 156 kB Swap: 8 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 4 kB Swap: 0 kB Swap: 0 kB Swap: 4 kB Swap: 0 kB Swap: 0 kB Swap: 4 kB Swap: 4 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB 如果所有Swap低端都是0 Kb，或者只有零星的字段是4k，那么一切正常。实际上在我们这个例子中（线上真实的每秒处理上千请求的Redis实例）有一些条目表示存在更多的内存页交换问题。为了调查这是否是一个严重的问题，我们使用其他命令以便打印出内存映射的大小： $ cat smaps | egrep &apos;^(Swap|Size)&apos; Size: 316 kB Swap: 0 kB Size: 4 kB Swap: 0 kB Size: 8 kB Swap: 0 kB Size: 40 kB Swap: 0 kB Size: 132 kB Swap: 0 kB Size: 720896 kB Swap: 12 kB Size: 4096 kB Swap: 156 kB Size: 4096 kB Swap: 8 kB Size: 4096 kB Swap: 0 kB Size: 4 kB Swap: 0 kB Size: 1272 kB Swap: 0 kB Size: 8 kB Swap: 0 kB Size: 4 kB Swap: 0 kB Size: 16 kB Swap: 0 kB Size: 84 kB Swap: 0 kB Size: 4 kB Swap: 0 kB Size: 4 kB Swap: 0 kB Size: 8 kB Swap: 4 kB Size: 8 kB Swap: 0 kB Size: 4 kB Swap: 0 kB Size: 4 kB Swap: 4 kB Size: 144 kB Swap: 0 kB Size: 4 kB Swap: 0 kB Size: 4 kB Swap: 4 kB Size: 12 kB Swap: 4 kB Size: 108 kB Swap: 0 kB Size: 4 kB Swap: 0 kB Size: 4 kB Swap: 0 kB Size: 272 kB Swap: 0 kB Size: 4 kB Swap: 0 kB 正如你在上面输出中所看到的，有一个720896 kB（其中只有12 kB的内存页交换）的内存映射，在另一个内存映射中交换了156 kB：只有很少一部分内存页被交换到磁盘，这没什么问题。 相反，如果有大量进程内存页被交换到磁盘，那么你的响应延迟问题可能和内存页交换有关。如果是这样的话，你可以使用vmstat命令来进一步检查你的Redis实例： $ vmstat 1 procs -----------memory---------- ---swap-- -----io---- -system-- ----cpu---- r b swpd free buff cache si so bi bo in cs us sy id wa 0 0 3980 697932 147180 1406456 0 0 2 2 2 0 4 4 91 0 0 0 3980 697428 147180 1406580 0 0 0 0 19088 16104 9 6 84 0 0 0 3980 697296 147180 1406616 0 0 0 28 18936 16193 7 6 87 0 0 0 3980 697048 147180 1406640 0 0 0 0 18613 15987 6 6 88 0 2 0 3980 696924 147180 1406656 0 0 0 0 18744 16299 6 5 88 0 0 0 3980 697048 147180 1406688 0 0 0 4 18520 15974 6 6 88 0 ^C 你需要注意查看si和so两列，这两列统计了内存页从内存交换到磁盘和从磁盘交换到内存的次数。如果在这两列中你看到非零值，就说明你的系统中存在内存页交换。 最后，可以使用iostat命令来检查系统的全局I/O活动。 $ iostat -xk 1 avg-cpu: %user %nice %system %iowait %steal %idle 13.55 0.04 2.92 0.53 0.00 82.95 Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await svctm %util sda 0.77 0.00 0.01 0.00 0.40 0.00 73.65 0.00 3.62 2.58 0.00 sdb 1.27 4.75 0.82 3.54 38.00 32.32 32.19 0.11 24.80 4.24 1.85 如果你的响应延迟问题是由Redis内存页交换导致的，你就需要降低系统中的内存压力，如果Redis使用了比可用内存更多内存的话你就增加更多内存，或者避免在同一个系统中运行其他需要大量内存的进程。 AOF和磁盘I/O引起的响应延迟另一个响应延迟的原因是Redis的AOF。Redis使用了两个系统调用来完成AOF功能。一个是使用write(2)来将数据写入到只追加的文件中，另一个是使用fdatasync(2)来刷新内核文件缓冲区到磁盘以满足用户指定的持久化级别。 write(2)和fdatasync(2)都会造成响应延迟。例如当系统进程同步时write(2)会造成阻塞，或者当输出缓冲区满时内核需要将数据刷到磁盘上以便能接受新的写入。 fdatasync(2)会导致更严重的响应延迟，许多内核和文件系统对它的结合使用会导致数毫秒到数秒的延迟。当特别是在有其他进程正在执行I/O时。出于这些原因，从Redis 2.4开始fdatasync(2)会在另一个线程中执行。 我们将看到在使用AOF功能时，不同配置如何影响Redis的响应延迟。 AOF的配置项appendfsync可以有三种不同的方式来执行磁盘的fsync（这些配置可以在运行时使用CONFIG SET命令动态修改）。 当appendfsync被设置为no时，Redis不执行fsync。这种配置下响应延迟的唯一原因就是write(2)了。这种情况下发生响应延迟一般没有解决方案，因为磁盘的处理速度跟不上Redis接收数据的速度，然而，如果当磁盘没有被其他进程的I/O拖慢时，这是很少见的。 当appendfsync被设置成everysec时，Redis每秒执行一次fsync。Redis使用另外的线程执行fsync，如果此时fsync正在执行，Redis使用缓冲区来延迟2秒执行write(2)（因为在Linux中对一个正在执行fsync的文件执行write将被阻塞）。然而，如果fsync执行时间太长，即使fsync正在执行，Redis也将执行write(2)，这会引起响应延迟。 当appendfsync被设置成always时，Redis将在每次写操作发生时，返回OK给客户端之前执行fsync（实际上Redis会尝试将同一时间的多个命令的执行使用fsync一次性进行写入）。这种模式下Redis性能很低，此时一般建议使用高速硬盘和文件系统的实现以便能更快地完成fsync。 大多数Redis用户将appendfsync配置项设置为no或everysec。将响应延迟降低到最小的建议是避免在同一个系统中有其他进程执行I/O。使用固态硬盘也能降低I/O造成的的响应延迟，但一般来说当Redis写AOF时，如果此时硬盘上没有其他查找操作，非固态硬盘的性能也还不错。 如果你想调查AOF引起的响应延迟问题，你可以使用strace命令： sudo strace -p $(pidof redis-server) -T -e trace=fdatasync 上面的命令将显示Redis在主线程中执行的所有fdatasync(2)系统调用情况。当appendfsync配置项被设置为everysec时，使用上面的命令无法查看到后台进程执行fdatasync系统调用情况，如果需要查看后台进程的情况，在strace命令上加上-f选项即可。 如果你同事想查看fdatasync和write两个系统调用的情况，使用下面的命令： sudo strace -p $(pidof redis-server) -T -e trace=fdatasync,write 然而因为write(2)也被用来向客户端套接字写入数据，所有可能会显示很多与磁盘I/O无关的信息。显然strace命令无法只显示慢系统调用的信息，所以我们使用下面的命令： sudo strace -f -p $(pidof redis-server) -T -e trace=fdatasync,write 2&gt;&amp;1 | grep -v &apos;0.0&apos; | grep -v unfinished 过期数据引起的响应延迟Redis有两种方式淘汰过期键： 一个被动删除过期键的方式是当一个键被一个命令访问时，如果发现它已经过期就删除之。 一个主动删除过期键的方式是每隔100毫秒删除掉一些过期键。 主动删除过期键这种方式被设计成自适应的。每隔100毫秒（即一秒执行10次）执行一次过期键删除，方式如下： 根据ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP的值采样键，删除所有已经过期的键。 如果采样出的键有超过25%的键过期了，重复这个采样过程。 ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP的默认值是20，采样删除过期键这个过程一秒执行10次，一般来说每秒最多有200个过期键被删除。那些已经过期很久的键也可以通过这种主动淘汰的方式被清除出数据库，所以被动删除方式意义不大。同时每秒钟删除200个键也不会引起Redis实例的响应延迟。 然而，主动淘汰算法是自适应的，如果在采样删除时有25%以上的键过期，将直接执行下一次循环。但考虑到我们每秒运行该算法10次，这意味着可能发生在同一秒内被采样的键25%以上都过期的情况。 基本上就是说，如果数据库在同一秒中有非常多键过期，而这些键至少占当前已经过期键的25%时，Redis为了让过期键占所有键的比例下降到25%以下将会阻塞。 这种做法是必要的，这可以避免已经过期的键占用太多内存。而且这种方式通常来说是绝对无害的，因为在同一秒有大量键过期的情况非常奇怪，但这种情况也不是完全不可能发生，因为用户可以使用EXPIREAT命令为键设置相同的过期时间。 简而言之：注意大量键同时过期引起的响应延迟。 Redis软件监控Redis 2.6引入了Redis软件监控的调试工具，用来跟踪那些无法用常规工具分析出的响应延迟问题。 Redis的软件监控是一个实验性地功能。虽然它被设计用于生产环境，由于在使用时它可能会与Redis服务器的正常运行产生意外的交互，所以使用前应该先备份数据库。 需要特别说明的是只有在万不得已的情况下再使用这种方式跟踪响应延迟问题。 下面是这个功能的工作细节： 用户使用CONFIG SET命令开启软件监控功能。 Redis不断地监控自己。 如果Redis检测到服务器被一些操作阻塞导致无法快速响应，则可能是响应延迟的问题所在，将会生成一份服务器在何处被阻塞的底层日志报告。 用户在Redis的Google Group中联系开发者，并展示监控报告的内容。 注意该功能无法在redis.conf文件中开启，因为这个功能被设计用于调试正在运行的实例。 使用下面的命令开启次功能： CONFIG SET watchdog-period 500 命令中的时间单位是毫秒。上面的示例中制定了尽在检测到服务器有超过500毫秒的延迟时才记录问题。最小的克配置时间是200毫秒。 当你不需要软件监控功能时，可以通过将watchdog-period参数设置为0来关闭它。非常重要：请记得在不需要软件监控时关闭它，因为一般来说长时间在实例上运行软件监控不是个好主意。 下面的例子展示了软件监控检测到了响应延迟超过配置时间的情况，在日志文件中输出的信息： [8547 | signal handler] (1333114359) --- WATCHDOG TIMER EXPIRED --- /lib/libc.so.6(nanosleep+0x2d) [0x7f16b5c2d39d] /lib/libpthread.so.0(+0xf8f0) [0x7f16b5f158f0] /lib/libc.so.6(nanosleep+0x2d) [0x7f16b5c2d39d] /lib/libc.so.6(usleep+0x34) [0x7f16b5c62844] ./redis-server(debugCommand+0x3e1) [0x43ab41] ./redis-server(call+0x5d) [0x415a9d] ./redis-server(processCommand+0x375) [0x415fc5] ./redis-server(processInputBuffer+0x4f) [0x4203cf] ./redis-server(readQueryFromClient+0xa0) [0x4204e0] ./redis-server(aeProcessEvents+0x128) [0x411b48] ./redis-server(aeMain+0x2b) [0x411dbb] ./redis-server(main+0x2b6) [0x418556] /lib/libc.so.6(__libc_start_main+0xfd) [0x7f16b5ba1c4d] ./redis-server() [0x411099] ------ 注意：在这个例子中DEBUG SLEEP命令用来使服务器阻塞。这个服务器阻塞的栈跟踪信息会随服务器上下文而异。 我们鼓励你将所搜集到的监控栈跟踪信息发送至Redis Google Group：获得的信息越多，就越能轻松了解你的Redis实例的问题所在。","categories":[{"name":"文档翻译","slug":"文档翻译","permalink":"https://nullcc.github.io/categories/文档翻译/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://nullcc.github.io/tags/Redis/"}]},{"title":"(译)Redis协议规范","slug":"(译)Redis协议规范","date":"2018-01-31T16:00:00.000Z","updated":"2022-04-15T03:41:13.010Z","comments":true,"path":"2018/02/01/(译)Redis协议规范/","link":"","permalink":"https://nullcc.github.io/2018/02/01/(译)Redis协议规范/","excerpt":"本文翻译自Redis Protocol specification。","text":"本文翻译自Redis Protocol specification。 Redis客户端和Redis服务器之间的通信遵循一个叫做RESP (REdis Serialization Protocol)的协议。这个协议是专门为Redis定制的，它可以用于其他客户端-服务器架构的软件项目。 RESP的设计考虑到了以下几个方面： 实现简单。可以快速被解析。人类可读。 RESP可以序列化不同的数据类型，比如整数、字符串、数组。它还为错误定制了特殊的类型。请求被从客户端以字符串数组的形式发送给Redis服务端执行。Redis返回一个特定的命令回复。 RESP是一个二进制安全的，并且它不需要将大量数据从一个进程传输到另一个进程，因为它使用前缀长度来传输数据块。 注意：这里的协议概述只对客户端-服务器的通信有效。Redis集群为了能够在Redis节点之间交换信息而使用了不同的二进制协议。 网络层一个客户端在连接到一个Redis服务器时会创建一个TCP连接，并连接到其6379端口。 虽然从技术上来说RESP并非必须使用TCP，但是在Redis的实际使用中RESP只使用TCP连接（或像Unix套接字这种面向流的连接）。 请求-响应模型Redis可以接受不同参数组合的命令。一旦收到一个命令，Redis将处理这个命令并向客户端响应一个回复。 这可能是最简单的模型了，然而这里有两个例外情况： Redis支持流水线操作（在后面的文档会描述）。这就使客户端可以一次性发送多条命令，并等待Redis服务端响应。 当一个Redis客户端订阅了一个发布/订阅的频道，协议将改变语义并变成一个推送协议，也就是说，客户端不再需要发送命令，因为服务端会自动在收到新消息时尽快发送新消息给客户端（新消息来自客户端订阅的频道）。 除了上面两种例外的情况，Redis协议是一个简单的请求-相应协议。 RESP协议描述RESP协议在Redis 1.2时被引入，但它在Redis 2.0时才被正式作为Redis客户端和服务器的交互协议。我们必须在Redis客户端实现这个协议。 RESP实际上是一个序列化协议，它支持如下数据类型：简单字符串、错误、整型、块字符串和数组。 在Redis中RESP被作为一个请求-响应协议使用： 客户端将命令整理成一个块字符串的RESP数组发送给Redis服务器。 Redis服务器根据命令的具体实现返回一个RESP数据类型给客户端。 在RESP中，数据类型取决于第一个字节： 响应的第一个字节为”+”表示简单字符串。 响应的第一个字节为”-“表示错误。 响应的第一个字节为”:”表示整型。 响应的第一个字节为”$”表示块字符串。 响应的第一个字节为”*”表示数组。 另外，在RESP中可以使用指定字符串或数组的特殊变体来代表空值。RESP使用”\\r\\n” (CRLF)来分割协议的不同部分。 RESP简单字符串简单字符串的编码方式是：以一个加号字符开头，后面跟一个不含CR或LF字符的字符串（可以没有换行符），最后以CRLF（即”\\r\\n”）结束。 简单字符串用来以最小的开销传输非二进制安全的字符串。比如很多Redis命令在成功执行后返回”OK”，这就是一个RESP简单字符串，它被编码成5个字节： &quot;+OK\\r\\n&quot; 为了发送二进制安全的字符串，需要使用RESP块字符串。 当Redis响应一个简单字符串时，客户端库应该将从’+’开始到字符串结尾的不包含结尾CRLF的字符串返回给调用者。 RESP错误RESP有一个叫错误的特殊数据类型。实际上错误类似于简单字符串，但是其第一个字符是一个减号(“-“)而不是加号。RESP中简单字符串和错误的真正区别在于错误会被客户端当成异常，而组成错误类型的字符串就是错误消息本身。 其基本的格式为： &quot;-Error message\\r\\n&quot; Redis只有在发生异常的时候才会发送错误响应，比如你尝试将一个操作施加到错误的数据类型上，或者命令不存在等等。当接收到一个错误响应时Redis客户端库应该抛出一个错误。 以下是错误回应的一些例子： -ERR unknown command &apos;foobar&apos; -WRONGTYPE Operation against a key holding the wrong kind of value 从”-“后的第一个字符到第一个空格或换行符，代表错误的类型。需要注意的是这只是Redis中的一种惯例做法，而不是RESP的错误格式要求。 比如，ERR表示一般的错误，而WRONGTYPE更具体一点，它表示客户端尝试将一个操作施加到错误的数据类型上。这种使用错误前缀的做法可以让客户端在不需要额外信息的情况下理解服务端返回的错误，不过随着时间的推移，这种情况可能会发生变化。 一个客户端实现可以针对不同的错误返回不同类型的异常，或者以直接提供错误名称字符串给调用者这种更通用的方式来捕获错误。 然而，由于这种功能很少被使用所以被认为是不重要的，一个受限制的客户端实现可能只是简单地返回一个一般的错误信息，比如false。 RESP整型这个类型很简单，就是一个”:”前缀加上数字，最后以CRLF作为结束符。比如”:0\\r\\n”或者”:1000\\r\\n”都是整型响应。 很多Redis命令都返回RESP整型，比如INCR、LLEN和LASTSAVE。 整型响应并没有什么特殊含义，对于INCR命令它只是一个递增的数字，对于LASTSAVE命令它是一个UNIX时间戳等等。然而，整型响应是一个有符号的64位整数。 整型响应也被广泛使用来表示true或false。一些命令像EXISTS或SISMEMBER会返回1表示true，0表示false。 其他命令像SADD，SREM和SETNX在命令确实被执行时返回1，否则返回0。 下面的一些命令会返回整型响应：SETNX, DEL, EXISTS, INCR, INCRBY, DECR, DECRBY, DBSIZE, LASTSAVE, RENAMENX, MOVE, LLEN, SADD, SREM, SISMEMBER, SCARD。 RESP块字符串块字符串被用来表示一个二进制安全的字符串，最大长度512MB。 块字符串使用如下方式进行编码： 以”$”开头，之后的数字表示字符串的长度，以CRLF结尾。 真正的字符串数据。 最后以CRLF结尾。 字符串”foobar”的编码如下： &quot;$6\\r\\nfoobar\\r\\n&quot; 一个空字符串的编码如下： &quot;$0\\r\\n\\r\\n&quot; 还可以使用RESP块字符串的特殊格式来表示不存在值或空值。在这种特殊格式中，字符串长度为-1，且没有数据，所以一个空值(Null)表示如下： &quot;$-1\\r\\n&quot; 这被叫做空块字符串。 当Redis服务器返回一个空块字符串时，客户端库API不应该返回一个空字符串，但可以返回一个nil对象。比如一个Ruby库应该返回’nil’而一个C库应该返回NULL（或者在返回对象中设置一个特殊的标志）等等。 RESP数组客户端使用RESP数组来向Redis服务器发送命令。同样，Redis服务器在执行某些命令后会使用RESP数组作为结果返回给客户端。一个例子是LRANGE命令会返回给客户端元素的列表作为回复。 RESP数组使用如下格式： 第一个字符为”*”，后面的数字表示数组中元素个数，最后以CRLF结尾。 数组中的每个元素都是RESP支持的类型。 一个空数组的表示如下： &quot;*0\\r\\n&quot; 一个包含两个块字符串”foo”和”bar”的数组表示如下： &quot;*2\\r\\n$3\\r\\nfoo\\r\\n$3\\r\\nbar\\r\\n&quot; 数组以*&lt;count&gt;CRLF开头，构成数组的其他数据类型一个接着一个的连在一起。例如一个有三个整型的数组编码如下： &quot;*3\\r\\n:1\\r\\n:2\\r\\n:3\\r\\n&quot; 数组中可以混合存放不同类型的数据，一个数组中不需要所有元素类型都相同。比如，一个包含了四个整型和一个块字符串的数组编码如下： *5\\r\\n :1\\r\\n :2\\r\\n :3\\r\\n :4\\r\\n $6\\r\\n foobar\\r\\n (清楚起见，服务器回复分成了多行来表示)。 第一行的*5\\r\\n指明回复的数组中有5个元素。然后由多个回复组成的多回复块将被传输给客户端。 空数组的概念也是存在的，这是另一种指定Null值的方式（一般来说我没使用空块字符串，不过由于历史原因我们有两种格式）。 一个例子是当BLPOP命令超时，将返回一个计数值为-1的空数组： &quot;*-1\\r\\n&quot; 一个客户端库API应该在Redis服务器回复一个空数组时返回一个空对象而不是空数组。这是区分空列表和不同情况的必要条件（比如BLPOP命令超时的情况）。 RESP也能表示嵌套数组。比如一个包含两个数组的数组编码如下： *2\\r\\n *3\\r\\n :1\\r\\n :2\\r\\n :3\\r\\n *2\\r\\n +Foo\\r\\n -Bar\\r\\n (清楚起见，服务器回复分成了多行来表示）。 上面的RESP数据类型编码了一个包含两个数组的数组，第一个数组的元素为3个整型：1，2和3，第二个数组包含了一个简单字符串和一个错误。 数组中的空值数组中的单个元素可能是空值。在Redis中这表示元素确实而不是空字符串。当指定的键不存在且我们用SORT命令排序时可能发生这种情况。以下是一个回复中数组包含一个空值的例子： *3\\r\\n $3\\r\\n foo\\r\\n $-1\\r\\n $3\\r\\n bar\\r\\n 第二个元素是一个空值。此时客户端库的返回应该像下面这样： [&quot;foo&quot;,nil,&quot;bar&quot;] 注意，这并不是前面几节中提到的异常，而是RESP更进一步的示例。 发送命令给Redis服务器现在你已经熟悉了RESP序列化格式了，写一个Redis客户端实现库也不是什么难事。我们可以更进一步了解一下客户端和服务器交互的细节了： 客户端将向Redis服务器发送块字符串组成的RESP数组。 Redis服务器向客户端回复合法的RESP数据。 一个典型的客户端和服务器交互过程如下。 客户端发送命令LLEN mylist来获取键mylist的值（一个列表）的长度，服务器回复一个整型，如下（C为客户端，S为服务器）： C: *2\\r\\n C: $4\\r\\n C: LLEN\\r\\n C: $6\\r\\n C: mylist\\r\\n S: :48293\\r\\n 通常我们为了表达上的清晰会将协议的不同部分用换行符分开，但在实际交互时客户端会将它们作为一个整体来发送：*2\\r\\n$4\\r\\nLLEN\\r\\n$6\\r\\nmylist\\r\\n。 多命令和流水线客户端可以使用相同的连接来发布多条命令。流水线支持客户端一次性向服务端发送多条命令，而不是一条条命令串行执行。所有命令的回复都可以一次性取得。 更多信息可以参考流水线。 内联命令有时候你只能使用telnet，且你需要发送一条命令给Redis服务端。然而Redis协议的实现很简单，在交互式会话中使用并不理想，并且Redis-cli并不总是可用。出于这些原因，Redis也接受一种为人类设计的特殊格式，叫做内联命令格式。 下面是服务器/客户端使用内联命令的例子（S为服务器，C为客户端）： C: PING S: +PONG 下面是回复整型的另一个例子： C: EXISTS somekey S: :0 基本上你只需要在telnet会话中处理空白符分隔的参数即可。由于没有任何一个命令以”*”开头，Redis在统一请求协议中可以使用，Redis可以检测到这种情况并解析你的命令。 高性能地解析Redis协议Redis协议的可读性很好且很容易实现，因此性能可以媲美二进制协议。 RESP使用前缀长度来批量传输数据，所以无须像解析JSON那样扫描数据来查找特殊的字符，也不需要将引用发送给服务器。 批量数据的长度可以用代码处理，每个字符执行一次操作，同时扫描CR字符，C语言代码如下： 12345678910111213141516#include &lt;stdio.h&gt;int main(void) &#123; unsigned char *p = \"$123\\r\\n\"; int len = 0; p++; while(*p != '\\r') &#123; len = (len*10)+(*p - '0'); p++; &#125; /* Now p points at '\\r', and the len is in bulk_len. */ printf(\"%d\\n\", len); return 0;&#125; 在第一个CR被标识出来以后，可以在没有任何处理的情况下直接跳过后面的LF。然后，可以不对有效载荷进行任何检查直接用单读的方式读取数据。最后留下的CR和LF字符可以不加处理直接忽略。 性能媲美二进制协议的Redis协议从一个很高的语言层次来进行设计，非常简单高效，这减少了客户端软件的bug数。","categories":[{"name":"文档翻译","slug":"文档翻译","permalink":"https://nullcc.github.io/categories/文档翻译/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://nullcc.github.io/tags/Redis/"}]},{"title":"某次面试的一道面试题","slug":"某次面试的一道面试题","date":"2018-01-30T16:00:00.000Z","updated":"2022-04-15T03:41:13.034Z","comments":true,"path":"2018/01/31/某次面试的一道面试题/","link":"","permalink":"https://nullcc.github.io/2018/01/31/某次面试的一道面试题/","excerpt":"最近面试遇到面试官让我现场敲代码解决一个问题，具体问题是，给定多个节点，这些节点之间有依赖关系（不考虑循环依赖，可能认为是无环图），接着以随机的顺序给定一个这些节点的列表，要求输出这些节点，具体规则时某个节点不能在其依赖的节点之前先被输出。","text":"最近面试遇到面试官让我现场敲代码解决一个问题，具体问题是，给定多个节点，这些节点之间有依赖关系（不考虑循环依赖，可能认为是无环图），接着以随机的顺序给定一个这些节点的列表，要求输出这些节点，具体规则时某个节点不能在其依赖的节点之前先被输出。 比如下图： 输出： 1[A, B, D, C, E] 是正确的，但输出： 1[B, A, D, C, E] 是错误的，因为B依赖于A，因此不能在A之前输出。 给出代码如下： 1234567891011121314151617181920212223242526272829303132333435#!/usr/bin/python#coding=utf-8def fn(nodes): if nodes is None or len(nodes) == 0: return None d = &#123;&#125; for node in nodes: d[node[0]] = node outputs = [] while nodes: node = nodes.pop(0) flag = True for parent in node[1]: if parent not in outputs: flag = False if flag: outputs.append(node[0]) else: nodes.append(node) return [d[name] for name in outputs]if __name__ == '__main__': A = [\"A\", []] B = [\"B\", [\"A\"]] C = [\"C\", [\"A\", \"D\"]] D = [\"D\", []] E = [\"E\", [\"B\", \"D\"]] inputs = [E, B, D, C, A] outputs = fn(inputs) print(outputs)","categories":[{"name":"面试题","slug":"面试题","permalink":"https://nullcc.github.io/categories/面试题/"}],"tags":[{"name":"面试题","slug":"面试题","permalink":"https://nullcc.github.io/tags/面试题/"}]},{"title":"二叉树的几种遍历方式","slug":"二叉树的几种遍历方式","date":"2018-01-27T16:00:00.000Z","updated":"2022-04-15T03:41:13.027Z","comments":true,"path":"2018/01/28/二叉树的几种遍历方式/","link":"","permalink":"https://nullcc.github.io/2018/01/28/二叉树的几种遍历方式/","excerpt":"\b二叉树的遍历在数据结构课程考试以及IT公司面试和笔试中还是相当常见的，这里稍作整理，以Python代码给出。","text":"\b二叉树的遍历在数据结构课程考试以及IT公司面试和笔试中还是相当常见的，这里稍作整理，以Python代码给出。 二叉树一般有四种遍历方式： 前序遍历 中序遍历 后序遍历 层级遍历 前序遍历 访问根节点 以前序遍历的方式访问左子树 以前序遍历的方式访问右子树 中序遍历 以中序遍历的方式访问左子树 访问根节点 以中序遍历的方式访问右子树 后序遍历 以后序遍历的方式访问左子树 以后序遍历的方式访问右子树 访问根节点 层级遍历 从根节点开始根据树的层次从左至右，从上到下访问节点 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122#!/usr/bin/python#coding=utf-8class Node(): \"\"\" 二叉树节点 \"\"\" def __init__(self, data=0, left=None, right=None): self.data = data self.left = left self.right = rightclass BTree(): \"\"\" 二叉树 \"\"\" def __init__(self): self.root = None def add(self, data): \"\"\" 向二叉树添加节点 \"\"\" if self.root is None: self.root = Node(data) return q = [self.root] while True: node = q.pop() if node.left is None: node.left = Node(data) return elif node.right is None: node.right = Node(data) return else: q.append(node.left) q.append(node.right) def preOrder(self, root): \"\"\" 前序遍历(递归方式) \"\"\" res = [] if root is None: return print(root.data), self.preOrder(root.left) self.preOrder(root.right) def inOrder(self, root): \"\"\" 中序遍历(递归方式) \"\"\" if root is None: return self.inOrder(root.left) print(root.data), self.inOrder(root.right) def postOrder(self, root): \"\"\" 后序遍历(递归方式) \"\"\" if root is None: return self.postOrder(root.left) self.postOrder(root.right) print(root.data), def levelOrder(self): \"\"\" 层级遍历 \"\"\" if self.root is None: return [] res = [] q = [self.root] while q: q1 = [] level = [] # 当前层级的节点列表 for node in q: level.append(node.data) if node.left is not None: q1.append(node.left) if node.right is not None: q1.append(node.right) q = q1 res.append(level) return resif __name__ == '__main__': tree = BTree() tree.add(0) tree.add(1) tree.add(2) tree.add(3) tree.add(4) tree.add(5) tree.add(6) print(\"前序遍历:\") tree.preOrder(tree.root) print(\"\\n\") print(\"中序遍历:\") tree.inOrder(tree.root) print(\"\\n\") print(\"后序遍历:\") tree.postOrder(tree.root) print(\"\\n\") print(\"层级遍历:\") res = tree.levelOrder() print(res)","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://nullcc.github.io/categories/数据结构/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://nullcc.github.io/tags/数据结构/"}]},{"title":"Redis中数据类型和内部编码的关系","slug":"Redis中数据类型和内部编码的关系","date":"2018-01-22T16:00:00.000Z","updated":"2022-04-15T03:41:13.017Z","comments":true,"path":"2018/01/23/Redis中数据类型和内部编码的关系/","link":"","permalink":"https://nullcc.github.io/2018/01/23/Redis中数据类型和内部编码的关系/","excerpt":"Redis中常用的数据类型主要有：字符串、列表、哈希、集合和有序集合，这些是键的type，那么这些type的底层实现是怎样的呢，本文将简单介绍一下各种数据类型对应的底层实现数据结构。在之前的几篇Redis源码分析文章中已经介绍了相关内容，不过这里作为一个整合会集中给出这些信息。 首先要知道的是Redis之所以会对不同的数据类型使用不同的内部编码方式主要还是为了节省内存，由于Redis会在内存中存放大量数据，因此根据数据特定来量身定做内部编码是非常有必要的。本文引用的全部源码都来自于Redis(version 3.2.11)。","text":"Redis中常用的数据类型主要有：字符串、列表、哈希、集合和有序集合，这些是键的type，那么这些type的底层实现是怎样的呢，本文将简单介绍一下各种数据类型对应的底层实现数据结构。在之前的几篇Redis源码分析文章中已经介绍了相关内容，不过这里作为一个整合会集中给出这些信息。 首先要知道的是Redis之所以会对不同的数据类型使用不同的内部编码方式主要还是为了节省内存，由于Redis会在内存中存放大量数据，因此根据数据特定来量身定做内部编码是非常有必要的。本文引用的全部源码都来自于Redis(version 3.2.11)。 在server.h文件中，使用宏定义给出了Redis支持的所有对象编码： 1234567891011121314151617181920212223242526272829303132333435/* Objects encoding. Some kind of objects like Strings and Hashes can be * internally represented in multiple ways. The 'encoding' field of the object * is set to one of this fields for this object. *//* 对象编码。一些对象比如字符串和哈希表在内部可以以不同的方式实现。这些对象的'encoding'字段 * 是下面的其中一个。 */// 原始编码#define OBJ_ENCODING_RAW 0 /* Raw representation */// 整型编码#define OBJ_ENCODING_INT 1 /* Encoded as integer */// 哈希表编码#define OBJ_ENCODING_HT 2 /* Encoded as hash table */// 压缩字典编码#define OBJ_ENCODING_ZIPMAP 3 /* Encoded as zipmap */// 链表编码#define OBJ_ENCODING_LINKEDLIST 4 /* Encoded as regular linked list */// 压缩链表编码#define OBJ_ENCODING_ZIPLIST 5 /* Encoded as ziplist */// 整数集合编码#define OBJ_ENCODING_INTSET 6 /* Encoded as intset */// 跳跃表编码#define OBJ_ENCODING_SKIPLIST 7 /* Encoded as skiplist */// 嵌入式字符串编码#define OBJ_ENCODING_EMBSTR 8 /* Embedded sds string encoding */// quicklist编码#define OBJ_ENCODING_QUICKLIST 9 /* Encoded as linked list of ziplists */ 下面就对这五种数据类型的内部编码进行介绍。 字符串（string）string的内部编码有三种： int（8字节长整型） embstr（长度小于或等于44个字节的字符串） raw（长度大于44个字节的字符串） 我们通过源码来理解这个事实，下面是object.c中的createStringObject函数： 123456789101112131415161718192021/* 从long long类型创建一个字符串对象 */robj *createStringObjectFromLongLong(long long value) &#123; robj *o; /* value ∈ [0, 10000)，这部分数字经常用到，内存中会预先创建一个这个范围的整数对象数组， * 对其增加引用计数后直接返回这个整数对象即可。 */ if (value &gt;= 0 &amp;&amp; value &lt; OBJ_SHARED_INTEGERS) &#123; incrRefCount(shared.integers[value]); o = shared.integers[value]; &#125; else &#123; /* value ∈ [LONG_MIN, LONG_MAX]，需要创建一个字符串对象，编码类型为OBJ_ENCODING_INT，即整数对象 */ if (value &gt;= LONG_MIN &amp;&amp; value &lt;= LONG_MAX) &#123; o = createObject(OBJ_STRING, NULL); o-&gt;encoding = OBJ_ENCODING_INT; o-&gt;ptr = (void*)((long)value); &#125; else &#123; // 超出long类型，将其编码成OBJ_STRING o = createObject(OBJ_STRING,sdsfromlonglong(value)); &#125; &#125; return o;&#125; 这段代码负责从一个long long类型的值创建一个string对象，可以看到当value ∈ [LONG_MIN, LONG_MAX]时，string的内部编码使用的是OBJ_ENCODING_INT，一个long long类型的值占用8字节，因此当string对象中保存的是8字节长整型时，内部会使用int编码方式。 继续看embstr和raw的情况，下面是object.c中的createStringObject函数： 12345678910111213/* Create a string object with EMBSTR encoding if it is smaller than * REIDS_ENCODING_EMBSTR_SIZE_LIMIT, otherwise the RAW encoding is * used. * * The current limit of 44 is chosen so that the biggest string object * we allocate as EMBSTR will still fit into the 64 byte arena of jemalloc. */#define OBJ_ENCODING_EMBSTR_SIZE_LIMIT 44robj *createStringObject(const char *ptr, size_t len) &#123; if (len &lt;= OBJ_ENCODING_EMBSTR_SIZE_LIMIT) return createEmbeddedStringObject(ptr,len); else return createRawStringObject(ptr,len);&#125; 这段代码的意思是当字符串长度小于OBJ_ENCODING_EMBSTR_SIZE_LIMIT时，使用embstr编码，否则使用raw编码。OBJ_ENCODING_EMBSTR_SIZE_LIMIT大小是44，需要注意的是在之前的一些版本中OBJ_ENCODING_EMBSTR_SIZE_LIMIT曾经为32和39，那么这个值是怎么来的呢，下面根据源代码进行分析。 createStringObject函数的注释中提到Redis会使用jemalloc来进行内存分配，jemalloc对此会分配64字节的内存。要详细了解jemalloc的内存分配策略和优势可以查阅相关资料。 下面是server.h中的部分代码，定义了robj结构体： 1234567891011// LRU时钟占用的位数#define LRU_BITS 24// redis对象结构体typedef struct redisObject &#123; unsigned type:4; // 对象类型 4bit unsigned encoding:4; // 对象编码 4bit unsigned lru:LRU_BITS; /* lru time (relative to server.lruclock) */ // LRU时间 24bit int refcount; // 对象引用计数 4字节 void *ptr; // 对象的数据指针 8字节&#125; robj; 可以计算出在64位机器上一个robj所占用的字节数为：4bit + 4bit + 24bit + 4字节 + 8字节 = 16字节。 再看sds.h中对sdshdr8的定义（除sdshdr5以外，sds.h中的sdshdr8、sdshdr16、sdshdr32和sdshdr64结构都是相同的，其中sdshdr5未被使用）： 123456struct __attribute__ ((__packed__)) sdshdr8 &#123; uint8_t len; /* used */ // 已使用的字符串长度 1字节 uint8_t alloc; /* excluding the header and null terminator */ // 分配的内存空间大小，不包括头部和空终止符 1字节 unsigned char flags; /* 3 lsb of type, 5 unused bits */ // 3个最低有效位表示类型，5个最高有效位未使用 1字节 char buf[]; // 字符数组 1字节&#125;; 可以计算出在一个sdshdr占用的字节数为：1字节 + 1字节 + 1字节 + 1字节 = 4字节。 所以对于分配了64字节，内部编码为embstr的字符串对象来说，字符串真正可用的字节数为：64字节 - 16字节 - 4字节 = 44字节。刚才提到了Redis之前的一些版本中OBJ_ENCODING_EMBSTR_SIZE_LIMIT的值为32和39，主要是sdshdr和robj的大小不同导致的。 列表（list）关于list的内部编码，不同版本的方法有所区别，先看Redis 3.2版本之前的做法： ziplist (压缩列表，当list中的元素个数小于list-max-ziplist-entries，默认512，且每个元素的大小都小于list-max-ziplist-value，默认64字节) linkedlist（链表，当list中的元素不满足ziplist的条件时，内部实现使用linkedlist） 在Redis 3.0版本的源码的redis.conf文件中可以看到： 12list-max-ziplist-entries 512list-max-ziplist-value 64 在Redis 3.2版本以及之后的版本，引入了一种叫做quicklist的数据结构，quicklist综合了ziplist和linkedlist的优点。从外部看，quicklist也是一个linkedlist，不过它的每个节点都是一个ziplist。我们来看看在redis.conf中的说明，在Redis 3.2.11版本的源码的redis.conf文件中： 1234567891011121314# Lists are also encoded in a special way to save a lot of space.# The number of entries allowed per internal list node can be specified# as a fixed maximum size or a maximum number of elements.# For a fixed maximum size, use -5 through -1, meaning:# -5: max size: 64 Kb &lt;-- not recommended for normal workloads# -4: max size: 32 Kb &lt;-- not recommended# -3: max size: 16 Kb &lt;-- probably not recommended# -2: max size: 8 Kb &lt;-- good# -1: max size: 4 Kb &lt;-- good# Positive numbers mean store up to _exactly_ that number of elements# per list node.# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),# but if your use case is unique, adjust the settings as necessary.list-max-ziplist-size -2 刚才我们提到在3.2+版本以后Redis使用quicklist来作为list类型的底层实现，其中的每个节点都是一个ziplist，这里list-max-ziplist-size的值为-2就表示每个ziplist的大小不能超过8kb（请注意是小写的b不是大写的B）。因此在3.2+版本的Redis中，list类型的表现如下： 123456127.0.0.1:6379&gt; lpush a 1 2 3(integer) 3127.0.0.1:6379&gt; type alist127.0.0.1:6379&gt; object encoding a&quot;quicklist&quot; 哈希（hash） ziplist（压缩列表，当hash中的元素个数小于hash-max-ziplist-entries，默认512，且每个元素的大小都小于hash-max-ziplist-value，默认64字节） hashtable（哈希表，当hash中的元素不满足ziplist的条件时，内部实现使用hashtable） 在redis.conf文件中可以看到： 12hash-max-ziplist-entries 512hash-max-ziplist-value 64 在t_hash.c的hashTypeSet函数中有如下一段代码： 12if (hashTypeLength(o) &gt; server.hash_max_ziplist_entries) hashTypeConvert(o, OBJ_ENCODING_HT); 这个if语句就是判断hash中元素个数是否超过hash-max-ziplist-entries设置的值，如果超过就将其转换成hashtable实现。 集合（set） intset（整数集合，当set中的元素都是整数且个数小于set-max-intset-entries，默认512） hashtable（哈希表，当set中的元素不满足intset的条件时，内部实现使用hashtable） 在redis.conf文件中可以看到： 1set-max-intset-entries 512 在t_set.c的setTypeAdd函数中有如下一段代码： 12if (intsetLen(subject-&gt;ptr) &gt; server.set_max_intset_entries) setTypeConvert(subject,OBJ_ENCODING_HT); 这个if语句就是判断set中元素个数是否超过set-max-intset-entries设置的值，如果超过就将其转换成hashtable实现。 有序集合（zset） ziplist（压缩链表，当zset中的元素个数小于zset-max-ziplist-entries，默认128，且每个元素的值都小于zset-max-ziplist-value，默认64字节） skiplist（跳跃表，当zset不满足ziplist的条件时，内部实现使用skiplist） 在redis.conf文件中可以看到： 12zset-max-ziplist-entries 128zset-max-ziplist-value 64 在t_zset.c的zsetConvertToZiplistIfNeeded函数中有如下一段代码： 123if (zset-&gt;zsl-&gt;length &lt;= server.zset_max_ziplist_entries &amp;&amp; maxelelen &lt;= server.zset_max_ziplist_value) zsetConvert(zobj,OBJ_ENCODING_ZIPLIST); 这个if语句就是判断zset中元素个数是否超过zset-max-ziplist-entries设置的值且其中长度最大的元素是否超过zset-max-ziplist-value，如果超过就将其转换成skiplist实现。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://nullcc.github.io/categories/数据库/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://nullcc.github.io/tags/Redis/"},{"name":"数据结构","slug":"数据结构","permalink":"https://nullcc.github.io/tags/数据结构/"}]},{"title":"论坛爬虫设计与实现","slug":"论坛爬虫设计与实现","date":"2017-12-31T16:00:00.000Z","updated":"2022-04-15T03:41:13.038Z","comments":true,"path":"2018/01/01/论坛爬虫设计与实现/","link":"","permalink":"https://nullcc.github.io/2018/01/01/论坛爬虫设计与实现/","excerpt":"需求概述论坛爬虫的基本需求有以下几个： 爬取指定论坛的帖子信息，并且能方便地新增其他论坛的爬取需求。 爬取到的帖子需要持久化到数据库中，在后台能够以论坛为单位查看爬取回来的帖子信息。 具有关键字过滤功能，可以对指定论坛设置若干个关键字，当爬取到的帖子匹配到关键字时，可以给指定邮箱或手机发送消息通知爬取事件。 爬虫后台可以实时监控爬取情况，并查看爬取信息。","text":"需求概述论坛爬虫的基本需求有以下几个： 爬取指定论坛的帖子信息，并且能方便地新增其他论坛的爬取需求。 爬取到的帖子需要持久化到数据库中，在后台能够以论坛为单位查看爬取回来的帖子信息。 具有关键字过滤功能，可以对指定论坛设置若干个关键字，当爬取到的帖子匹配到关键字时，可以给指定邮箱或手机发送消息通知爬取事件。 爬虫后台可以实时监控爬取情况，并查看爬取信息。 架构设计有了基础需求，就可以设计出一个基础架构，以下是论坛爬虫架构图： 爬虫前端可以以一个独立的子项目存在，因为今后会有扩展需求，爬虫前端必须考虑如何让多个论坛的爬取代码互不干扰且又可以抽象出一些共性。之所以使用消息队列(MQ)，主要是考虑到爬虫前端获得信息后，这个信息一般会有多个消费方，比较常见的有存储服务、关键字提取服务、事件报警服务等，如果在前期这些服务不是很复杂，也可以考虑合并成一个服务来处理，不过最好还是使用MQ来解耦，以备将来可能需要拆分服务。管理后台负责一些基本的展示类需求，如查看帖子、监控爬虫状态信息等。 具体实现todo","categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://nullcc.github.io/categories/爬虫/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://nullcc.github.io/tags/爬虫/"}]},{"title":"设计模式(23)——备忘录模式","slug":"设计模式(23)——备忘录模式","date":"2017-12-22T16:00:00.000Z","updated":"2022-04-15T03:41:13.041Z","comments":true,"path":"2017/12/23/设计模式(23)——备忘录模式/","link":"","permalink":"https://nullcc.github.io/2017/12/23/设计模式(23)——备忘录模式/","excerpt":"本文介绍备忘录模式的概念和应用。","text":"本文介绍备忘录模式的概念和应用。 基本思想和原则在不破坏封装性的前提下，捕获一个对象的内部状态，并在对象之外保存这个状态。这样以后就可将该对象恢复到原先保存的状态。 动机当需要保存一个对象的状态以备之后恢复时，可以使用备忘录模式。这样相当于提供了一个回滚操作，随时可以回滚到之前的状态。 实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class Player &#123; private int hp; public Player(int hp) &#123; this.hp = hp; &#125; public int getHp() &#123; return this.hp; &#125; public void setHp(int hp) &#123; this.hp = hp; &#125; public Memento createMemento() &#123; return new Memento(this.hp); &#125; public void restoreMemento(Memento memento) &#123; this.hp = memento.getHp(); &#125;&#125;public class Memento &#123; private int hp; public Memento(int hp) &#123; this.hp = hp; &#125; public int getHp() &#123; return this.hp; &#125; public void setHp(int hp) &#123; this.hp = hp; &#125;&#125;public class MementoManager &#123; private Memento memento; public MementoManager(Memento memento) &#123; this.memento = memento; &#125; public Memento getMemento() &#123; return this.memento; &#125; public void setMemento(Memento memento) &#123; this.memento = memento; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; Player player = new Player(100); MementoManager mementoManager = new MementoManager(player.createMemento()); System.out.println(\"玩家初始血量：\" + player.getHp()); player.setHp(60); System.out.println(\"玩家受到攻击，玩家当前血量：\" + player.getHp()); player.restoreMemento(mementoManager.getMemento()); System.out.println(\"读取存档，玩家当前血量：\" + player.getHp()); &#125;&#125; 输出如下： 123玩家初始血量：100玩家受到攻击，玩家当前血量：60读取存档，玩家当前血量：100 上面代码模拟了玩家初始HP为100，此时进行一次保存（创建一个备忘录），受到攻击后，血量下降，我们直接读取存档，玩家血量恢复到初始状态的场景。 优点备忘录模式将对象状态的一个备份保存在外部，这样就将对象和这个备份状态解耦了，我们可以随时将对象的状态回滚到之前的某个状态，只要之前这个状态有做备份。 注意事项使用备忘录模式需要注意的是对象的备份状态如果不需要了应当删除之，否则状态积累越多对内存压力也越大。另外大对象的状态可能很复杂很大，进行一次备份的成本很高，这个也需要适当考虑备份的频度。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(21)——组合模式","slug":"设计模式(21)——组合模式","date":"2017-12-21T16:00:00.000Z","updated":"2022-04-15T03:41:13.041Z","comments":true,"path":"2017/12/22/设计模式(21)——组合模式/","link":"","permalink":"https://nullcc.github.io/2017/12/22/设计模式(21)——组合模式/","excerpt":"本文介绍组合模式的概念和应用。","text":"本文介绍组合模式的概念和应用。 基本思想和原则将对象组合成树形结构以表示“部分-整体”的层次结构，使得用户对单个对象和组合对象的使用具有一致性。 动机当需要表示“部分-整体”这种层次结构时，组合模式是比较好的选择。 \b实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111public abstract class UIComponent &#123; private String name; public UIComponent(String name) &#123; this.name = name; &#125; public String getName() &#123; return this.name; &#125; public abstract void addChild(UIComponent component); public abstract void removeChild(UIComponent component); public abstract ArrayList&lt;UIComponent&gt; getChildren();&#125;public class Container extends UIComponent &#123; private ArrayList&lt;UIComponent&gt; children = new ArrayList&lt;UIComponent&gt;(); public Container(String name) &#123; super(name); &#125; @Override public void addChild(UIComponent component) &#123; this.children.add(component); &#125; @Override public void removeChild(UIComponent component) &#123; this.children.remove(component); &#125; @Override public ArrayList&lt;UIComponent&gt; getChildren() &#123; return this.children; &#125;&#125;public class Component extends UIComponent &#123; public Component(String name) &#123; super(name); &#125; @Deprecated public void addChild(UIComponent component) throws UnsupportedOperationException &#123; throw new UnsupportedOperationException(); &#125; @Deprecated public void removeChild(UIComponent component) throws UnsupportedOperationException &#123; throw new UnsupportedOperationException(); &#125; @Deprecated public ArrayList&lt;UIComponent&gt; getChildren() throws UnsupportedOperationException &#123; throw new UnsupportedOperationException(); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; UIComponent window = compositeUITree(); String info = displayTree(window); System.out.println(info); &#125; public static UIComponent compositeUITree() &#123; UIComponent window = new Container(\"window\"); UIComponent panel1 = new Container(\"panel1\"); UIComponent listView = new Container(\"listView\"); window.addChild(panel1); window.addChild(listView); UIComponent imageView1 = new Component(\"imageView1\"); UIComponent textView1 = new Component(\"textView1\"); panel1.addChild(imageView1); panel1.addChild(textView1); UIComponent panel2 = new Container(\"panel2\"); UIComponent imageView2 = new Component(\"imageView2\"); UIComponent textView2 = new Component(\"textView2\"); panel2.addChild(imageView2); panel2.addChild(textView2); UIComponent panel3 = new Container(\"panel3\"); UIComponent imageView3 = new Component(\"imageView3\"); UIComponent textView3 = new Component(\"textView3\"); panel3.addChild(imageView3); panel3.addChild(textView3); listView.addChild(panel2); listView.addChild(panel3); return window; &#125; public static String displayTree(UIComponent root) &#123; String info = \"\"; for (UIComponent c:root.getChildren()) &#123; if (c instanceof Container) &#123; info += c.getName() + \"\\n\" + displayTree(c); &#125; else &#123; info += c.getName() + \"\\n\"; &#125; &#125; return info; &#125;&#125; 输出如下： 12345678910panel1imageView1textView1listViewpanel2imageView2textView2panel3imageView3textView3 上面的代码模拟了一个UI组件树，使用组合模式可以非常容易地模拟这种结构。首先需要组装这棵树，当需要遍历整个组件树时，可以使用递归的方式遍历，就如displayTree中那样。实际项目中很多情境下会用到组合模式，比如公司人员层级结构、控件树等，当然对象之间的关系一般会保存在数据库中，需要时取出后进行组装，这里就不展开说明了。 这个例子是从上至下遍历，如果需要从下往上遍历，可以给每个节点加入一个parent属性。 优点组合模式对高层模块调用比较友好，高层不需要对具体对象是独立个体还是组合个体加以区分，直接使用即可。另外还可以在树形结构上自由增删节点。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(22)——访问者模式","slug":"设计模式(22)——访问者模式","date":"2017-12-21T16:00:00.000Z","updated":"2022-04-15T03:41:13.041Z","comments":true,"path":"2017/12/22/设计模式(22)——访问者模式/","link":"","permalink":"https://nullcc.github.io/2017/12/22/设计模式(22)——访问者模式/","excerpt":"本文介绍访问者模式的概念和应用。","text":"本文介绍访问者模式的概念和应用。 基本思想和原则封装一些作用于某种数据结构中的各元素的操作，它可以在不改变数据结构的前提下定义作用于这些元素的新操作。 动机如果我们要获取很多类的不同对象的信息时，可以考虑使用访问者模式。这可以将访问类对象信息这个操作和类本身分离。 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165public abstract class Staff &#123; private String name; private int age; private int salary; public Staff(String name, int age, int salary) &#123; this.name = name; this.age = age; this.salary = salary; &#125; public String getName() &#123; return this.name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return this.age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public int getSalary() &#123; return this.salary; &#125; public void setSalary(int salary) &#123; this.salary = salary; &#125; public abstract void accept(IVisitor visitor);&#125;public class Empolyee extends Staff &#123; private String job; public Empolyee(String name, int age, int salary) &#123; super(name, age, salary); &#125; public String getJob() &#123; return this.job; &#125; public void setJob(String job) &#123; this.job = job; &#125; public void accept(IVisitor visitor) &#123; visitor.visit(this); &#125;&#125;public class Manager extends Staff &#123; private int performance; public Manager(String name, int age, int salary) &#123; super(name, age, salary); &#125; public int getPerformance() &#123; return this.performance; &#125; public void setPerformance(int performance) &#123; this.performance = performance; &#125; public void accept(IVisitor visitor) &#123; visitor.visit(this); &#125;&#125;public interface IVisitor &#123; public void visit(Empolyee empolyee); public void visit(Manager manager);&#125; private String info = \"\"; @Override public void visit(Empolyee empolyee) &#123; this.info += this.getBasicInfo(empolyee) + \" \" + this.getEmpolyeeJob(empolyee) + \"\\n\"; &#125; @Override public void visit(Manager manager) &#123; this.info += this.getBasicInfo(manager) + \" \" + this.getManagerPerformance(manager) + \"\\n\"; &#125; private String getBasicInfo(Staff staff) &#123; return staff.getName() + \" \" + staff.getAge() + \" \" + staff.getSalary(); &#125; private String getEmpolyeeJob(Empolyee empolyee) &#123; return empolyee.getJob(); &#125; private int getManagerPerformance(Manager manager) &#123; return manager.getPerformance(); &#125; public void showStaffInfo() &#123; System.out.println(this.info); &#125;&#125;public class BonusVisitor implements IVisitor &#123; private final static int EMPOLYEE_COEFFICIENT = 2; private final static int MANAGER_COEFFICIENT = 4; private int totalBonus = 0; @Override public void visit(Empolyee empolyee) &#123; this.totalBonus += empolyee.getSalary() * EMPOLYEE_COEFFICIENT; &#125; @Override public void visit(Manager manager) &#123; this.totalBonus += manager.getSalary() * MANAGER_COEFFICIENT; &#125; public void showTotalBonus() &#123; System.out.println(\"所有人的奖金总和是: \" + this.totalBonus); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; InfoVisitor infoVisitor = new InfoVisitor(); BonusVisitor bonusVisitor = new BonusVisitor(); for (Staff staff:getStaffs()) &#123; staff.accept(infoVisitor); staff.accept(bonusVisitor); &#125; infoVisitor.showStaffInfo(); bonusVisitor.showTotalBonus(); &#125; private static ArrayList&lt;Staff&gt; getStaffs() &#123; ArrayList&lt;Staff&gt; staffs = new ArrayList&lt;Staff&gt;(); Empolyee empolyee1 = new Empolyee(\"Jack\", 30, 8000); Empolyee empolyee2 = new Empolyee(\"Bob\", 23, 4000); Empolyee empolyee3 = new Empolyee(\"Jane\", 33, 9000); Manager manager1 = new Manager(\"John\", 50, 20000); empolyee1.setJob(\"Sale\"); empolyee2.setJob(\"Engineer\"); empolyee3.setJob(\"Accounting\"); manager1.setPerformance(100000); staffs.add(empolyee1); staffs.add(empolyee2); staffs.add(empolyee3); staffs.add(manager1); return staffs; &#125;&#125; 输出如下： 123456Jack 30 8000 SaleBob 23 4000 EngineerJane 33 9000 AccountingJohn 50 20000 100000所有人的奖金总和是: 122000 IVisitor声明了访问者的抽象接口，有两个具体的实现类：InfoVisitor和BonusVisitor，分别负责获取雇员的个人信息和计算奖金，这两个类的职责不同，一个打印信息，一个计算奖金，所以分为两个访问者类来实现没什么问题，实际开发中也提倡这么做。 优点访问者模式符合单一职责原则，被访问者和访问者都可以各自独立发展，互不干扰。扩展性也不错，当需要增加新的访问需求时，只需要增加新的访问者实现类即可。 缺点访问者模式必须了解一个具体类的细节，者违反了迪米特法则。另外有一种情况的扩展比较困难，当我在具体类中加入新的属性时，就需要修改访问者类，如果访问者类比较多，修改量会很大。访问者模式没有依赖抽象，而是依赖具体，这也违反了依赖倒置原则。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(20)——解释器模式","slug":"设计模式(20)——解释器模式","date":"2017-12-20T16:00:00.000Z","updated":"2022-04-15T03:41:13.040Z","comments":true,"path":"2017/12/21/设计模式(20)——解释器模式/","link":"","permalink":"https://nullcc.github.io/2017/12/21/设计模式(20)——解释器模式/","excerpt":"本文介绍解释器模式的概念和应用。","text":"本文介绍解释器模式的概念和应用。 基本思想和原则给定一门语言，定义它的文法的一种表示，并定义一个解释器，该解释器使用该表示来解释语言中的句子。 动机当有一些简单的语法需要解析时，可以考虑使用解释器模式建立规则，比如解析各种格式的日志。 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103public abstract class Expression &#123; public abstract int interpreter(HashMap&lt;String, Integer&gt; var);&#125;public class VarExpression extends Expression &#123; private String key; public VarExpression(String key) &#123; this.key = key; &#125; @Override public int interpreter(HashMap&lt;String, Integer&gt; var) &#123; return var.get(this.key); &#125;&#125;public abstract class SymbolExpression extends Expression &#123; protected Expression left; protected Expression right; public SymbolExpression(Expression left, Expression right) &#123; this.left = left; this.right = right; &#125;&#125;public class AddExpression extends SymbolExpression &#123; public AddExpression(Expression left, Expression right) &#123; super(left, right); &#125; public int interpreter(HashMap&lt;String, Integer&gt; var) &#123; return this.left.interpreter(var) + this.right.interpreter(var); &#125;&#125;public class SubExpression extends SymbolExpression&#123; public SubExpression(Expression left, Expression right) &#123; super(left, right); &#125; public int interpreter(HashMap&lt;String, Integer&gt; var) &#123; return this.left.interpreter(var) - this.right.interpreter(var); &#125;&#125;public class Calculator &#123; private Expression expression; private Stack&lt;Expression&gt; stack = new Stack&lt;Expression&gt;(); public Calculator(String expStr) &#123; char[] charArray = expStr.toCharArray(); Expression left = null; Expression right = null; for (int i = 0; i &lt; charArray.length; i++) &#123; switch (charArray[i]) &#123; case '+': left = this.stack.pop(); right = new VarExpression(String.valueOf(charArray[++i])); this.stack.push(new AddExpression(left, right)); break; case '-': left = this.stack.pop(); right = new VarExpression(String.valueOf(charArray[++i])); this.stack.push(new SubExpression(left, right)); break; default: this.stack.push(new VarExpression(String.valueOf(charArray[i]))); &#125; &#125; this.expression = this.stack.pop(); &#125; public int run(HashMap&lt;String, Integer&gt; var) &#123; return this.expression.interpreter(var); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; String expStr1 = \"a+b-c\"; HashMap&lt;String, Integer&gt; var1 = new HashMap&lt;String, Integer&gt;(); var1.put(\"a\", 10); var1.put(\"b\", 2); var1.put(\"c\", 5); Calculator calculator1 = new Calculator(expStr1); int res1 = calculator1.run(var1); System.out.println(expStr1 + \"=\" + res1); String expStr2 = \"a+b-c-d\"; HashMap&lt;String, Integer&gt; var2 = new HashMap&lt;String, Integer&gt;(); var2.put(\"a\", 100); var2.put(\"b\", 34); var2.put(\"c\", 12); var2.put(\"d\", 8); Calculator calculator2 = new Calculator(expStr1); int res2 = calculator2.run(var2); System.out.println(expStr1 + \"=\" + res2); &#125;&#125; 输出如下： 12a+b-c=7a+b-c-d=122 优点解释器模式的优点是扩展性比较好，当需要增加语法规则时，只需要增加独立的类并实现即可。 缺点当语法规则复杂时，类数量膨胀非常厉害，解释器模式使用递归的方式来处理问题，会一定程度上影响性能。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(19)——享元模式","slug":"设计模式(19)——享元模式","date":"2017-12-19T16:00:00.000Z","updated":"2022-04-15T03:41:13.040Z","comments":true,"path":"2017/12/20/设计模式(19)——享元模式/","link":"","permalink":"https://nullcc.github.io/2017/12/20/设计模式(19)——享元模式/","excerpt":"本文介绍享元模式的概念和应用。","text":"本文介绍享元模式的概念和应用。 基本思想和原则使用共享对象可有效地支持大量的细粒度的对象。 动机当系统中有大量相似对象存在，它们具有一定的共性，但又有自己特殊的地方，我们可以利用享元模式将这些共性提炼成对象的外部特征，预先创建出这些对象，之后需要时可以直接取出使用，然后再赋予其非共性的属性。 实现数据库连接池就是一个典型的享元模式的应用，预先创建一些数据库连接，当需要时直接取用，而不是立刻创建一个，这种池技术可以有效降低系统开销。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public class DBConnection &#123; private int id; public DBConnection(int id) &#123; this.id = id; &#125; public int getId() &#123; return this.id; &#125;&#125;public class DBConnectionFactory &#123; private static Vector&lt;DBConnection&gt; pool = new Vector&lt;DBConnection&gt;(); private static Vector&lt;Integer&gt; connectionStates = new Vector&lt;Integer&gt;(); public static void init(int n) &#123; for(int i = 0; i &lt; n; i++) &#123; DBConnection conn = new DBConnection(i); pool.add(conn); connectionStates.add(0); &#125; &#125; public static DBConnection getDBConnection() &#123; for(int i = 0; i &lt; pool.size(); i++) &#123; if (connectionStates.get(i).equals(0)) &#123; connectionStates.set(i, 1); return pool.get(i); &#125; &#125; return null; &#125; public static void releaseDBConnection(DBConnection conn) &#123; int index = pool.indexOf(conn); connectionStates.set(index, 0); &#125;&#125;public class DBClientThread extends Thread &#123; private DBConnection conn; public DBClientThread(DBConnection conn) &#123; this.conn = conn; &#125; public void run() &#123; System.out.println(\"Use DBConnection id: \" + this.conn.getId()); try &#123; Thread.sleep(250); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; DBConnectionFactory.releaseDBConnection(this.conn); this.conn = null; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; DBConnectionFactory.init(5); for (int i = 0; i &lt; 10; i++) &#123; DBConnection conn = DBConnectionFactory.getDBConnection(); if (conn != null) &#123; DBClientThread thread = new DBClientThread(conn); thread.start(); &#125; else &#123; System.out.println(\"Can not get a db conn.\"); &#125; try &#123; Thread.sleep(50); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 输出如下： 12345678910Use DBConnection id: 0Use DBConnection id: 1Use DBConnection id: 2Use DBConnection id: 3Use DBConnection id: 4Use DBConnection id: 0Use DBConnection id: 1Use DBConnection id: 2Use DBConnection id: 3Use DBConnection id: 4 上面的代码模拟了数据库连接池的使用，我们预先创建5个数据库连接，然后创建出10个线程去获取这个连接。当要获取连接时，不是去实时地创建连接，而是从连接池中获取，使用完连接后还要将连接放回池中（其实就是改变连接的状态）。这几个连接对象被共享出来，供各个线程获取。这是享元模式的一种很重要的应用。 优点享元模式预先创建出一些对象，然后缓存这些对象，当需要时直接取出使用。可以帮助系统减少对象的创建，降低内存占用，提高系统性能。 缺点享元模式会使系统的复杂度提高，我们需要维护这些对象的创建和存在。而且如果系统不需要对象时，这些预先创建的对象还会存在，也是占用内存的，不过这个一般不会成为太大问题。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(18)——桥接模式","slug":"设计模式(18)——桥接模式","date":"2017-12-18T16:00:00.000Z","updated":"2022-04-15T03:41:13.040Z","comments":true,"path":"2017/12/19/设计模式(18)——桥接模式/","link":"","permalink":"https://nullcc.github.io/2017/12/19/设计模式(18)——桥接模式/","excerpt":"本文介绍桥接模式的概念和应用。","text":"本文介绍桥接模式的概念和应用。 基本思想和原则将抽象和实现解耦，使得两者可以独立地变化。 动机当抽象和具体实现无法很好契合时，使用继承会导致产生过多的类，此时可以考虑使用桥接模式，使用聚合的方式来连接抽象和具体实现。另外当对重用性有较高要求时也可以使用桥接模式。 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public abstract class Factory &#123; IElectronicProduct electronicProduct; public Factory(IElectronicProduct electronicProduct) &#123; this.electronicProduct = electronicProduct; &#125; public void produce() &#123; this.electronicProduct.beProduced(); &#125; public IElectronicProduct getElectronicProduct() &#123; return this.electronicProduct; &#125;&#125;public class ElectronicProductFactory extends Factory &#123; public ElectronicProductFactory(IElectronicProduct electronicProduct) &#123; super(electronicProduct); &#125; @Override public void produce() &#123; super.produce(); super.getElectronicProduct().beUsed(); &#125;&#125;public interface IElectronicProduct &#123; public void beProduced(); public void beUsed();&#125;public class Radio implements IElectronicProduct &#123; @Override public void beProduced() &#123; System.out.println(\"Radio be produced.\"); &#125; @Override public void beUsed() &#123; System.out.println(\"Radio be used.\"); &#125;&#125;public class Television implements IElectronicProduct &#123; @Override public void beProduced() &#123; System.out.println(\"Television be produced.\"); &#125; @Override public void beUsed() &#123; System.out.println(\"Television be used.\"); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; Factory electronicProductFactory1 = new ElectronicProductFactory(new Radio()); electronicProductFactory1.produce(); Factory electronicProductFactory2 = new ElectronicProductFactory(new Television()); electronicProductFactory2.produce(); &#125;&#125; 输出如下： 1234Radio be produced.Radio be used.Television be produced.Television be used. 上面代码模拟了一个工厂生产电子产品的场景，抽象类Factory内部聚合了一个实现了IElectronicProduct接口的类的对象，我们可以自由增加具体的产品类，只要实现了IElectronicProduct这个接口即可。同样只要抽象层和具体实现之间的接口不变，这里指的是Factory类的produce方法，抽象层和具体实现就可以各自独立变化互不影响。 优点桥接模式通过将抽象层聚合具体实现的方式来将抽象和实现分离，使二者可以独立扩展。另外封装了具体实现，使之不会暴露给外部，具有很好的封装性。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(17)——状态模式","slug":"设计模式(17)——状态模式","date":"2017-12-18T16:00:00.000Z","updated":"2022-04-15T03:41:13.039Z","comments":true,"path":"2017/12/19/设计模式(17)——状态模式/","link":"","permalink":"https://nullcc.github.io/2017/12/19/设计模式(17)——状态模式/","excerpt":"本文介绍状态模式的概念和应用。","text":"本文介绍状态模式的概念和应用。 基本思想和原则当一个对象内在状态改变时允许改变其行为，这个对象看起来像改变了其类。 动机当某个对象的状态改变时，其行为也会发生改变，就可以考虑使用状态模式来处理。 实现我们用一个水三态转换的例子来说明状态模式，正常情况水有三种形态：液态、固态和气态，分别对应水、冰和水蒸气。水的三态转换过程如下图： 三种状态之间可以任意转换，我们下面就模拟这个过程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181public abstract class WaterState &#123; protected Context context; public void setContext(Context context) &#123; this.context = context; &#125; // 液化 public abstract void liquefy(); // 汽化 public abstract void evaporate(); // 凝固 public abstract void freeze(); // 熔化 public abstract void melt(); // 凝华 public abstract void desublimate(); // 升华 public abstract void sublimate();&#125;public class LiquidState extends WaterState &#123; @Override public void liquefy() &#123; System.out.println(\"水蒸气-&gt;水：液化过程\"); &#125; @Override public void evaporate() &#123; this.context.setCurrentState(Context.gasState); this.context.getCurrentState().evaporate(); &#125; @Override public void freeze() &#123; this.context.setCurrentState(Context.solidState); this.context.getCurrentState().freeze(); &#125; @Override public void melt() &#123; System.out.println(\"冰-&gt;水：熔化过程\"); &#125; @Override public void desublimate() &#123; // do nothing &#125; @Override public void sublimate() &#123; // do nothing &#125;&#125;public class SolidState extends WaterState &#123; @Override public void liquefy() &#123; // do nothing &#125; @Override public void evaporate() &#123; // do nothing &#125; @Override public void freeze() &#123; System.out.println(\"水-&gt;冰：凝固过程\"); &#125; @Override public void melt() &#123; this.context.setCurrentState(Context.liquidState); this.context.getCurrentState().melt(); &#125; @Override public void desublimate() &#123; System.out.println(\"水蒸气-&gt;冰：凝华过程\"); &#125; @Override public void sublimate() &#123; this.context.setCurrentState(Context.gasState); this.context.getCurrentState().sublimate(); &#125;&#125;public class GasState extends WaterState &#123; @Override public void liquefy() &#123; this.context.setCurrentState(Context.liquidState); this.context.getCurrentState().liquefy(); &#125; @Override public void evaporate() &#123; System.out.println(\"水-&gt;水蒸气：气化过程\"); &#125; @Override public void freeze() &#123; // do nothing &#125; @Override public void melt() &#123; // do nothind &#125; @Override public void desublimate() &#123; this.context.setCurrentState(Context.solidState); this.context.getCurrentState().desublimate(); &#125; @Override public void sublimate() &#123; System.out.println(\"冰-&gt;水蒸气：升华过程\"); &#125;&#125;public class Context &#123; private WaterState currentState; public final static WaterState liquidState = new LiquidState(); public final static WaterState solidState = new SolidState(); public final static WaterState gasState = new GasState(); public void setCurrentState(WaterState currentState) &#123; this.currentState = currentState; this.currentState.setContext(this); &#125; public WaterState getCurrentState() &#123; return this.currentState; &#125; public void liquefy() &#123; this.currentState.liquefy(); &#125; public void evaporate() &#123; this.currentState.evaporate(); &#125; public void freeze() &#123; this.currentState.freeze(); &#125; public void melt() &#123; this.currentState.melt(); &#125; public void desublimate() &#123; this.currentState.desublimate(); &#125; public void sublimate() &#123; this.currentState.sublimate(); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; Context context = new Context(); context.setCurrentState(new LiquidState()); context.evaporate(); // 水-&gt;水蒸气：汽化过程 context.desublimate(); // 水蒸气-&gt;冰：凝华过程 context.melt(); // 冰-&gt;水：熔化过程 context.freeze(); // 水-&gt;冰：凝固过程 context.sublimate(); // 冰-&gt;水蒸气：升华过程 context.liquefy(); // 水蒸气-&gt;水：液化过程 &#125;&#125; 输出如下： 123456水-&gt;水蒸气：气化过程水蒸气-&gt;冰：凝华过程冰-&gt;水：熔化过程水-&gt;冰：凝固过程冰-&gt;水蒸气：升华过程水蒸气-&gt;水：液化过程 简单分析一下这个例子，我们有一个State的继承体系，WaterState是一个抽象类，定义了所有状态间转换的方法，并且在内部持有一个Context类实例，这个实例在状态转换过程中会使用到。LiquidState、SolidState和GasState是具体的状态类，需要实现WaterState定义的所有抽象方法。Context类持有一个状态实例currentState，表示水当前的状态，很重要的一点是Context类中也定义了所有状态转换的方法，调用时会委托给currentState来处理。 优点状态模式避免了过多的switch-case和if-else语句，代码清晰。符合开闭原则、单一职责原则，当需要增加状态时，只需要增加子类，不需要修改原有的代码。封装性很好，外部不需要了解状态之间具体是如何转换的，所有的状态间过渡方式都由各个状态子类负责。 缺点状态模式的缺点是有可能会导致过多的状态子类。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(16)——外观模式","slug":"设计模式(16)——外观模式","date":"2017-12-17T16:00:00.000Z","updated":"2022-04-15T03:41:13.039Z","comments":true,"path":"2017/12/18/设计模式(16)——外观模式/","link":"","permalink":"https://nullcc.github.io/2017/12/18/设计模式(16)——外观模式/","excerpt":"本文介绍外观模式的概念和应用。","text":"本文介绍外观模式的概念和应用。 基本思想和原则要求一个子系统的外部与其内部的通信必须通过一个统一的对象进行。外观模式提供了一个高层次的接口，使得子系统更易于使用。 动机当某个子系统的业务逻辑和内部关系比较复杂时，可以考虑使用外观模式做一层封装，对外屏蔽这个子系统，高层模块不直接和子系统交互，而是和外观对象交互，外观对象在内部将真正的业务处理过程委托给子系统来处理。 实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Consulate &#123; public void getTravelVisa() &#123; System.out.println(\"Consulate get travel visa.\"); &#125;&#125;public class AirlineCompany &#123; public void bookingAirlineTickets() &#123; System.out.println(\"AirlineCompany booking airline tickets.\"); &#125;&#125;public class Hotel &#123; public void bookingRoom() &#123; System.out.println(\"Hotel booking room.\"); &#125;&#125;public class Context &#123; private Consulate consulate = new Consulate(); private AirlineCompany airlineCompany = new AirlineCompany(); private Hotel hotel = new Hotel(); public void travelAbroad() &#123; System.out.println(\"------ travel abroad start ------\"); this.consulate.getTravelVisa(); this.airlineCompany.bookingAirlineTickets(); this.hotel.bookingRoom(); System.out.println(\"------ travel abroad end ------\"); &#125;&#125;public class TravelAgency &#123; private Context context = new Context(); public void travelAbroad() &#123; this.context.travelAbroad(); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; TravelAgency travelAgency = new TravelAgency(); travelAgency.travelAbroad(); &#125;&#125; 输出如下： 12345------ travel abroad start ------Consulate get travel visa.AirlineCompany booking airline tickets.Hotel booking room.------ travel abroad end ------ 上面的代码模拟了一个人出国旅游的过程，出国旅游之前至少需要以下几个流程：签证、订机票、订酒店房间。如果让用户（高层模块）直接去处理这几件事情（一个子系统）是很麻烦的事情，于是旅行社这种机构出现了，只要用户提交了申请材料，它就能帮助用户办完这一切事物，旅行社相当于出国旅行这一系列操作的一个“外观”，用户只需要和旅行社沟通就能搞定一切事情，不需要分别去找领事馆、航空公司和当地酒店，对用户而言原本一堆复杂的事情经过旅行社“包装”一下变得异常轻松。 优点外观模式减少了高层模块对子系统内部的依赖性，高层模块只需要依赖外观对象，而对子系统内部是一无所知的。另外外观模式使得子系统变动的灵活性比较高，只要外观对象不变，子系统的变化就不会影响到高层模块。同时外观模式还能对子系统实施安全控制，只提供允许外部访问的功能。 缺点外观模式的一个问题就是外观对象的责任重大，修改它的代价可能会很大。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(15)——观察者模式","slug":"设计模式(15)——观察者模式","date":"2017-12-16T16:00:00.000Z","updated":"2022-04-15T03:41:13.039Z","comments":true,"path":"2017/12/17/设计模式(15)——观察者模式/","link":"","permalink":"https://nullcc.github.io/2017/12/17/设计模式(15)——观察者模式/","excerpt":"本文介绍观察者模式的概念和应用。","text":"本文介绍观察者模式的概念和应用。 基本思想和原则定义对象间一种一对多的依赖关系，使得每当一个对象改变状态，则所有依赖于它的对象都会得到通知并被自动更新。另外观察者模式又被称为发布/订阅模式。 动机当一个对象的状态变化会导致其他对象的变化时，可以考虑使用观察者模式。 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class Server extends Observable &#123; private String name; public Server(String name) &#123; this.name = name; &#125; public String getName() &#123; return this.name; &#125; public void setName(String name) &#123; this.name = name; &#125; public void cpuOverload() &#123; System.out.println(this.getName() + \" CPU overload.\"); super.setChanged(); super.notifyObservers(\"cpu overload\"); &#125; public void diskOverload() &#123; System.out.println(this.getName() + \" disk overload.\"); super.setChanged(); super.notifyObservers(\"disk overload\"); &#125;&#125;public class Monitor implements Observer &#123; private String name; public Monitor(String name) &#123; this.name = name; &#125; public String getName() &#123; return this.name; &#125; public void setName(String name) &#123; this.name = name; &#125; public void update(Observable o, Object arg) &#123; String msg = (String)arg; Server server = (Server)o; if (msg.equalsIgnoreCase(\"cpu overload\")) &#123; System.out.println(this.getName() + \" got cpu overload message of \" + server.getName() + \".\"); &#125; else if (msg.equalsIgnoreCase(\"disk overload\")) &#123; System.out.println(this.getName() + \" got disk overload message of \" + server.getName() + \".\"); &#125; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; Server server1 = new Server(\"Server_1\"); Monitor monitor1 = new Monitor(\"Monitor_1\"); Monitor monitor2 = new Monitor(\"Monitor_2\"); server1.addObserver(monitor1); server1.addObserver(monitor2); server1.cpuOverload(); server1.diskOverload(); &#125;&#125; 输出如下： 123456Server_1 CPU overload.Monitor_2 got cpu overload message of Server_1.Monitor_1 got cpu overload message of Server_1.Server_1 disk overload.Monitor_2 got disk overload message of Server_1.Monitor_1 got disk overload message of Server_1. 上面的代码模拟了一个监控器监视服务器状态的场景，这个场景中服务器是被观察者，监控器是观察者。注意Server类继承了Observable类，Observable类中已经实现了addObserver、deleteObserver和notifyObservers这几个方法，用于添加观察者，删除观察者和通知观察者，其在内部维护了一个观察者数组，通知观察者时遍历这个观察者数组，一个个通知即可。另外Server类在cpuOverload和diskOverload两个方法中将CPU过载和磁盘过载的事件通知观察者。 Monitor类实现了Observer接口，这个接口只有一个方法需要实现： 1void update(Observable o, Object arg); update方法是在被观察者通知观察者的时候被调用的，在这个方法中观察者会收到两个参数：被观察者对象的引用和一个通知数据参数，观察者在这个方法中对被观察者发生的状态变动做出自己的响应。 Java中对观察者模式已经有了很好的封装，一般情况下我们只需将被观察者继承Observable类，观察者实现Observer接口，然后编写自己的业务逻辑即可，非常方便。 优点观察者模式建立了一套抽象的事件触发机制，观察者和被观察之间可以独立扩展。 缺点在使用观察者模式时要注意不要建立过长的观察链，这有可能导致性能问题，出问题时排查也较为困难。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(14)——迭代器模式","slug":"设计模式(14)——迭代器模式","date":"2017-12-14T16:00:00.000Z","updated":"2022-04-15T03:41:13.039Z","comments":true,"path":"2017/12/15/设计模式(14)——迭代器模式/","link":"","permalink":"https://nullcc.github.io/2017/12/15/设计模式(14)——迭代器模式/","excerpt":"本文介绍迭代器模式的概念和应用。","text":"本文介绍迭代器模式的概念和应用。 基本思想和原则迭代器模式提供一种方法访问一个容器对象中的各个元素，而又不需要暴露该对象的内部细节。 动机迭代器模式有两个主要的使用动机：封装性和统一的访问模式。现在很多语言都提供了现成的迭代器模式实现，比如Java中的各种Collection类型：ArrayList、Set、HashMap等，大部分情况下我们不用自己去实现迭代器模式，只要使用现成的集合类型即可。 实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class Book &#123; private ArrayList&lt;Page&gt; pageArrayList = new ArrayList&lt;Page&gt;(); public void add(Page page) &#123; this.pageArrayList.add(page); &#125; public void remove(Page page) &#123; this.pageArrayList.remove(page); &#125; public IBookIterator iterator() &#123; return new IBookIterator(this.pageArrayList); &#125;&#125;public class IBookIterator implements Iterator &#123; private ArrayList&lt;Page&gt; pageArrayList; private int currentIdx = 0; public IBookIterator(ArrayList&lt;Page&gt; bookArrayList) &#123; this.pageArrayList = bookArrayList; &#125; public boolean hasNext() &#123; return currentIdx &lt; this.pageArrayList.size(); &#125; @Override public Page next() &#123; return (Page)this.pageArrayList.get(this.currentIdx++); &#125;&#125;public class Page &#123; private int pageNumber; public Page(int pageNumber) &#123; this.pageNumber = pageNumber; &#125; public int getPageNumber() &#123; return pageNumber; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; Book book = new Book(); book.add(new Page(1)); book.add(new Page(2)); book.add(new Page(3)); book.add(new Page(4)); book.add(new Page(5)); IBookIterator bookIterator = book.iterator(); while (bookIterator.hasNext()) &#123; Page page = bookIterator.next(); System.out.println(\"Page number: \" + page.getPageNumber()); &#125; &#125;&#125; 输出如下： 12345Page number: 1Page number: 2Page number: 3Page number: 4Page number: 5 上面的代码模拟了一本书翻页的过程，一本书中包含很多页面，这就是一个应用迭代模式的场景。Book类有有一个iterator方法，这个方法会返回一个IBookIterator类型的书中页面的迭代器，IBookIterator类实现了Iterator接口中的hasNext和next方法。 优点封装性好，提供统一的访问模式。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(13)——适配器模式","slug":"设计模式(13)——适配器模式","date":"2017-12-13T16:00:00.000Z","updated":"2022-04-15T03:41:13.039Z","comments":true,"path":"2017/12/14/设计模式(13)——适配器模式/","link":"","permalink":"https://nullcc.github.io/2017/12/14/设计模式(13)——适配器模式/","excerpt":"本文介绍适配器模式的概念和应用。","text":"本文介绍适配器模式的概念和应用。 基本思想和原则将一个类的接口变换成客户端所期待的另一种接口，从而使原本因接口不匹配而无法在一起工作的两个类能够在一起工作。 动机当我们需要对某个类做一些修改以适应原有系统时，此时如果系统已经良好运行且修改原有模块的代价很大时，可以考虑使用适配器模式。适配器模式的引入可以最小化修改量，并让原本不兼容的模块在一起工作。 实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485// 电源抽象类public interface Power &#123; public HashMap&lt;String, Double&gt; output(Double power);&#125;// 用电器抽象类public abstract class ElectricAppliance &#123; private Double ratedVoltage; private Double ratedPower; public ElectricAppliance(Double ratedVoltage, Double ratedPower) &#123; this.ratedVoltage = ratedVoltage; this.ratedPower = ratedPower; &#125; public Double getRatedPower() &#123; return ratedPower; &#125; public Double getRatedVoltage() &#123; return ratedVoltage; &#125; public void input(Double voltage, Double power) &#123; System.out.println(this.getClass().getName() + \" get voltage \" + voltage + \"V power \" + power + \"W.\"); if (voltage.equals(this.getRatedVoltage()) &amp;&amp; power.equals(this.getRatedPower())) &#123; System.out.println(this.getClass().getName() + \" work!\"); &#125; else &#123; System.out.println(this.getClass().getName() + \" can't work!\"); &#125; &#125;&#125;// 家用电源输出220Vpublic class HouseholdPower implements Power &#123; public HashMap&lt;String, Double&gt; output(Double power) &#123; HashMap&lt;String, Double&gt; res = new HashMap&lt;&gt;(); res.put(\"voltage\", new Double(220)); res.put(\"power\", power); return res; &#125;&#125;// 笔记本电脑，接受输入20V/60Wpublic class Laptop extends ElectricAppliance &#123; public Laptop(Double ratedVoltage, Double ratedPower) &#123; super(ratedVoltage, ratedPower); &#125;&#125;// 笔记本电脑电源适配器，接受输入220V，功率60W，输出20V/60Wpublic class PowerAdapter extends ElectricAppliance implements Power &#123; public PowerAdapter(Double ratedVoltage, Double ratedPower) &#123; super(ratedVoltage, ratedPower); &#125; public HashMap&lt;String, Double&gt; output(Double power) &#123; HashMap&lt;String, Double&gt; res = new HashMap&lt;&gt;(); res.put(\"voltage\", new Double(20)); res.put(\"power\", power); return res; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; Power householdPower = new HouseholdPower(); ElectricAppliance laptop = new Laptop(new Double(20), new Double(60)); PowerAdapter powerAdapter = new PowerAdapter(new Double(220), new Double(60)); System.out.println(\"----- Not used power adapter. -----\"); // 家用电源直接对笔记本电脑供电，电脑无法工作 HashMap&lt;String, Double&gt; householdPowerOutput = householdPower.output(new Double(60)); laptop.input(householdPowerOutput.get(\"voltage\"), householdPowerOutput.get(\"power\")); System.out.println(); System.out.println(\"----- Used power adapter. -----\"); // 通过电源适配器给电脑供电，电脑正常工作 powerAdapter.input(householdPowerOutput.get(\"voltage\"), householdPowerOutput.get(\"power\")); HashMap&lt;String, Double&gt; powerAdapterOutput = powerAdapter.output(new Double(60)); laptop.input(powerAdapterOutput.get(\"voltage\"), powerAdapterOutput.get(\"power\")); &#125;&#125; 输出如下： 123456789----- Not used power adapter. -----patterns.adapter.Laptop get voltage 220.0V power 60.0W.patterns.adapter.Laptop can&apos;t work!----- Used power adapter. -----patterns.adapter.PowerAdapter get voltage 220.0V power 60.0W.patterns.adapter.PowerAdapter work!patterns.adapter.Laptop get voltage 20.0V power 60.0W.patterns.adapter.Laptop work! 上面的代码模拟了电源适配器(Power Adapter)的工作过程，我们在用笔记本电脑时会看到电源线的一端有一个像盒子一样的东西，这东西就是电源适配器。在中国家用电源电压是220V，普通的电子产品一般额定电压也就是5V-20V左右，直接将电器接入220V的电压会损坏电器，这时就需要电源适配器来将输入电压转换成电器需要的电压。 上面的电源适配器接受220V/60W的额定输入，经过转换后，输出为20V/60W，可以直接提供给笔记本电脑使用。简单起见这里忽略了电源适配器的发热损耗，变压器的工作原理是输入功率和输出功率相等（不计损耗时）。 优点适配器模式是一种补救模式，它可以使两个原本不兼容的类在一起工作，而这对高层模块是透明的，高层模块甚至不会知道真正处理任务的类是哪个。适配器模式的灵活性很好，当需要让两个不兼容的类一起工作时，我们就创建一个适配器，如果不需要这个适配器了，直接删除就好，不需要改动其他模块的代码。 缺点需要注意适配器模式的使用时机，如果系统的功能正处于开发阶段，就不要使用适配器模式，这个模式是给那些已经良好运行且修改原有代码代价很大的时候使用的。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(11)——责任链模式","slug":"设计模式(11)——责任链模式","date":"2017-12-12T16:00:00.000Z","updated":"2022-04-15T03:41:13.038Z","comments":true,"path":"2017/12/13/设计模式(11)——责任链模式/","link":"","permalink":"https://nullcc.github.io/2017/12/13/设计模式(11)——责任链模式/","excerpt":"本文介绍责任链模式的概念和应用。","text":"本文介绍责任链模式的概念和应用。 基本思想和原则使多个对象都有机会处理请求，从而避免了请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链传递该请求，直到有对象处理它为止。 动机如果一个处理过程中，多个处理环节的优先级有顺序关系（甚至我们希望可以随意组合这些环节以构成不同的处理方式），而且具体由哪个环节来处理依赖于一些条件，我们可以将其抽象为责任链模式。 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128public abstract class Handler &#123; private Handler nextHandler; public void setNextHandler(Handler handler) &#123; this.nextHandler = handler; &#125; public final Response handleMessage(Request request) &#123; Response response = null; if (this.getHanlerLevel().equals(request.getLevel())) &#123; response = this.makeResponse(request); &#125; else &#123; if (this.nextHandler != null) &#123; System.out.println(\"pass to next handler...\"); response = this.nextHandler.handleMessage(request); &#125; &#125; return response; &#125; protected abstract Level getHanlerLevel(); protected abstract Response makeResponse(Request request);&#125;public class BranchManager extends Handler &#123; @Override protected Level getHanlerLevel() &#123; return new Level(1); &#125; @Override protected Response makeResponse(Request request) &#123; return new Response(\"BranchManager: OK\"); &#125;&#125;public class Director extends Handler &#123; @Override protected Level getHanlerLevel() &#123; return new Level(2); &#125; @Override protected Response makeResponse(Request request) &#123; return new Response(\"Director: OK\"); &#125;&#125;public class GeneralManager extends Handler &#123; @Override protected Level getHanlerLevel() &#123; return new Level(3); &#125; @Override protected Response makeResponse(Request request) &#123; return new Response(\"GeneralManager: OK\"); &#125;&#125;public class Level &#123; private int level; public Level(int level) &#123; this.level = level; &#125; public int getLevel() &#123; return level; &#125; @Override public boolean equals(Object obj) &#123; if (obj instanceof Level) &#123; Level level = (Level) obj; return this.level == level.level; &#125; return false; &#125;&#125;public class Request &#123; private Level level; public Request(Level level) &#123; this.level = level; &#125; public Level getLevel() &#123; return this.level; &#125;&#125;public class Response &#123; private String content; public Response(String content) &#123; this.content = content; &#125; public String getContent() &#123; return content; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; Handler branchManager = new BranchManager(); Handler director = new Director(); Handler generalManager = new GeneralManager(); branchManager.setNextHandler(director); director.setNextHandler(generalManager); Request request1 = new Request(new Level(1)); Response response1 = branchManager.handleMessage(request1); System.out.println(response1.getContent()); Request request2 = new Request(new Level(2)); Response response2 = branchManager.handleMessage(request2); System.out.println(response2.getContent()); Request request3 = new Request(new Level(3)); Response response3 = branchManager.handleMessage(request3); System.out.println(response3.getContent()); &#125;&#125; 输出如下： 123456BranchManager: OKpass to next handler...Director: OKpass to next handler...pass to next handler...GeneralManager: OK 上面的代码模拟了一个公司的审批流程，员工发起一个请求，请求有三种等级，等级一只需要部门经理审批，等级二需要总监审批，等级三需要总经理审批，责任链的形式为：部门经理-&gt;总监-&gt;总经理。我们在场景中分别模拟了这三种等级。 需要注意的是，我们在场景类中设置了责任链中各个节点的关系： 12branchManager.setNextHandler(director);director.setNextHandler(generalManager); 在责任链模式中，最核心的概念是责任的传递，链上的每个节点在接收到一个请求时都需要做如下判断：如果该请求的等级是自己可以处理的就直接处理并返回一个响应，否则就传递给链上的下一个节点处理。之后如果需要在链上增加新的处理节点也非常简单，只需要增加一个处理节点的类，然后更新链中节点关系即可。 优点责任链模式对调用方屏蔽了请求处理的细节，调用方不需要去了解具体是哪个类在处理请求，也不需要了解是这么处理的，只要将请求传给链中的第一个对象，就会获得一个返回结果。这有利于类间解耦。 缺点责任链的缺点是有可能建立一条很长的处理链导致系统性能受到影响，这在实际设计过程中要注意。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(12)——装饰器模式","slug":"设计模式(12)——装饰器模式","date":"2017-12-12T16:00:00.000Z","updated":"2022-04-15T03:41:13.039Z","comments":true,"path":"2017/12/13/设计模式(12)——装饰器模式/","link":"","permalink":"https://nullcc.github.io/2017/12/13/设计模式(12)——装饰器模式/","excerpt":"本文介绍装饰器模式的概念和应用。","text":"本文介绍装饰器模式的概念和应用。 基本思想和原则动态地给一个对象添加一些额外的职责。就增加功能来说，装饰器模式比生成子类更加灵活。 动机我们经常有这样的需求，需要对一个对象进行扩展，一种能直接想到的方案是对原有类派生出多个子类，在子类中实现扩展的功能。这么做是可以，但是缺点也非常明显，继承是一种静态行为，如果我有很多特性需要动态添加到某个对象上，用继承的方式需要派生出非常多的子类，整个类体系臃肿不堪，类数量暴增，到最后根本无法维护。这时候使用装饰器模式可以有效避免这种灾难，装饰器是一种动态的行为，可以动态的增加一些功能到对象上，并且各个装饰器和原来的类耦合很小，扩展起来非常方便。 实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public abstract class Handler &#123; public abstract void handle();&#125;public class RequestHandler extends Handler &#123; @Override public void handle() &#123; System.out.println(\"Request processing success.\"); &#125;&#125;public abstract class Decorator extends Handler &#123; private Handler handler; public Decorator(Handler handler) &#123; this.handler = handler; &#125; @Override public void handle() &#123; this.handler.handle(); &#125;&#125;public class LogDecorator extends Decorator &#123; public LogDecorator(Handler handler) &#123; super(handler); &#125; private void log() &#123; System.out.println(\"LogDecorator log.\"); &#125; public void handle() &#123; super.handle(); this.log(); &#125;&#125;public class BlacklistDecorator extends Decorator &#123; public BlacklistDecorator(Handler handler) &#123; super(handler); &#125; private void filterBlacklist() &#123; System.out.println(\"BlacklistDecorator filter blacklist.\"); &#125; public void handle() &#123; this.filterBlacklist(); super.handle(); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; Handler requestHandler = new RequestHandler(); System.out.println(\"------ Has no decorator ------\"); requestHandler.handle(); System.out.println(); System.out.println(\"------ Add a log decorator ------\"); requestHandler = new LogDecorator(requestHandler); requestHandler.handle(); System.out.println(); System.out.println(\"------ Add a blacklist decorator ------\"); requestHandler = new BlacklistDecorator(requestHandler); requestHandler.handle(); &#125;&#125; 输出如下： 1234567891011------ Has no decorator ------Request processing success.------ Add a log decorator ------Request processing success.LogDecorator log.------ Add a blacklist decorator ------BlacklistDecorator filter blacklist.Request processing success.LogDecorator log. 上面的代码模拟了一个请求被处理的过程，其中RequestHandler类负责实际地处理一个请求，此时我们想在这个基础上增加过滤黑名单和记录日志的功能。使用装饰器模式可以很容易地实现这个功能，装饰器抽象类Decorator\b\b可以派生出很多具体的装饰器类，这些装饰器类在内部都维护一个Handler类的实例，装饰器也需要实现Handler类的handle方法，在内部实际上是将具体的处理过程委托给了this.handler去处理，在具体的处理前后增加需要扩展的功能。 BlacklistDecorator装饰器是在请求被处理前进行黑名单检查，LogDecorator装饰器是在请求被处理后进行日志记录。可以看到当在原对象上增加一个装饰器，实际上就是对原对象进行了一层包装，增加几个装饰器就是包装几层。 实际上很多Web Framework中的过滤器都是采用装饰器的模式来实现的。 优点装饰器和被装饰的对象相互之间可以独立扩展，耦合度比较低。装饰器模式可以实现对一个对象动态地扩展功能，非常灵活。装饰器模式是比生成子类更轻量级的扩展方式。 缺点要注意对一个对象的装饰层数不要太多，否则调试起来会比较困难。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(10)——中介者模式","slug":"设计模式(10)——中介者模式","date":"2017-12-11T16:00:00.000Z","updated":"2022-04-15T03:41:13.038Z","comments":true,"path":"2017/12/12/设计模式(10)——中介者模式/","link":"","permalink":"https://nullcc.github.io/2017/12/12/设计模式(10)——中介者模式/","excerpt":"本文介绍中介者模式的概念和应用。","text":"本文介绍中介者模式的概念和应用。 基本思想和原则用一个中介对象封装一系列的对象交互，中介者使各对象不需要显式地相互作用，从而使其耦合松散，而且可以独立地改变它们之间的交互。 动机实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108public class AutoDealer &#123; private AutoManufacturer autoManufacturer; private InsuranceCompany insuranceCompany; private Customer customer; public void setAutoManufacturer(AutoManufacturer autoManufacturer) &#123; this.autoManufacturer = autoManufacturer; &#125; public AutoManufacturer getAutoManufacturer() &#123; return autoManufacturer; &#125; public void setInsuranceCompany(InsuranceCompany insuranceCompany) &#123; this.insuranceCompany = insuranceCompany; &#125; public InsuranceCompany getInsuranceCompany() &#123; return insuranceCompany; &#125; public void setCustomer(Customer customer) &#123; this.customer = customer; &#125; public Customer getCustomer() &#123; return customer; &#125; // 签订保险协议 public void signInsuranceAgreement() &#123; this.insuranceCompany.draftInsuranceAgreement(); this.customer.payForInsurance(); &#125; // 购买汽车 public void buyCar() &#123; this.autoManufacturer.produceCar(); this.customer.payForCar(); &#125;&#125;public class AutoManufacturer &#123; private AutoDealer autoDealer; public AutoManufacturer(AutoDealer autoDealer) &#123; this.autoDealer = autoDealer; &#125; // 生产汽车 public void produceCar() &#123; System.out.println(\"Auto manufacturer produce car.\"); &#125;&#125;public class Customer &#123; private AutoDealer autoDealer; public Customer(AutoDealer autoDealer) &#123; this.autoDealer = autoDealer; &#125; public void payForInsurance() &#123; System.out.println(\"Customer pay for insurance.\"); &#125; public void payForCar() &#123; System.out.println(\"Customer pay for car.\"); &#125; // 签订保险协议 public void signInsuranceAgreement() &#123; this.autoDealer.signInsuranceAgreement(); &#125; // 购买汽车 public void buyCar() &#123; this.autoDealer.buyCar(); &#125;&#125;public class InsuranceCompany &#123; private AutoDealer autoDealer; public InsuranceCompany(AutoDealer autoDealer) &#123; this.autoDealer = autoDealer; &#125; // 起草保险协议 public void draftInsuranceAgreement() &#123; System.out.println(\"Insurance company draft an insurance agreement.\"); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; AutoDealer autoDealer = new AutoDealer(); AutoManufacturer autoManufacturer = new AutoManufacturer(autoDealer); InsuranceCompany insuranceCompany = new InsuranceCompany(autoDealer); Customer customer = new Customer(autoDealer); autoDealer.setAutoManufacturer(autoManufacturer); autoDealer.setInsuranceCompany(insuranceCompany); autoDealer.setCustomer(customer); customer.buyCar(); customer.signInsuranceAgreement(); &#125;&#125; 输出如下： 1234Auto manufacturer produce car.Customer pay for car.Insurance company draft an insurance agreement.Customer pay for insurance. 上面的代码模拟了一个消费者购买汽车的过程，这里面存在四个基本实体：汽车厂家、保险公司、汽车经销商和消费者。具体的交互过程主要有两个：消费者订购汽车和消费者为汽车购买保险。消费者订购汽车的过程是：厂家生产一辆汽车，消费者付款购买汽车。消费者为汽车购买保险的过程是，保险公司起草一份保险协议，消费者付款购买保险。现实中的过程要比这个复杂一些，这里为了好理解做了简化处理。 很明显这里面存在多方交互的问题，消费者——汽车经销商——汽车厂家，消费者——汽车经销商——保险公司，这里的汽车经销商其实就是一个中介者，负责协调多方实现某个过程。作为消费者，购买汽车时并不需要直接去找具体的汽车厂家，而是让汽车经销商来协调这一切，最后完成购买。买汽车保险也是类似，汽车经销商会帮你搞定一切保险事宜，消费者最后只需要签字并付款即可。 在中介者模式的实现上，要注意中介者需要在内部维护各个具体类的实例，各个具体类也需要维护一个中介者的实例，当具体类需要调用其他类的方法时（即和其他类产生了交互），应该调用中介者中某个方法，让中介者来协调这一系列的调用，中介者在这个方法中通过调用各个具体类的方法来完成整个过程。 优点当有多个类相互之间有很多交互时，使用中介者模式可以减少类之间的耦合，原本一个具体的实现类需要依赖于多个类，现在只需要依赖中介者。 缺点中介者模式的缺点很明显，当具体实现类增加，类之间的交互增加时，中介者会变得非常庞大和臃肿。实际编码过程中应当注意这个问题，在必要时做进一步的拆分。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(9)——原型模式","slug":"设计模式(9)——原型模式","date":"2017-12-11T16:00:00.000Z","updated":"2022-04-15T03:41:13.042Z","comments":true,"path":"2017/12/12/设计模式(9)——原型模式/","link":"","permalink":"https://nullcc.github.io/2017/12/12/设计模式(9)——原型模式/","excerpt":"本文介绍原型模式的概念和应用。","text":"本文介绍原型模式的概念和应用。 基本思想和原则用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。 动机当我们需要创建大量对象时，如果用传统的new一个对象的方式，在效率上会比较低，因为类的初始化过程开销较大，我们可以通过直接拷贝一个对象来获得一个新对象。另外在多线程场景下，由于在同一个对象上操作为了保证线程安全就需要加锁，降低了并发性能，这时我们一般希望产生不同对象进行操作。这些场景都是原型模式的适用场景。 实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class Minion implements Cloneable &#123; private int id; public Minion(int id) &#123; this.id = id; &#125; public void sing() &#123; System.out.println(\"Ba-ba-ba, Ba-banana. Ba-ba-ba, Ba-banana～\"); &#125; public void eat() &#123; System.out.println(\"Minion eat!\"); &#125; public void sleep() &#123; System.out.println(\"Minion sleep!\"); &#125; public void setId(int id) &#123; this.id = id; &#125; public int getId() &#123; return this.id; &#125; @Override public Minion clone() &#123; Minion minion = null; try &#123; minion = (Minion)super.clone(); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); &#125; return minion; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; int count = 10; Minion minion = new Minion(0); for (int i = 0; i &lt; count; i++) &#123; Minion cloneMinion = minion.clone(); cloneMinion.setId(i); System.out.print(\"Minion \" + cloneMinion.getId() + \" is singing: \"); cloneMinion.sing(); &#125; &#125;&#125; 输出如下： 12345678910Minion 0 is singing: Ba-ba-ba, Ba-banana. Ba-ba-ba, Ba-banana～Minion 1 is singing: Ba-ba-ba, Ba-banana. Ba-ba-ba, Ba-banana～Minion 2 is singing: Ba-ba-ba, Ba-banana. Ba-ba-ba, Ba-banana～Minion 3 is singing: Ba-ba-ba, Ba-banana. Ba-ba-ba, Ba-banana～Minion 4 is singing: Ba-ba-ba, Ba-banana. Ba-ba-ba, Ba-banana～Minion 5 is singing: Ba-ba-ba, Ba-banana. Ba-ba-ba, Ba-banana～Minion 6 is singing: Ba-ba-ba, Ba-banana. Ba-ba-ba, Ba-banana～Minion 7 is singing: Ba-ba-ba, Ba-banana. Ba-ba-ba, Ba-banana～Minion 8 is singing: Ba-ba-ba, Ba-banana. Ba-ba-ba, Ba-banana～Minion 9 is singing: Ba-ba-ba, Ba-banana. Ba-ba-ba, Ba-banana～ 上面的代码中定义了一个“小黄人”类Minion，需要产生大量小黄人，这里可以使用原型模式。我们将Minion类实现了Cloneable接口，如果去查看Cloneable接口，会发现这个接口里实际上没有任何方法： 12public interface Cloneable &#123;&#125; 很奇怪吧？那么这个clone方法从何而来呢？在Object类中我们找到下面的代码： 1protected native Object clone() throws CloneNotSupportedException; 可以看出clone方法是从Object类继承而来的，Java中几乎所有对象都是继承自Object类，另外@Override修饰也说明了clone方法是继承而来的。所以Cloneable接口只是简单地标记一个类可以调用clone方法。 需要注意的是，拷贝分为浅拷贝和深拷贝。上面这种方式是浅拷贝，浅拷贝的意思是只复制基本类型的数据，对于引用数据，比如数组、集合、哈希表或其他类的实例对象，只复制引用。如果要实现深拷贝，需要自行在clone方法中对响应对象做拷贝操作。 优点原型模式在需要大量产生对象的时候比直接new一个对象性能上更好，类初始化的时候比较耗费资源，原型模式是直接在内存中通过拷贝二进制数据来获得一个新对象，因此性能上占优势。而且原型模式在拷贝时不会调用构造函数，这在某些时候有一定用处。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(8)——命令模式","slug":"设计模式(8)——命令模式","date":"2017-12-10T16:00:00.000Z","updated":"2022-04-15T03:41:13.042Z","comments":true,"path":"2017/12/11/设计模式(8)——命令模式/","link":"","permalink":"https://nullcc.github.io/2017/12/11/设计模式(8)——命令模式/","excerpt":"本文介绍命令模式的概念和应用。","text":"本文介绍命令模式的概念和应用。 基本思想和原则将一个请求封装成一个对象，从而让你使用不同的请求把客户端参数化，对请求排队或者记录请求日志，可以提供命令的撤销和恢复功能。 动机为了使客户类在不需要了解具体细节的情况下获得想要的结果，我们将一个个命令封装成对象，即命令对象，由一个调度者来使命令对象得以执行。 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class ProductManager &#123; public void makeRequirements() &#123; System.out.println(\"ProductManager make requirements.\"); &#125;&#125;public class Designer &#123; public void design() &#123; System.out.println(\"Designer desgin.\"); &#125;&#125;public class Programer &#123; public void writeCode() &#123; System.out.println(\"Programer write code.\"); &#125;&#125;public abstract class Command &#123; protected ProductManager productManager = new ProductManager(); protected Designer designer = new Designer(); protected Programer programer = new Programer(); public abstract void execute();&#125;public class FunACommand extends Command &#123; @Override public void execute() &#123; this.programer.writeCode(); &#125;&#125;public class FunBCommand extends Command&#123; @Override public void execute() &#123; this.productManager.makeRequirements(); this.designer.design(); this.programer.writeCode(); &#125;&#125;public class ProjectManager &#123; private Command command; public void setCommand(Command command) &#123; this.command = command; &#125; public void action() &#123; this.command.execute(); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; ProjectManager projectManager = new ProjectManager(); Command command1 = new FunACommand(); projectManager.setCommand(command1); projectManager.action(); &#125;&#125; 输出如下： 1Programer write code. 如果现在想执行FunBCommand，只需要将\b调用端改为： 12345678910public class Test &#123; public static void main(String[] args) &#123; ProjectManager projectManager = new ProjectManager(); // Command command1 = new FunACommand(); Command command2 = new FunACommand(); projectManager.setCommand(command2); projectManager.action(); &#125;&#125; 输出如下： 123ProductManager make requirements.Designer desgin.Programer write code. 上面的代码场景中有四个角色：产品经理、设计师、程序员和项目经理，其中项目经理作为一个调度者，负责向下传递命令给产品经理、设计师和程序员。我们将具体的命令封装成命令对象，客户类只需要实例化具体的命令对象，然后传递给项目经理，就能执行相应的命令。具体到上面的例子，FunACommand和FunBCommand分别封装了两个命令，其中FunACommand只需要程序员参与，FunBCommand则需要产品经理、设计师和程序员的共同参与。如果有新的命令，可以继续用这种模式封装起来。 优点将调用者和具体执行任务的类之间解耦，调用者不需要了解具体执行任务的类，只需要知道命令即可。命令模式的可扩展性也很好，当需要增加新的命令时，增加具体命令的类即可完成扩展。 缺点当具体的命令非常多的情况下，会造成整个Command类体系非常庞大。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"谈谈控制反转(IoC)和依赖注入(DI)","slug":"谈谈控制反转(IoC)和依赖注入(DI)","date":"2017-12-08T16:00:00.000Z","updated":"2022-04-15T03:41:13.043Z","comments":true,"path":"2017/12/09/谈谈控制反转(IoC)和依赖注入(DI)/","link":"","permalink":"https://nullcc.github.io/2017/12/09/谈谈控制反转(IoC)和依赖注入(DI)/","excerpt":"我们常常看到控制反转和依赖注入这两个词，到底是什么含义，在现实编码过程中又有什么意义呢，本文将谈谈这些问题。","text":"我们常常看到控制反转和依赖注入这两个词，到底是什么含义，在现实编码过程中又有什么意义呢，本文将谈谈这些问题。 控制反转(Inverse of Control, IoC)一开始看上去会让人一头雾水：要控制谁？反转给谁？怎么反转？依赖注入(Dependency Injection, DI)从字面上稍微好理解一点，就是把某个事物所依赖的东西注入到这个事物中。 先来看一段代码： 123456public class Driver &#123; public void drive() &#123; Car car = new Car(); car.run(); &#125;&#125; 上面的代码中司机要开车，于是直接在drive函数中实例化了一辆小汽车来驾驶，这样写能运行是没错，但是有一个问题，如果现在我要驾驶一辆卡车，就得修改Driver类的drive函数，那之后要驾驶摩托车呢？吊车呢？这就很麻烦了，仅仅是为了驾驶不同种类的车辆就要修改Driver类的drive函数，也就是说Driver类和具体的车辆类产生了很强的耦合性。这种情况我们是不愿意看到的，这里有三种解耦方式：构造函数注入、变量注入、接口注入。 构造函数注入修改代码如下： 12345678910public class Driver &#123; private Vehicle vehicle; public Driver(Vehicle vehicle) &#123; this.vehicle = vehicle; &#125; public void drive() &#123; this.vehicle.run(); &#125;&#125; 这里我们做了两件事，一是将具体车辆改为用构造函数参数的形式传入，二是构造函数参数的类型是一个抽象类Vehicle。在声明变量时，我们应该尽量使用抽象类或接口作为变量类型，这符合里式替换原则。经过改造以后，在实例化Driver时，想传入小汽车、卡车或者摩托车都行，只要这些具体的交通工具类实现了抽象类Vehicle中的run方法即可。 变量注入1234567891011public class Driver &#123; private Vehicle vehicle; public void setVehicle(Vehicle vehicle) &#123; this.vehicle = vehicle; &#125; public void drive() &#123; this.vehicle.run(); &#125;&#125; 定义setVehicle函数后，可以在外部设置Driver类实例的vehicle属性，这就是变量注入。 接口注入123456789101112131415public interface IVehicleChanger &#123; public abstract void changeVehicle(Vehicle vehicle);&#125;public class Driver implements IVehicleChanger&#123; private Vehicle vehicle; public void changeVehicle(Vehicle vehicle) &#123; this.vehicle = vehicle; &#125; public void drive() &#123; this.vehicle.run(); &#125;&#125; 使用接口注入时，需要将所有类的依赖抽取到一个接口，调用类需要实现该接口的注入方法。接口注入和属性注入有点相似，不过需要多定义一个接口，相对麻烦一些。 上面这些代码和解释都是为了说明控制反转和依赖注入的本质：将一个类所依赖的东西的控制权从类本身中移除，将控制权交给外部。在最初的一段代码中，Driver类要亲自实例化一个具体的交通工具，这种方式代码的可复用性和可扩展性都很差，严重违反了开闭原则。三种解耦方案其实就是让Driver类不要去管具体交通工具库类实例化的过程，只要用它就好了。 那么控制反转和依赖注入这两个词的区别是什么？其实就是驾驶交通工具和驾驶小汽车的区别，控制反转这种讲法更抽象一些，依赖注入则更具体一些：依赖注入是控制反转的一种实现方式。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(5)——代理模式","slug":"设计模式(5)——代理模式","date":"2017-12-07T16:00:00.000Z","updated":"2022-04-15T03:41:13.042Z","comments":true,"path":"2017/12/08/设计模式(5)——代理模式/","link":"","permalink":"https://nullcc.github.io/2017/12/08/设计模式(5)——代理模式/","excerpt":"本文介绍代理模式的概念和应用。","text":"本文介绍代理模式的概念和应用。 基本思想和原则为某个对象提供一个代理以控制对这个对象的访问。 将真正提供服务的类隐藏起来，并使用一个类来代理它。这个代理类在对外提供服务的同时还可以对被代理类做一些增强，比如前置处理和后置处理。 动机当有一个类不想暴露给外部，但又要对外提供服务时，为了隐藏这个类又让它提供服务，我们可以创建一个代理类，代理类对外暴露的接口和被代理类是一样的，用户可以像使用被代理类一样毫无差别地使用代理类，而实际上真正提供服务的是被代理类，这对用户是完全透明的。 实现一个静态的实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243public interface IStaff &#123; public void serve();&#125;public class CustomerServiceStaff implements IStaff &#123; private IStaff staff = null; public CustomerServiceStaff(IStaff _staff) &#123; this.staff = _staff; &#125; @Override public void serve() &#123; this.answerThePhone(); this.staff.serve(); this.recordTheEvent(); &#125; private void answerThePhone() &#123; System.out.println(\"Answer the phone...\"); &#125; private void recordTheEvent() &#123; System.out.println(\"Record the event...\"); &#125;&#125;public class Engineer implements IStaff &#123; @Override public void serve() &#123; System.out.println(\"Engineer serve...\"); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; IStaff engineer = new Engineer(); IStaff customerServiceStaff = new CustomerServiceStaff(engineer); customerServiceStaff.serve(); &#125;&#125; 输出如下： 123Answer the phone...Engineer serve...Record the event... 上面代码中，定义了一个IStaff接口，里面有一个serve方法，还定义了CustomerServiceStaff和Engineer两个实现类实现了IStaff接口。用户致电客服人员要求提供系统维护服务，但客服人员并不会自己提供这个服务，而是代理给后方的工程师。在这个体系中，工程师是被代理者，客服人员是代理者。工程师对用户来说是透明的，用户只需要和客服人员沟通就可以获得服务，而不管这个服务是谁提供的。 另外注意到在客服人员服务时，不但将任务委托给工程师，还在任务前接听电话，任务后记录事件。这实际上是对工程师职责的扩展，实现了面向切片编程(Aspect Oriented Programming, AOP)，这是一个相当强大的编程概念。这里工程师只要负责自己的工作就好了，一些外部工作都交给客服人员这个代理类来实现 静态代理工作起来当然没问题，不过如果每个被代理类都要实现一个特定的静态代理，也是挺麻烦的。我们更抽象一点，可以实现动态代理。上面实现的静态代理在编译期就知道代理谁了，所以适用性比较窄。动态代理简单来说就是在运行时才决定代理谁。 使用动态代理的例子： 12345678910111213141516171819202122232425262728293031323334353637public interface IMachine &#123; public void work();&#125;public class MachineA implements IMachine &#123; public void work() &#123; System.out.println(\"MachineA work!\"); &#125;&#125;public class MyInvocationHandler implements InvocationHandler &#123; private Object target = null; public MyInvocationHandler(Object object) &#123; this.target = object; &#125; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; return method.invoke(this.target, args); &#125;&#125;public class DynamicProxy&lt;T&gt; &#123; public static &lt;T&gt; T newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) &#123; return (T) Proxy.newProxyInstance(loader, interfaces, h); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; IMachine machine = new MachineA(); InvocationHandler handler = new MyInvocationHandler(machine); IMachine proxy = DynamicProxy.newProxyInstance(machine.getClass().getClassLoader(), machine.getClass().getInterfaces(), handler); proxy.work(); &#125;&#125; 输出如下： 1MachineA work! 动态代理比较难理解，这里解释一下。 首先创建一个IMachine接口，其中定义了一个work方法，所有具体的机器类都要实现这个方法。每个代理实例都需要一个相关联的调用处理器，当代理实例的方法被调用时，这个处理器会将方法在内部代理给被代理类的实例处理。我们创建一个类MyInvocationHandler实现java.lang.reflect.InvocationHandler接口，它有一个私有变量target，就是被代理类的实例，在构造函数中可以指定被代理类的实例进行初始化，另外还需要实现invoke方法，这个方法是InvocationHandler接口唯一要实现的方法。当在它所关联的代理类实例上调用方法时，invoke方法将被调用，其内部会将方法调用委托给被代理类的实例执行，并返回结果。 当然还需要创建一个动态代理类DynamicProxy，这个类有一个newProxyInstance方法，其内部调用了Proxy.newProxyInstance方法，该方法会返回一个代理类实例，这个代理类实例会将特定的方法调用委托给与它关联的调用处理器来处理。 优点利用代理模式可以让真正提供服务的类专注与它的逻辑，代理类负责一些琐碎的事情。这种模式的扩展性也很好，代理类可以使用AOP的方式在真正的任务前后做一些处理，比较典型的方式是执行任务前做一些准备，执行任务后做一些清理并记录日志等。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(6)——建造者模式","slug":"设计模式(6)——建造者模式","date":"2017-12-07T16:00:00.000Z","updated":"2022-04-15T03:41:13.042Z","comments":true,"path":"2017/12/08/设计模式(6)——建造者模式/","link":"","permalink":"https://nullcc.github.io/2017/12/08/设计模式(6)——建造者模式/","excerpt":"本文介绍建造者模式的概念和应用。","text":"本文介绍建造者模式的概念和应用。 基本思想和原则将一个复杂对象的构建和它的表示分离，使同样的构建过程可以创建不同的表示。 使用Builder类封装类实例的创建过程，客户代码使用各个Builder来构建对象而不是直接使用new创建类实例。 动机当一个类中某个方法中的内部调用顺序不同，会产生不同的结果时，可以考虑使用建造者模式。将产生特定调用顺序的类实例用Builder进行封装，供客户代码使用可以简化整个对象创建过程。当需要产生新的调用顺序地类实例时，只需要创建对应的Builder即可。 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126public abstract class Robot &#123; private ArrayList&lt;String&gt; actionSequence = new ArrayList&lt;String&gt;(); protected abstract void speak(); protected abstract void walk(); protected abstract void think(); public final void demo() &#123; for (int i = 0; i &lt; this.actionSequence.size(); i++) &#123; String action = this.actionSequence.get(i); if (action.equalsIgnoreCase(\"speak\")) &#123; this.speak(); &#125; else if (action.equalsIgnoreCase(\"walk\")) &#123; this.walk(); &#125; else if (action.equalsIgnoreCase(\"think\")) &#123; this.think(); &#125; &#125; &#125; public final void setActionSequence(ArrayList actionSequence) &#123; this.actionSequence = actionSequence; &#125;&#125;public class RobotA extends Robot &#123; @Override protected void speak() &#123; System.out.println(\"RobotA speak...\"); &#125; @Override protected void walk() &#123; System.out.println(\"RobotA walk...\"); &#125; @Override protected void think() &#123; System.out.println(\"RobotA think...\"); &#125;&#125;public class RobotB extends Robot &#123; @Override protected void speak() &#123; System.out.println(\"RobotB speak...\"); &#125; @Override protected void walk() &#123; System.out.println(\"RobotB walk...\"); &#125; @Override protected void think() &#123; System.out.println(\"RobotB think...\"); &#125;&#125;public abstract class RobotBuilder &#123; public abstract void setActionSequence(ArrayList&lt;String&gt; actionSequence); public abstract Robot getRobot();&#125;public class RobotABuilder extends RobotBuilder &#123; private RobotA robotA = new RobotA(); public void setActionSequence(ArrayList&lt;String&gt; actionSequence) &#123; this.robotA.setActionSequence(actionSequence); &#125; public Robot getRobot() &#123; return this.robotA; &#125;&#125;public class RobotBBuilder extends RobotBuilder &#123; private RobotB robotB = new RobotB(); public void setActionSequence(ArrayList&lt;String&gt; actionSequence) &#123; this.robotB.setActionSequence(actionSequence); &#125; public Robot getRobot() &#123; return this.robotB; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; ArrayList&lt;String&gt; actionSequence1 = new ArrayList&lt;String&gt;(); actionSequence1.add(\"speak\"); actionSequence1.add(\"think\"); actionSequence1.add(\"walk\"); ArrayList&lt;String&gt; actionSequence2 = new ArrayList&lt;String&gt;(); actionSequence1.add(\"think\"); actionSequence1.add(\"walk\"); actionSequence1.add(\"speak\"); RobotBuilder robotABuilder = new RobotABuilder(); RobotBuilder robotBBuilder = new RobotBBuilder(); robotABuilder.setActionSequence(actionSequence1); robotABuilder.getRobot().demo(); System.out.println(\"\\n\"); robotABuilder.setActionSequence(actionSequence2); robotABuilder.getRobot().demo(); System.out.println(\"\\n\"); robotBBuilder.setActionSequence(actionSequence1); robotBBuilder.getRobot().demo(); System.out.println(\"\\n\"); robotBBuilder.setActionSequence(actionSequence2); robotBBuilder.getRobot().demo(); &#125;&#125; 输出： 123456789101112131415RobotA speak...RobotA think...RobotA walk...RobotA think...RobotA walk...RobotA speak...RobotB speak...RobotB think...RobotB walk...RobotB think...RobotB walk...RobotB speak... 上面的代码中，机器人可以有说话、走路和思考三种动作，demo方法用来执行机器人演示，我们想随意组合这些动作为一个动作序列来演示，。因此动作序列是可配置的。由于动作序列可以多种多样，使用建造者模式可以将各个动作序列封装在各个Builder中，客户代码直接调用Builder即可创建具有特定动作序列的机器人。 优点建造者模式对一个类产生具体实例做了相应的封装，使客户代码不需要了解具体类的内部细节，可以直接使用相应的Builder来创建实例。各个Builder之间具有很好的隔离性，都可以独立做改变而不会互相影响。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(7)——策略模式","slug":"设计模式(7)——策略模式","date":"2017-12-07T16:00:00.000Z","updated":"2022-04-15T03:41:13.042Z","comments":true,"path":"2017/12/08/设计模式(7)——策略模式/","link":"","permalink":"https://nullcc.github.io/2017/12/08/设计模式(7)——策略模式/","excerpt":"本文介绍策略模式的概念和应用。","text":"本文介绍策略模式的概念和应用。 基本思想和原则定义一组算法，将每个算法都封装起来，使它们之间可以互换。 动机当一个操作中可以使用多种算法相互替换时，可以挨个实现各个算法，然后用一个上下文封装起来，当需要执行操作时，选择一个策略传入上下文即可。 实现123456789101112131415161718192021222324252627282930313233343536373839public interface IStrategy &#123; public abstract void work();&#125;public class Strategy1 implements IStrategy &#123; public void work() &#123; System.out.println(\"Strategy1 work!\"); &#125;&#125;public class Strategy2 implements IStrategy &#123; public void work() &#123; System.out.println(\"Strategy2 work!\"); &#125;&#125;public class Context &#123; private IStrategy strategy = null; public void setStrategy(IStrategy strategy) &#123; this.strategy = strategy; &#125; public void work() &#123; this.strategy.work(); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; IStrategy strategy1 = new Strategy1(); IStrategy strategy2 = new Strategy2(); Context context = new Context(); context.setStrategy(strategy1); context.work(); context.setStrategy(strategy2); context.work(); &#125;&#125; 输出结果： 12Strategy1 work!Strategy2 work! 优点策略模式中各种算法可以随意切换，想要替换一个算法非常简单，当需要增加新的算法时，只要创建一个新类实现相应接口即可直接使用，同时也避免了在上下文中使用条件判断语句来选择具体算法。 缺点如果有大量算法，策略模式中可能会有非常多的具体算法类，并且上层模块需要了解每个策略模式的具体算法。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(4)——模板方法模式","slug":"设计模式(4)——模板方法模式","date":"2017-12-06T16:00:00.000Z","updated":"2022-04-15T03:41:13.041Z","comments":true,"path":"2017/12/07/设计模式(4)——模板方法模式/","link":"","permalink":"https://nullcc.github.io/2017/12/07/设计模式(4)——模板方法模式/","excerpt":"本文介绍模板方法模式的概念和应用。","text":"本文介绍模板方法模式的概念和应用。 基本思想和原则定义一个操作的算法框架，将框架内的步骤延迟到子类实现，使子类可以在不改变算法框架的情况下重写算法的各个步骤。 如果某个抽象类的方法具有一个固定的流程，流程中有多个步骤，但各个步骤的具体实现不同，我们就可以使用模板方法模式在抽象类中将这些步骤封装在一个方法内。子类只需要实现各个步骤，不需要修改这个方法。由于模板方法模式在父类中定义了算法框架，只是其中各个步骤的具体实现延迟到了子类，因此是一个模板，可以由子类套用。 动机如果有一个类体系，所有子类都实现了同一个方法，而这个方法可以抽象为几个固定的步骤，在当前这种设计下，每个子类的代码在一定程度上是有重复的。可以使用模板方法模式重构。 实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public abstract class Computer &#123; protected abstract void boot(); protected abstract void runningProcesses(); protected abstract void powerOff(); public final void run() &#123; this.boot(); this.runningProcesses(); this.powerOff(); &#125;&#125;public class AppleComputer extends Computer &#123; protected void boot() &#123; System.out.println(\"AppleComputer boot...\"); &#125; protected void runningProcesses() &#123; System.out.println(\"AppleComputer running processes...\"); &#125; protected void powerOff() &#123; System.out.println(\"AppleComputer power off...\"); &#125;&#125;public class DellComputer extends Computer &#123; protected void boot() &#123; System.out.println(\"DellComputer boot...\"); &#125; protected void runningProcesses() &#123; System.out.println(\"DellComputer running processes...\"); &#125; protected void powerOff() &#123; System.out.println(\"DellComputer power off...\"); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; AppleComputer appleComputer = new AppleComputer(); DellComputer dellComputer = new DellComputer(); appleComputer.run(); System.out.println(\"\\n\"); dellComputer.run(); &#125;&#125; 运行结果： 1234567AppleComputer boot...AppleComputer running processes...AppleComputer power off...DellComputer boot...DellComputer running processes...DellComputer power off... 优点模板方法模式将一个算法框架抽象出来，在父类实现，子类只需要实现算法中的相应步骤。我们将公共代码提取到父类，当实现子类时就不会存在重复代码。在增加子类时，不需要修改已有的任何类，符合开闭原则，其代码的复用性和可维护性都很好。 需要注意的是，模板方法在Java中经常使用final修饰以防止子类覆写，各个步骤方法一般用protected修饰，这样不会将内部步骤暴露给外部，符合迪米特法则。 缺点严格来说模板方法模式没有什么很明显的缺点，唯一需要注意的是识别使用模板方法模式的时机，因为在超类中定义模板方法后，不应在子类覆写，如果一些子类有非常特殊的流程，比如在固定流程中间穿插某些步骤，则模板方法模式可能不再适用于这个类体系。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(0)——设计模式和设计模式原则","slug":"设计模式(0)——设计模式和设计模式原则","date":"2017-12-04T16:00:00.000Z","updated":"2022-04-15T03:41:13.038Z","comments":true,"path":"2017/12/05/设计模式(0)——设计模式和设计模式原则/","link":"","permalink":"https://nullcc.github.io/2017/12/05/设计模式(0)——设计模式和设计模式原则/","excerpt":"本文介绍了什么是设计模式、设计模式列表和设计模式六大原则。","text":"本文介绍了什么是设计模式、设计模式列表和设计模式六大原则。 什么是设计模式在《设计模式：可复用面向对象软件的基础》（以下都简称《设计模式》）一书中提到： Christopher Alexander说过：“每一个模式描述了一个在我们周围不断发生的问题，以及该问题的解决方案的核心。这样，你就能一次又一次地使用该方案而不必做重复劳动。” Christopher Alexander是一位建筑师，这是在他的著作中对建筑模式的总结。对应到软件工程，在长期的实践和积累中，人们总结了很多场景下提高代码的可复用性、可扩展性、可维护性和降低模块间耦合的经验，提炼成一个个框架供以后使用。 设计模式列表《设计模式》一书介绍了23种设计模式： 模式英文名 模式中文名 Abstract Factory 抽象工厂模式 Adapter 适配器模式 Bridge 桥接模式 Builder 建造者模式 Chain of Responsibility 责任链模式 Command 命令模式 Composite 组合模式 Decorator 装饰器模式 Facade 外观模式 Factory Method 工厂方法模式 Flyweight 享元模式 Interpreter 解释器模式 Iterator 迭代器模式 Mediator 中介者模式 Memento 备忘录模式 Observer 观察者模式 Prototype 原型模式 Proxy 代理模式 Singleton 单例模式 State 状态模式 Strategy 策略模式 Template Method 模板方法模式 Visitor 访问者模式 模式依据其目的可以分为创建型、结构型和行为型三种，下面是《设计模式》一书中对23种设计模式的分类： 设计模式之前的关系： 在后续文章中，将使用Java演示和分析这23种设计模式。 设计模式原则提到设计模式，不可避免地要谈谈设计模式的六大原则： 单一职责原则 里式替换原则 依赖倒置原则 接口隔离原则 迪米特法则 开闭原则 单一职责原则(Single Responsibility Principle, SRP)定义：不要存在多于一个导致类变更的原因。 其实不光是类，对于接口、类和方法都要遵循单一职责原则。单一职责原则从字面上很好理解，一个接口、类和方法只做一件事。 遵循单一职责原则有如下好处： 每个接口、类和方法只承担一项职责，意义更清晰。 使代码可读性、可维护性和可扩展性更佳。 当需求有变化需要修改代码时，只需要修改相应功能的代码，不会对其他功能的代码造成影响。 里式替换原则(Liskov Substitution Principle, LSP)定义：如果对每一个类型为S的对象o1，都有类型为T的对象o2，使得以T定义的所有程序P在所有的对象o1代换o2时，程序P的行为没有变化，那么类型S是类型T的子类型。 上面的定义是严格的定义，比较通俗的定义是：所有引用基类（父类）的地方必须能透明地使用其子类的对象。 里式替换原则包含如下几个含义： 子类可以实现父类的抽象方法，但不能覆盖父类已经实现的方法（即非抽象方法）。 子类可以增加自己特有的方法。 当子类重载父类的方法时，方法的前置条件（入参）要比父类更宽松。 当子类实现父类的抽象方法时，方法的后置条件（出参）要比父类更严格。 依赖倒置原则(Dependence Inversion Principle, DIP)定义： 高层模块不应该依赖低层模块，两者都应该依赖其抽象。 抽象不应该依赖细节。 细节应该依赖抽象。 设想一个情景，工厂生产零件，其中有工厂类Factory、螺丝类Screw、钉子类Nail等。如果Factory直接依赖于某个零件类，当需要生产新的零件时，势必要修改Factory，这样一来Factory和各个零件类的耦合就非常大。为了解决这个问题，我们可以引入一个接口IProduce，所有零件类都实现这个接口，Factory直接使用IProduce接口来进行生产，之后如果有新的零件类加入进来，只需要实现IProduce接口，Factory不需要做任何修改。 这个例子中，Factory类是高层模块，具体的零件类是低层模块，引入的IProduce接口就是抽象，其中，Factory使用IProduce接口生产零件，具体零件类实现了IProduce接口，这满足定义中的第一点。同时IProduce接口不依赖于具体零件类，它是一个独立的存在，这满足定义中的第二点。各个具体的零件类实现依赖于IProduce接口，这满足定义中的第三点。 依赖导致原则的好处非常明显，它降低了模块间的耦合性，提高系统的稳定性、灵活性、可维护性和可扩展性，降低了修改代码的风险和成本。 接口隔离原则(Interface Segregation Principle, ISP)定义： 客户端不应该依赖它不需要的接口。 一个类对另一个类的依赖应该建立在最小的接口上。 假设有一个接口I，定义了一些方法，类A通过接口I依赖于类C，类B通过接口I依赖于类，但类Ahead类B都只依赖接口I的其中一部分方法。也就是说，接口I不是类A和类B的最小接口。这种情况下，类C和类D为了实现接口，就要实现那些类A和类B不需要的方法，如果后续还对接口I添加方法或还有其他类依赖于接口I，就会造成接口I过于臃肿，这是不好的设计。 解决方案是遵循接口隔离原则，该原则实际上是指导我们面向接口编程。应该分析类A和类B对接口的需求，将接口I拆分成多个小接口，客户类和实现类根据拆分后的接口做调整。 接口隔离原则要注意以下几点： 拆分和细化接口，不要让接口过于臃肿，另外也要注意接口细化的粒度，拆分地太细会导致复杂度升高，要适度拆分。 接口即契约，对一个模块制定接口时，只暴露出需要的功能，接口中不应有多余的功能存在。 设计模块时，要尽量提高模块的内聚性，尽量减少模块对外的依赖关系。 迪米特法则(Least Knowledge Principle, LKP, 最小知道原则)定义：一个对象应该对其他对象保持最少的了解。 迪米特法则又叫作最少知道原则。软件工程提倡模块化，一个模块要低耦合，高内聚。当两个类彼此之间互相了解得越多，它们的耦合就越大，当其中一个类变化时，另一个类受的影响就越大。迪米特法则说的就是要尽量降低类与类之间的耦合。 类和类之间完全没有耦合是不可能的，如果完全不耦合，程序就没办法工作了。但是必须要保证是适当的耦合度，。个类互相耦合时，最好是将这个耦合度控制在类级别或方法级别，应避免互相去了解类的实现细节。 开闭原则(Open Closed Principle, OCP)定义：一个软件实体如类、模块和函数应该对扩展开放，对修改关闭。 开闭原则告诉我们当软件的需求有变化时，应尽量通过扩展软件的功能来响应变化，而不是通过修改已有的代码来响应变化。实际上开闭原则是其他五个原则的归纳汇总，其他五个原则都在教我们怎么做，其中都在阐述开闭原则这个核心思想。 开闭原则给我们的启示是：用抽象构建功能框架，用实现扩展功能细节。这是构建高可维护性、高可扩展性、高灵活性和高稳定性系统的最佳方式。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(1)——单例模式","slug":"设计模式(1)——单例模式","date":"2017-12-04T16:00:00.000Z","updated":"2022-04-15T03:41:13.038Z","comments":true,"path":"2017/12/05/设计模式(1)——单例模式/","link":"","permalink":"https://nullcc.github.io/2017/12/05/设计模式(1)——单例模式/","excerpt":"本文介绍单例模式的概念和应用。","text":"本文介绍单例模式的概念和应用。 基本思想和原则保证一个类仅有一个实例，自行实例化并提供一个访问它的全局访问点。 单例模式有以下三个要素： 保证类实例的唯一性。 提供一个获取类唯一实例的全局访问点。 类必须自行创建这个唯一的实例，不能由用户手动创建。 动机在一些情况下我们希望某个类在系统中有且仅有一个实例，常见的例子有全局id生成器、任务管理器、全局计时器、配置对象等。如果这些类的对象有多个，可能造成数据不一致的情况，因此需要一种通用的模式来保证这些类在系统中有且仅有一个实例。 实现单例模式的实现可以分为懒汉式和饿汉式两种，懒汉式指的是等到需要的时候再创建对象，饿汉式则是在类被加载时就创建一个对象。因此懒汉式存在线程安全的问题，而饿汉式没有线程安全问题。 我们先来看懒汉式： 123456789101112public class Singleton &#123; private Singleton() &#123;&#125; private static Singleton _instcance = null; public static Singleton getInstance() &#123; if (_instcance == null) &#123; _instcance = new Singleton(); &#125; return _instcance; &#125;&#125; 上面的方式不是线程安全的，如果想要线程安全，可以使用同步： 123456789101112public class Singleton &#123; private Singleton() &#123;&#125; private static Singleton _instcance = null; public static synchronized Singleton getInstance() &#123; if (_instcance == null) &#123; _instcance = new Singleton(); &#125; return _instcance; &#125;&#125; 这种方式将整个getInstance方法同步了，因此每次调用的时候都要同步，效率比较低。还可以只在第一次的时候同步： 1234567891011121314151617public class Singleton &#123; private Singleton() &#123;&#125; private static Singleton _instcance = null; public static Singleton getInstance() &#123; if (_instcance == null) &#123; synchronized(Singleton.class)&#123; if (_instcance == null) &#123; _instcance = new Singleton(); &#125; &#125; &#125; return _instcance; &#125;&#125; 上面这种方式只在第一次的时候同步，当已经存在实例时直接返回。 还可以使用静态内部类实现单例： 1234567891011public class Singleton &#123; private static class InnerClass&#123; private static final Singleton _instance = new Singleton(); &#125; private Singleton() &#123;&#125; public static Singleton getInstance() &#123; return InnerClass._instance; &#125;&#125; 静态内部类也不需要同步，因为在类加载的时候就创建了实例。 主函数如下： 12345678public class Test &#123; public static void main(String[] args) &#123; Singleton obj1 = Singleton.getInstance(); Singleton obj2 = Singleton.getInstance(); System.out.println(obj1.hashCode()); System.out.println(obj2.hashCode()); &#125;&#125; 打印的结果是obj1和obj2的hashCode相同（每次运行得到的hashCode不同）： 1217612913201761291320","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(2)——工厂方法模式","slug":"设计模式(2)——工厂方法模式","date":"2017-12-04T16:00:00.000Z","updated":"2022-04-15T03:41:13.040Z","comments":true,"path":"2017/12/05/设计模式(2)——工厂方法模式/","link":"","permalink":"https://nullcc.github.io/2017/12/05/设计模式(2)——工厂方法模式/","excerpt":"本文介绍工厂方法模式的概念和应用。","text":"本文介绍工厂方法模式的概念和应用。 基本思想和原则定义一个用于创建对象的接口，让子类决定将哪一个类实例化。工厂方法模式使一个类的实例化延迟到其子类。 客户代码不要依赖具体类，而是要依赖抽象。 动机假设某个类有两个个子类A、B，当需要客户代码需要使用这个类体系时，最简单的方式是根据需要的功能选择相应的子类来创建对象。但是这么做有一个问题，当子类很多时或子类的初始化比较复杂时，客户代码会严重地依赖子类的具体实现。比如客户代码可能是下面这样的（为了简单起见，代码写在了一起）： 1234567891011121314151617181920212223242526public abstract class GameConsole &#123; public void play() &#123;&#125;&#125;public class PlayStation extends GameConsole &#123; @Override public void play()&#123; System.out.println(\"PlayStation play!\"); &#125;&#125;public class XBox extends GameConsole &#123; @Override public void play()&#123; System.out.println(\"XBox play!\"); &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; GameConsole playstation = new PlayStation(); playstation.play(); GameConsole xbox = new XBox(); xbox.play(); &#125;&#125; 这里客户代码要直接和具体的游戏机类发生耦合，创建一个游戏机实例都要关注到具体的游戏机类。如果之后游戏机种类增加，客户代码还需要自行做一些判断。 使用工厂方法可以缓解这种问题，下面是工厂方法的实现。 实现1234567891011121314151617181920212223242526272829303132333435363738public interface IGameConsole &#123; public void play();&#125;public class PlayStation implements IGameConsole &#123; @Override public void play() &#123; System.out.println(\"PlayStation play!\"); &#125;&#125;public class XBox implements IGameConsole &#123; @Override public void play() &#123; System.out.println(\"XBox play!\"); &#125;&#125;public class GameConsoleFactory &#123; public static IGameConsole getGameConsole(String name) &#123; if (name.equalsIgnoreCase(\"playstation\")) &#123; return new PlayStation(); &#125; else if (name.equalsIgnoreCase(\"xbox\")) &#123; return new XBox(); &#125; else &#123; return null; &#125; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; IGameConsole playstation = GameConsoleFactory.getGameConsole(\"playstation\"); playstation.play(); IGameConsole xbox = GameConsoleFactory.getGameConsole(\"xbox\"); xbox.play(); &#125;&#125; 运行后输出： 12PlayStation play!XBox play! 在工厂方法中，我们将游戏机这个概念抽象成一个接口IGameConsole，其中有一个方法play。具体的游戏机类PlayStation和XBox通过实现IGameConsole接口实现了自己的逻辑。另外需要建立一个GameConsoleFactory工厂类，这个工厂类的作用就是为客户代码提供一种创建具体类实例的入口，注意GameConsoleFactory.getGameConsole是一个静态方法。此时客户代码不需要了解具体的游戏机实现类，只要知道IGameConsole接口为我们提供了什么方法就可以了。之后如果增加新的游戏机类，只需要修改GameConsoleFactory.getGameConsole，客户代码不用改动。 优点缺点","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"设计模式(3)——抽象工厂模式","slug":"设计模式(3)——抽象工厂模式","date":"2017-12-04T16:00:00.000Z","updated":"2022-04-15T03:41:13.041Z","comments":true,"path":"2017/12/05/设计模式(3)——抽象工厂模式/","link":"","permalink":"https://nullcc.github.io/2017/12/05/设计模式(3)——抽象工厂模式/","excerpt":"本文介绍抽象工厂模式的概念和应用。","text":"本文介绍抽象工厂模式的概念和应用。 基本思想和原则为创建一组相关或相互依赖的对象提供一组接口，而且无须指定它们的具体类。 高层模块不应直接依赖低层模块，应该依赖其抽象，工厂就是这个抽象。 动机让我们考虑产品族和产品类型这两个概念。举个产品族的例子，苹果和戴尔两家公司都生产计算机，计算机是一种统称，是一个抽象概念，这是产品族。其中计算机又可以分为服务器、台式机、笔记本等，这是计算机的类型，一个具体概念，这是产品类型。因此产品族和产品类型的对比如下表： 名称 抽象/具体 概念方向 产品族 抽象 横向 产品类型 具体 纵向 实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182public abstract class AppleComputer &#123; public abstract void run();&#125;public class AppleDesktop extends AppleComputer &#123; @Override public void run() &#123; System.out.println(\"AppleDesktop run!\"); &#125;&#125;public class AppleNotebook extends AppleComputer &#123; @Override public void run() &#123; System.out.println(\"AppleNotebook run!\"); &#125;&#125;public abstract class DellComputer &#123; public abstract void run();&#125;public class DellDesktop extends DellComputer &#123; @Override public void run() &#123; System.out.println(\"DellDeskComputer run!\"); &#125;&#125;public class DellNotebook extends DellComputer &#123; @Override public void run() &#123; System.out.println(\"DellNotebook run!\"); &#125;&#125;public abstract class ComputerFactory &#123; public abstract AppleComputer createAppleComputer(); public abstract DellComputer createDellComputer();&#125;public class DesktopFactory extends ComputerFactory &#123; @Override public AppleComputer createAppleComputer() &#123; return new AppleDesktop(); &#125; @Override public DellComputer createDellComputer() &#123; return new DellDesktop(); &#125;&#125;public class NotebookFactory extends ComputerFactory &#123; @Override public AppleComputer createAppleComputer() &#123; return new AppleNotebook(); &#125; @Override public DellComputer createDellComputer() &#123; return new DellNotebook(); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; ComputerFactory desktopFactory = new DesktopFactory(); ComputerFactory notebookFactory = new NotebookFactory(); AppleComputer appleDesktop = desktopFactory.createAppleComputer(); AppleComputer appleNotebook = notebookFactory.createAppleComputer(); DellComputer dellDesktop = desktopFactory.createDellComputer(); DellComputer dellNotebook = notebookFactory.createDellComputer(); appleDesktop.run(); appleNotebook.run(); dellDesktop.run(); dellNotebook.run(); &#125;&#125; 运行后输出： 1234AppleDesktop run!AppleNotebook run!DellDeskComputer run!DellNotebook run! 优点封装性好，将高层模块和具体实现类解耦，高层模块不需要关心具体实现类的细节，只需要和工厂打交道，由工厂去创建对象。 缺点抽象工厂模式很难对产品族进行扩展，来看看上面的例子如果要增加一个惠普品牌的计算机需要做哪些新增或修改： 新增HPDesktop类。 新增HPNotebook类。 修改ComputerFactory，在其中添加createHPComputer这个方法。 修改DesktopFactory，实现createHPComputer这个方法。 修改NotebookFactory，实现createHPComputer这个方法。 可以看到，2个新增，3个修改，工作量不仅很大，而且还要改动原来已经可以工作的类。也就是说，想要扩展产品族，就需要对原来的契约双方都做修改，很明显违反了开闭原则。 相反，使用抽象工厂模式对产品类型进行扩展要容易得多，上面的例子如果要增加一个服务器类型的计算机需要的新增或修改： 新增AppleServer类。 新增DellServer类。 新增ServerFactory类。 可以发现使用抽象工厂模式扩展产品类型都是新增，不需要修改原来的代码，符合开闭原则。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://nullcc.github.io/tags/设计模式/"}]},{"title":"垃圾回收(GC)算法介绍(5)——分代垃圾回收","slug":"垃圾回收(GC)算法介绍(5)——分代垃圾回收","date":"2017-12-03T16:00:00.000Z","updated":"2022-04-15T03:41:13.032Z","comments":true,"path":"2017/12/04/垃圾回收(GC)算法介绍(5)——分代垃圾回收/","link":"","permalink":"https://nullcc.github.io/2017/12/04/垃圾回收(GC)算法介绍(5)——分代垃圾回收/","excerpt":"本文介绍分代垃圾回收策略的基础。","text":"本文介绍分代垃圾回收策略的基础。 分代垃圾回收概述在之前的垃圾回收算法介绍中，我们提到大部分对象创建后没多久就“死了”，也就是被垃圾回收掉了。因此很容易联想到可以将堆上的对象分为新生代对象和老年代对象，相应的区域就是新生代空间和老年代空间，针对不同区域执行不同的GC算法。可以说分代垃圾回收有一种分而治之的意思在里面。 在分代垃圾回收策略中，我们在每个对象上维护一个“年龄”域，在新生代区中每进行一次GC，那些没有被回收的对象的年龄就自增1。当对象的年龄达到一个上限后，将其移动到老年代空间。老年代空间里都是一些经历过多次GC而没有被回收的对象。 需要注意的一点是，分代垃圾回收并不是一种具体的GC算法，而是一种策略，在新生代空间和老年代空间中执行的GC算法还是之前讨论的那三种基本算法。 堆的结构一个叫David Ungar的人提出了一种分代垃圾回收策略，这种策略在新生代空间中使用GC复制算法，在老年代空间使用标记-清除算法。 下面是对的结构示意图： 解释一下上图中几个空间的含义。其中生成空间就是创建新对象所在的空间。当生成空间满了的时候，会对新生代进行GC。有两个幸存空间，对应于GC复制算法的from空间和to空间，每次只利用其中的一个。当进行新生代GC时，会将生成空间和幸存from空间的活跃对象都复制到幸存to空间中。只有在一定次数的新生代GC过程中幸存下来的对象才会被复制到老年代空间中去。 记录集上图中还有一个$rs_set，这是做什么的呢？现在考虑一个问题，当我们在新生代中执行GC时，要如何找到活跃对象？有两种方式，一是从根出发，寻找新生代中的根引用对象和其子对象，二是从老年代中的对象出发，查找那些位于新生代的对象。因为完全可能存在这样的新生代对象：既不是根直接引用的新生代对象，也不是根直接引用的新生代对象的子对象，这种对象是被老年代对象所引用的对象。如果想找到这些对象，最直接的办法是遍历一遍老年代空间，递归地判断它们的子对象是否是新生代对象。但是这样一来，GC新生代空间时还要去查看一遍老年代对象，效率很低。$rs_set就是用来保存那些引用了新生代对象的老年代对象的。 $rs_set一般可以是一个数组，其中保存了引用了新生代对象的老年代对象的指针。在新生代GC时只需要遍历一次$rs_set，找出那些新生代对象，对其做处理就可以了。 写入屏障在更新一个对象的指针时，要判断对象是否是老年代对象，且指针指向的对象是否是新生代的对象，我们需要一个叫做“写入屏障”的东西。写入屏障会判断这种情况，并在条件为真时将该老年代对象加入$rs_set中，方便下次新生代GC时使用。下面是写入屏障的伪代码： 12345678write_barrier(obj, field, new_obj)&#123; if(&amp;obj &gt;= $old_start &amp;&amp; &amp;new_obj &lt; $old_start &amp;&amp; obj.rememberd == FALSE)&#123; $rs[$rs_index] = obj $rs_index++ obj.rememberd = TRUE &#125; obj.field = new_obj&#125; 对象的结构在Ungar的分代垃圾回收中，对象头需要包含三个信息： age: 对象的年龄 forwarded：已经复制完成的标志 remembered：已经向记录集记录完毕的标志 这里age是记录新生代对象经历过的GC次数，当达到一定次数后就要被移动到老年代空间。forwarded用来标记是否已经完成复制，防止重复复制对象。remembered用来标记是否已经添加到记录集中，防止重复添加。 优点和缺点分代垃圾回收使用了分而治之的方式，对存活时间不同的对象采用不同的处理方案，不需要每次GC都遍历整个堆空间，吞吐率得到了提高。 它的缺点是写入屏障会对指针更新操作带来额外的负担。另外如果一个程序中大部分对象存活时间都很长的话，会增加新生代GC的压力，并且导致老年代GC频繁地运行。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"垃圾回收","slug":"垃圾回收","permalink":"https://nullcc.github.io/tags/垃圾回收/"}]},{"title":"谈谈社会工程学","slug":"谈谈社会工程学","date":"2017-12-03T16:00:00.000Z","updated":"2022-04-15T03:41:13.043Z","comments":true,"path":"2017/12/04/谈谈社会工程学/","link":"","permalink":"https://nullcc.github.io/2017/12/04/谈谈社会工程学/","excerpt":"社会工程学简称社工，之后都会以社工来称呼。本文只分析网络社工的一些技巧，不会涉及现实生活中对人的直接社工。","text":"社会工程学简称社工，之后都会以社工来称呼。本文只分析网络社工的一些技巧，不会涉及现实生活中对人的直接社工。 社工是一个范围非常广的领域，包括但不局限于目标料收集、推理逻辑、联想、暴力猜解、诱导、欺骗、渗透、反向思维、踩点、密码库匹配等一系列手段。社工在社会上可以说是非常实用的一项技巧，但注意不要用在不正当的领域，不能触犯法律。 缘起为什么会专门写一篇关于社工的文章呢，主要原因是我本人曾在国内某网络安全公司担任web安全工程师一职，不可避免地接触到很多社工领域的东西。本文主要介绍的是网络社工的一些技巧和实际应用。 网络社工网络给社工带了很多便利性，利用网络，时常可以通过社工手段了解一个人如下信息： 姓名 性别 出生日期 籍贯 学历 习惯 兴趣爱好 职业 社交网络 性格 网络ID 当然能挖掘的远远不止这些信息，如果运气好，甚至能将目标对象的各种密码找出来，之后会看到一个这方面的例子。 对于一个目标对象，一般情况下我们能够在一开始知晓其一部分信息，比如一个人的姓名和性别，网络ID等。从这些已知信息出发，去寻找目标对象的其他信息。 网络社工过程中，可以充分利用： 各种搜索引擎（Google、百度、搜搜、360搜索、Bing、Duckduckgo等） 各种社交网络（微博、人人网、豆瓣、博客、百度贴吧、各种论坛等） 对于搜索引擎，需要说明的是部分搜索引擎提供了高级搜索模式，可以使用一些特定语法更快速更精确地进行搜索，典型的比如Google Hack。各大搜索引擎最好都去尝试，增加找到的几率。 对于用来搜索的关键字也要注意，一般可以用于精确搜索的关键字类型： 网络ID（QQ号、微信号、各种社交网络的昵称、ID等） 邮箱 姓名 所在城市 所在学校 所在工作单位 还可以尝试组合各种关键字进行搜索。 对于一些社交网络，可以充分利用其搜索功能。比如人人网和微博。在人人网还比较热门的时期（早期还叫校内网），基本上只要是学生都会去注册，玩的人很多，活跃度也相当高。如果知道姓名，可以尝试去人人网搜索。如果不知道姓名，只知道学校和专业，也可以使用高级搜索功能搜索。任意组合一些已知信息去搜索有时也是必要的。 当知道其网络ID或者邮箱一类的信息时，除了搜索引擎以外，还可以尝试使用Google Hack一类的方式，挖掘其在社交网络中的留言互动信息。 不要忘记微博这个信息宝库，使用其搜索功能搜索姓名、网络ID等信息往往也有惊喜。 比较高阶的网络社工手段还会用到社工库，社工库基本上就是一些网站泄漏出来的账号密码的集合。比较有名是CSDN 600+W条明文密码泄漏，在黑产中社工库很常见，地下黑市甚至有明码标价交易。不过这类信息一般人是拿不到了，专业团队手上才有。可以利用网络ID等信息到社工库里面碰碰运气，说不定有意外收获。 实际案例这里分享一个之前工作中成功进行社工的案例。 有客户委托调查某个支持台独的民间组织的官方邮箱，想获取其往来邮件进行调查。这个难度其实很高，邮箱是hotmail的，那时候hotmail基本不存在XSS的漏洞，或者是挖掘起来很困难，这也就意味着想要直接套取对方密码基本不可能。当时我的做法是尝试重置邮箱密码，但是问题又来了，重置邮箱密码需要安全邮箱重置邮件或者重置的密码问题。前者我是没有的，只能试试重置密码问题了，不过有尝试次数，貌似是一天三次。问题只有一个：我最喜欢的一首歌是什么？这就很难了，歌曲成千上万，我怎么能猜对，就算有个曲库，我一个个歌名去尝试，一天最多三个，那要等到什么时候。 当然不能这么善罢甘休，我先去调查这个组织的具体情况，获知这个组织是的主要成员是台湾一些中年妇女，就是各种大妈。这对猜密码有什么帮助呢？台湾的一群中年妇女能喜欢什么歌曲呢？首先这群人年轻的时候大概是80年-90年代，当时20-30岁，那个年代有什么歌星比较出名？我第一个想到的人是邓丽君，就你了。邓丽君有名的歌曲很多啊，找了几个来试，第三次居然成功了，歌名就是不说了，这已经不重要了。于是我重置了对方的邮箱密码，使用工具将邮件全部收取下来。 这里案例中，解决的路线是：邮箱-&gt;重置密码问题-&gt;调查组织人员特点-&gt;猜想对方喜好-&gt;猜想问题答案。 这里面有运气成分，但更多的还是解决思路。 总结网络社工的目的是搜集目标资料，获取对方详细的资料，更进一步地就是和人的直接沟通，这需要更多的技巧，复杂性也更高。","categories":[{"name":"网络安全","slug":"网络安全","permalink":"https://nullcc.github.io/categories/网络安全/"}],"tags":[{"name":"社会工程学","slug":"社会工程学","permalink":"https://nullcc.github.io/tags/社会工程学/"}]},{"title":"数据库的三星索引","slug":"数据库的三星索引","date":"2017-12-01T16:00:00.000Z","updated":"2022-04-15T03:41:13.034Z","comments":true,"path":"2017/12/02/数据库的三星索引/","link":"","permalink":"https://nullcc.github.io/2017/12/02/数据库的三星索引/","excerpt":"摘抄自《数据库索引设计与优化》p46： 第一颗星：如果与一个查询相关的索引行是相邻的，或者至少相距足够近的化，那这个索引就可以被标记上第一颗星。这最小化了必须扫描的索引片的宽度。","text":"摘抄自《数据库索引设计与优化》p46： 第一颗星：如果与一个查询相关的索引行是相邻的，或者至少相距足够近的化，那这个索引就可以被标记上第一颗星。这最小化了必须扫描的索引片的宽度。 第二颗星：如果索引行的顺序于查询语句的需求一致，则索引可以被标记上第二颗星。这排除了排序需求。 第三颗星：如果索引行包含查询语句中的所有列，那么索引就可以被标记上第三颗星。这避免了访问表的操作：仅访问索引就可以了。 对于这三颗星，第三颗通常是最重要的。将一个列排除在索引之外可能会导致许多速度较慢的磁盘随机读。我们把一个至少包含第三颗星的索引称为对应查询语句的宽索引。 一个查询语句的三星索引是这个查询的最佳索引。第三颗星说的其实就是索引覆盖的问题。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://nullcc.github.io/categories/数据库/"}],"tags":[{"name":"数据库索引","slug":"数据库索引","permalink":"https://nullcc.github.io/tags/数据库索引/"}]},{"title":"MySQL中Innodb的聚簇索引和非聚簇索引","slug":"MySQL中Innodb的聚簇索引和非聚簇索引","date":"2017-11-30T16:00:00.000Z","updated":"2022-04-15T03:41:13.015Z","comments":true,"path":"2017/12/01/MySQL中Innodb的聚簇索引和非聚簇索引/","link":"","permalink":"https://nullcc.github.io/2017/12/01/MySQL中Innodb的聚簇索引和非聚簇索引/","excerpt":"本文将介绍聚簇索引和非聚簇索引的相关知识。","text":"本文将介绍聚簇索引和非聚簇索引的相关知识。 聚簇索引数据库表的索引从数据存储方式上可以分为聚簇索引和非聚簇索引（又叫二级索引）两种。Innodb的聚簇索引在同一个B-Tree中保存了索引列和具体的数据，在聚簇索引中，实际的数据保存在叶子页中，中间的节点页保存指向下一层页面的指针。“聚簇”的意思是数据行被按照一定顺序一个个紧密地排列在一起存储。一个表只能有一个聚簇索引，因为在一个表中数据的存放方式只有一种。 一般来说，将通过主键作为聚簇索引的索引列，也就是通过主键聚集数据。下图展示了Innodb中聚簇索引的结构（图片来自《高性能MySQL(第三版)》）： 这里要特别注意页的概念，一个页可以理解为一块具有一定大小的连续的存储区域。相同页内的数据行在物理上是相邻的，因此逻辑上键值相邻的页在物理上可能相隔很远。 在中间的某个节点页中，主键&lt;11的叶子页和11&lt;主键&lt;21的叶子页分别被两个指针所指向，且主键&lt;11的叶子页也有一个指针指向了11&lt;主键&lt;21的叶子页，其余页之间的关系也是一样。 聚簇索引的优点 聚簇索引将索引和数据行保存在同一个B-Tree中，查询通过聚簇索引可以直接获取数据，相比非聚簇索引需要第二次查询（非覆盖索引的情况下）效率要高。 聚簇索引对于范围查询的效率很高，因为其数据是按照大小排列的， 聚簇索引的缺点 聚簇索引的更新代价比较高，如果更新了行的聚簇索引列，就需要将数据移动到相应的位置。这可能因为要插入的页已满而导致“页分裂”。 插入速度严重依赖于插入顺序，按照主键进行插入的速度是加载数据到Innodb中的最快方式。如果不是按照主键插入，最好在加载完成后使用OPTIMIZE TABLE命令重新组织一下表。 聚簇索引在插入新行和更新主键时，可能导致“页分裂”问题。 聚簇索引可能导致全表扫描速度变慢，因为可能需要加载物理上相隔较远的页到内存中（需要耗时的磁盘寻道操作）。 非聚簇索引非聚簇索引，又叫二级索引。二级索引的叶子节点中保存的不是指向行的物理指针，而是行的主键值。当通过二级索引查找行，存储引擎需要在二级索引中找到相应的叶子节点，获得行的主键值，然后使用主键去聚簇索引中查找数据行，这需要两次B-Tree查找。 总结下面是Innodb聚簇索引和非聚簇索引的示意图（图片来自《高性能MySQL(第三版)》：","categories":[{"name":"数据库","slug":"数据库","permalink":"https://nullcc.github.io/categories/数据库/"}],"tags":[{"name":"数据库索引","slug":"数据库索引","permalink":"https://nullcc.github.io/tags/数据库索引/"},{"name":"MySQL","slug":"MySQL","permalink":"https://nullcc.github.io/tags/MySQL/"}]},{"title":"MySQL中Innodb的B-Tree索引","slug":"MySQL中Innodb的B-Tree索引","date":"2017-11-29T16:00:00.000Z","updated":"2022-04-15T03:41:13.015Z","comments":true,"path":"2017/11/30/MySQL中Innodb的B-Tree索引/","link":"","permalink":"https://nullcc.github.io/2017/11/30/MySQL中Innodb的B-Tree索引/","excerpt":"本文将介绍MySQL中Innodb的B-Tree索引的相关知识。","text":"本文将介绍MySQL中Innodb的B-Tree索引的相关知识。 数据库索引是数据库开发中非常重要的一个部分，如果想写出好的查询，就必须对索引有比较深刻的理解。人们对索引的了解经常是一知半解的，这就导致很多误用的情况，不但没有提高查询性能，反而大大降低了性能。 简单地说，索引是一种数据结构，我们可以利用索引来快速找到数据。一个现实生活中的例子是查字典，查字典有很多种方式，就中文字典来说，可以使用偏旁部首、拼音、笔画数等信息来查询，这实际上就是对一个字的不同属性建立了索引信息。查字典这个场景可以类比到数据库中，字典对应数据库的一张表，每个字对应表中的一行，每行中有偏旁部署、拼音、笔画数等信息的列，然后针对这些列建立索引。 B-Tree索引大部分情况下我们谈论到索引都是B-Tree索引。在B-Tree索引中，数据是按照顺序被存储的，因此很适合查询范围数据。B-Tree索引的节点页不存放具体数据，它存放的是索引列的值和指向下一层节点页的指针，叶子页中则存放了指向具体数据的指针，且叶子页到根的距离都是相同的。下面是B-Tree索引的示意图（图片来自《高性能MySQL(第三版)》）： 从图中可以发现，每个节点页中都保存了索引列的值，用来和具体的值进行比较，以找到合适的通往下层节点的指针。比如key1这个节点页，其左指针指向的是值小于key1的节点页，右指针指向的是值大于等于key1但小于key2的节点页。需要注意的是，每个叶子页中都保存了下一个叶子页的指针，方便直接查询下一个数据，这样就不用返回上一层重新查找了。 在同层的节点页之间，也有如下关系（key是节点页的索引列的值）： key1 &lt; key2 &lt; … &lt; keyN 在查找时，不需要进行全表扫描，而是从根节点出发，根节点保存了指向子节点的指针，存储引擎顺着这些指针向下查找，通过比较节点页的值和要查找的值就可以找到通往下层节点页的指针，直到找到匹配的一个或多个节点，如果没有找到匹配的节点，说明查询的值在表中不存在。 在上图中只画出了一层节点页，实际上可能存在很多层节点页，表中的数据数量和树的深度相关。上图中叶子页中值是顺序排列的，因此范围查找的效率很高。 B-Tree索引对于以下几类查询是适用的： 全值匹配 对索引中的所有列进行匹配。 匹配最左前缀 使用最左前缀规则匹配列。 匹配列前缀 匹配某一列的值的开头部分。 匹配范围值 匹配一个列中某个范围的值。 精确匹配某一列并范围匹配另外一列 这个应该很好理解。 只访问索引的查询（覆盖索引） 如果查询访问的所有列都在索引范围内，可以使用覆盖索引，这将大大提高查询的效率，因为不需要再去查找具体的数据行了。 由于数据是按顺序排列的，所以对于order by操作，只要是符合最左前缀规则的，B-Tree索引都可以提供支持。 刚才一直提到最左前缀，这到底是什么呢？用一个例子说明最合适不过了，假设有一个表如下： 123456CREATE TABLE user( last_name varchar(50) not null, first_name varchar(50) not null, birthday date not null, key(last_name, first_name, birthday)); 这个表有一个索引，包含了last_name、first_name和birthday三列。以下是几种利用索引有限制的情况： 查询特定first_name的用户将不能利用索引。 查询特定birthday的用户将不能利用索引。 查询特定first_name和特定birthday的用户将不能利用索引。 查询last_name以’h’结尾的用户将不能利用索引。 当查询特定的last_name和特定的birthday时（没有指定first_name），只能利用索引的第一列。 如果某个条件是一个范围查询，则其后的所有列将无法利用索引，比如where last_name=&#39;Smith&#39; and first_name like &#39;J%&#39; and birthday = &#39;1988-12-23&#39;，这个查询只能利用索引的前两列。 由此可以看出，在B-Tree索引中，索引列的顺序是非常重要的。并不是只要建立的相应列的索引，查询就一定能用上。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://nullcc.github.io/categories/数据库/"}],"tags":[{"name":"数据库索引","slug":"数据库索引","permalink":"https://nullcc.github.io/tags/数据库索引/"},{"name":"MySQL","slug":"MySQL","permalink":"https://nullcc.github.io/tags/MySQL/"}]},{"title":"垃圾回收(GC)算法介绍(4)——GC标记-压缩算法","slug":"垃圾回收(GC)算法介绍(4)——GC标记-压缩算法","date":"2017-11-29T16:00:00.000Z","updated":"2022-04-15T03:41:13.032Z","comments":true,"path":"2017/11/30/垃圾回收(GC)算法介绍(4)——GC标记-压缩算法/","link":"","permalink":"https://nullcc.github.io/2017/11/30/垃圾回收(GC)算法介绍(4)——GC标记-压缩算法/","excerpt":"GC标记-压缩算法是GC标记-清除算法和GC复制算法相结合的一种算法。","text":"GC标记-压缩算法是GC标记-清除算法和GC复制算法相结合的一种算法。 GC标记-压缩算法概述GC标记-压缩算法有两个阶段：标记阶段和压缩阶段。 其中标记阶段和GC标记-清除算法的标记阶段完全相同。在压缩阶段中，将堆中的活跃对象按顺序移动到堆的一侧，消除对象之间的空闲区域，起到压缩的作用。 接下来将陆续介绍几种GC标记-压缩算法。 Lisp2算法Lisp2算法是Donald E. Knuth提出的，在该算法中，每个对象都有一个forwarding指针，初始值为NULL，每次GC结束后设置为空。这个指针保存在压缩阶段后每个活跃对象的新地址。下面是Lisp2算法中对象的示意图： 下面通过几张示例图来了解Lisp2算法的基本过程，执行GC前堆的情况如下： 标记阶段后： 压缩阶段后： 可以看到，Lisp2算法将活跃对象移动到堆的一侧，且没有改变它们之间的相对顺序，另外对象之间的空闲空间也被压缩了，在执行GC标记-压缩算法后，对象和对象之间紧挨着的。 我们来看一下压缩阶段发生了什么，伪代码如下： 12345compaction_phase()&#123; set_forwarding_ptr() update_obj_ptr() move_obj()&#125; 由于标记阶段和GC标记-清除算法相同（遍历堆找出所有活跃对象），我们只看压缩阶段，分为三个步骤： 更新forwarding指针 更新对象指针 移动对象 阶段一——更新forwarding指针forwarding指针用来标识压缩阶段后每个活跃对象的新地址，Lisp2算法在压缩阶段的一开始就需要为每个活跃对象标识出它们的新地址。做法很简单，以下是伪代码： 12345678910set_forwarding_ptr()&#123; scan = new_address = $heap_start while(scan &lt; $heap_end)&#123; if(scan.mark == TRUE)&#123; scan.forwarding = new_address new_address += scan.size &#125; scan += scan.size &#125;&#125; 执行set_forwarding_ptr前，堆的情况如下图： 执行set_forwarding_ptr后，堆的情况如下图： 每个对象中的红色箭头(forwarding指针)指向的位置就是该对象在GC后在堆中的位置。 阶段二——更新对象指针伪代码如下： 12345678910111213update_obj_ptr()&#123; for(obj in $root) obj = obj.forwarding scan = $heap_start while(scan &lt; $heap_end)&#123; if(scan.mark == TRUE)&#123; for(child in scan.children) child = child.forwarding &#125; scan += scan.size &#125;&#125; 更新对象指针的步骤也很好理解，首先将根直接引用对象的指针更新为各自的forwarding指针，然后遍历堆，将所有活跃对象的子对象引用也更新为各自的forwarding指针。这就完成了活跃对象的指针更新。更新对象指针后堆的情况如下图：； 阶段三——移动对象该阶段会遍历堆，将所有活动对象移动到其forwarding指针指向的地址。伪代码如下： 12345678910111213move_obj()&#123; scan = $free = $heap_start while(scan &lt; $heap_end)&#123; if(scan.mark == TRUE)&#123; new_address = scan.forwarding copy_data(new_address, scan, scan.size) new_address.mark = FALSE new_address.forwarding = NULL $free += new_address.size &#125; scan += scan.size &#125;&#125; 注意在移动对象时，需要将新地址处对象的mark设为FALSE，forwarding设为NULL。$free是堆的空闲区域开始地址。 移动对象阶段后，堆的情况如下图： 优点和缺点Lisp2算法的优点是相比GC复制算法，堆的空间利用率提高了，因为不再需要区分from空间和to空间，压缩带来的好处是堆的已使用空间更加紧凑，内存分配效率高。 Lisp2算法的缺点也很明显，需要遍历四次堆：标记阶段一次、更新forwarding指针阶段一次、更新对象指针阶段一次、移动对象阶段一次。堆越大，该算法的成本越高，吞吐率也会下降。另外在Lisp2算法中每个对象都要有一个forwarding指针，不管对象有多大这个空间消耗都是固定的。 Two-Finger算法有人提出了一种叫Two-Finger的算法，这个算法的特点是所有对象的大小必须一致，在这种算法中也有forwarding指针，但不需要为每个对象专门准备forwarding指针，因为可以使用原对象的某个域充当forwarding指针。这个算法相比Lisp2算法的优势是只需要遍历两次堆，因此吞吐率比较高。 Two-Finger算法分为两个步骤： 移动对象 更新对象指针 刚才提到在Two-Finger算法中每个对象的大小必须一致，因此可以将活跃对象移动到非活跃对象的空间中，GC之后所有活跃对象就会集中在堆的一侧，空闲空间在另一侧。 如上图所示，从堆的末尾向前寻找活跃对象，将其填充到前面的非活跃对象空间内。 阶段一——移动对象由于所有对象的大小都是一样的，假设这个大小为OBJ_SIZE，有一个$free指针从$heap_start开始，一个live指针从堆中最后一个对象处($heap_end - OBJ_SIZE)开始。$free指针负责查找非活跃对象，live指针寻找活跃对象，并将其移动到$free指针指向的空间。另外需要在live指针指向的每个原对象处设置一个forwarding指针，将其指向移动后的那个对象。 伪代码如下： 1234567891011121314151617move_obj()&#123; $free = $heap_start live = $heap_end - OBJ_SIZE while(TRUE)&#123; while($free.mark == TRUE) // 直到找到一个非活跃对象为止 $free += OBJ_SIZE while(live.mark == FALSE) // 直到找到一个活跃对象为止 live -= OBJ_SIZE if($free &lt; live)&#123; copy_data($free, live, OBJ_SIZE) live.forwarding = $free live.mark = FALSE &#125; else &#123; break &#125; &#125;&#125; 阶段二——更新对象指针对象移动完毕后，堆中地址位于$free指针之前的对象应该都是活跃对象了，$free指针之后的对象则有两种可能： 非活跃对象 已经被移动的对象 非活跃对象我们已不关心，这里需要关注已经被移动的对象。 有了上面的信息我们就有一个判断准则：如果一个活跃对象（或其子对象）的地址位于$free之后，它已经已经被移动到了$free之前的某个位置，这个位置保存在这个对象的forwarding指针内。此时我们必须更新这个对象和所有其子对象的指针。 伪代码如下： 1234567891011121314151617update_obj_ptr()&#123; for(obj in $root)&#123; if(&amp;obj &gt;= $free)&#123; obj = obj.forwarding &#125; &#125; scan = $heap_start while(scan &lt; $free)&#123; scan.mark = FALSE for(child in scan.children)&#123; if(&amp;child &gt;= $free)&#123; child = child.forwarding &#125; &#125; scan += OBJ_SIZE &#125;&#125; 优点和缺点Two-Finger算法的优点是只需要遍历两次堆，效率较高，吞吐率也比Lisp2算法好，且不需要专门为每个用户维护一个forwarding指针，不浪费空间。 Two-Finger算法的缺点也是相当明显的，它有一个明显的限制：所有对象大小必须一致，这是非常麻烦的一个限制，有一种缓解的方式是将堆分成几个部分，每个部分中的对象大小一致，然后各个部分使用Two-Finger算法做GC。另外一个问题是，Two-Finger算法移动对象时没有将有引用关系的对象放在同一个内存页内，导致无法利用高速缓存。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"垃圾回收","slug":"垃圾回收","permalink":"https://nullcc.github.io/tags/垃圾回收/"}]},{"title":"Web后端系统架构漫谈(5)——微服务和远程过程调用(RPC)","slug":"Web后端系统架构漫谈(5)——微服务和远程过程调用(RPC)","date":"2017-11-27T16:00:00.000Z","updated":"2022-04-15T03:41:13.023Z","comments":true,"path":"2017/11/28/Web后端系统架构漫谈(5)——微服务和远程过程调用(RPC)/","link":"","permalink":"https://nullcc.github.io/2017/11/28/Web后端系统架构漫谈(5)——微服务和远程过程调用(RPC)/","excerpt":"为什么需要微服务随着后端系统的演进，在单一项目上做开发已经不能适应快速的迭代和团队分工，功能模块之间开始出现明显的界限，部分功能模块变成公共模块，需要暴露接口供外部调用。这时候是采用微服务的时候了。微服务的关键是识别功能模块的分界，将各个功能模块抽取出来，独立成一个服务，以提高复用性、可维护性和改进性能。","text":"为什么需要微服务随着后端系统的演进，在单一项目上做开发已经不能适应快速的迭代和团队分工，功能模块之间开始出现明显的界限，部分功能模块变成公共模块，需要暴露接口供外部调用。这时候是采用微服务的时候了。微服务的关键是识别功能模块的分界，将各个功能模块抽取出来，独立成一个服务，以提高复用性、可维护性和改进性能。 拿一个网络商城来说，比较常见的拆分方式如下： 用户服务 订单服务 商品信息服务 物流信息服务 数据统计服务 消息推送服务 在后端架构中，这些服务均位于service层，service层的上游是web server层，当web server层的服务器收到请求后，会调用service层的一个或多个服务。在Web后端系统架构漫谈(五)——负载均衡-负载均衡.html)中有一幅图，展示了web后端系统架构的负载均衡体系，其中从web server层到service层有负载均衡，微服务化后，各个服务可以独立部署和维护，因此也可以独立进行负载均衡，这有利于整个后端系统的可扩展性和高可用性。 拆分服务后，各个服务也可以使用不同的技术选型，比如用户服务使用Java，消息推送服务使用Python，这在大公司的大型系统中是有很优势的。 RPC框架在web server调用service层的具体服务时，一般是跨服务器调用，或者叫“远程过程调用”，就是常说的RPC。 在同一个进程空间内，一个函数要调用另一个函数的场景是我们习以为常的情况。但是如果要通过网络远程调用函数，就会经历一个比较复杂的过程，下面看一下RPC的过程示意图： （图片来源：https://www.cs.rutgers.edu/~pxk/417/notes/03-rpc.html ） 我们来看看执行一个RPC要经历的过程： 客户端调用本地的client stub，client stub会将函数名和参数等信息按照一定格式序列化成字节流。 client stub使用系统调用将信息发送给内核。 内核使用某种协议（比如TCP）将网络数据包发送给远端服务器。 远端服务器的server stub收到消息，将字节流反序列化，获得函数名和参数。 远端服务器在本地调用使用参数调用该函数，获得结果。 获得结果后，将结果返回给server stub。 远端服务器的server stub将结果序列化成字节流，发送给内核。 远端服务器的内核将网络数据包发送给客户端。 客户端从内核中读取消息。 客户端client stub将字节流反序列化后获得调用结果。 这个过程还是比较复杂的，涉及了数据的序列化和反序列化、连接池、I/O管理、线程管理、收发队列、超时管理等。如果每次调用都要关注这些细节，就会变得很繁琐。RPC框架的出现就是为了屏蔽这些复杂性，它已经将这些通用的功能封装好。 Apache Thrift下面我们将来实践一把RPC框架，这里选择的是Apache Thrift。Apache Thrift是Facebook开发的一款RPC框架，开源后捐献给Apache软件基金会。 安装首先从这里下载Thrift的源码，对照这里安装。 我们使用Python来演示，假设项目根目录为thrift_demo。首先建立一个shared.thrift文件： 1234567891011121314151617181920212223242526272829303132333435363738394041/* * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership. The ASF licenses this file * to you under the Apache License, Version 2.0 (the * \"License\"); you may not use this file except in compliance * with the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, * software distributed under the License is distributed on an * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY * KIND, either express or implied. See the License for the * specific language governing permissions and limitations * under the License. *//** * This Thrift file can be included by other Thrift files that want to share * these definitions. */namespace cpp sharednamespace d share // \"shared\" would collide with the eponymous D keyword.namespace dart sharednamespace java sharednamespace perl sharednamespace php sharednamespace haxe sharednamespace netcore sharedstruct SharedStruct &#123; 1: i32 key 2: string value&#125;service SharedService &#123; SharedStruct getStruct(1: i32 key)&#125; 然后建立tutorial.thrift文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155/* * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership. The ASF licenses this file * to you under the Apache License, Version 2.0 (the * \"License\"); you may not use this file except in compliance * with the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, * software distributed under the License is distributed on an * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY * KIND, either express or implied. See the License for the * specific language governing permissions and limitations * under the License. */# Thrift Tutorial# Mark Slee (mcslee@facebook.com)## This file aims to teach you how to use Thrift, in a .thrift file. Neato. The# first thing to notice is that .thrift files support standard shell comments.# This lets you make your thrift file executable and include your Thrift build# step on the top line. And you can place comments like this anywhere you like.## Before running this file, you will need to have installed the thrift compiler# into /usr/local/bin./** * The first thing to know about are types. The available types in Thrift are: * * bool Boolean, one byte * i8 (byte) Signed 8-bit integer * i16 Signed 16-bit integer * i32 Signed 32-bit integer * i64 Signed 64-bit integer * double 64-bit floating point value * string String * binary Blob (byte array) * map&lt;t1,t2&gt; Map from one type to another * list&lt;t1&gt; Ordered list of one type * set&lt;t1&gt; Set of unique elements of one type * * Did you also notice that Thrift supports C style comments? */// Just in case you were wondering... yes. We support simple C comments too./** * Thrift files can reference other Thrift files to include common struct * and service definitions. These are found using the current path, or by * searching relative to any paths specified with the -I compiler flag. * * Included objects are accessed using the name of the .thrift file as a * prefix. i.e. shared.SharedObject */include \"shared.thrift\"/** * Thrift files can namespace, package, or prefix their output in various * target languages. */namespace cpp tutorialnamespace d tutorialnamespace dart tutorialnamespace java tutorialnamespace php tutorialnamespace perl tutorialnamespace haxe tutorialnamespace netcore tutorial/** * Thrift lets you do typedefs to get pretty names for your types. Standard * C style here. */typedef i32 MyInteger/** * Thrift also lets you define constants for use across languages. Complex * types and structs are specified using JSON notation. */const i32 INT32CONSTANT = 9853const map&lt;string,string&gt; MAPCONSTANT = &#123;'hello':'world', 'goodnight':'moon'&#125;/** * You can define enums, which are just 32 bit integers. Values are optional * and start at 1 if not supplied, C style again. */enum Operation &#123; ADD = 1, SUBTRACT = 2, MULTIPLY = 3, DIVIDE = 4&#125;/** * Structs are the basic complex data structures. They are comprised of fields * which each have an integer identifier, a type, a symbolic name, and an * optional default value. * * Fields can be declared \"optional\", which ensures they will not be included * in the serialized output if they aren't set. Note that this requires some * manual management in some languages. */struct Work &#123; 1: i32 num1 = 0, 2: i32 num2, 3: Operation op, 4: optional string comment,&#125;/** * Structs can also be exceptions, if they are nasty. */exception InvalidOperation &#123; 1: i32 whatOp, 2: string why&#125;/** * Ahh, now onto the cool part, defining a service. Services just need a name * and can optionally inherit from another service using the extends keyword. */service Calculator extends shared.SharedService &#123; /** * A method definition looks like C code. It has a return type, arguments, * and optionally a list of exceptions that it may throw. Note that argument * lists and exception lists are specified using the exact same syntax as * field lists in struct or exception definitions. */ void ping(), i32 add(1:i32 num1, 2:i32 num2), i32 calculate(1:i32 logid, 2:Work w) throws (1:InvalidOperation ouch), /** * This method has a oneway modifier. That means the client only makes * a request and does not listen for any response at all. Oneway methods * must be void. */ oneway void zip()&#125;/** * That just about covers the basics. Take a look in the test/ folder for more * detailed examples. After you run this file, your generated code shows up * in folders with names gen-&lt;language&gt;. The generated code isn't too scary * to look at. It even has pretty indentation. */ 运行命令： 1thrift -r --gen py tutorial.thrift 生成gen-py目录，在gen-py目录下建立client.py和server.py两个文件。 client.py： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import sysimport glob# sys.path.append('gen-py')# sys.path.insert(0, glob.glob('../../lib/py/build/lib*')[0])from tutorial import Calculatorfrom tutorial.ttypes import InvalidOperation, Operation, Workfrom thrift import Thriftfrom thrift.transport import TSocketfrom thrift.transport import TTransportfrom thrift.protocol import TBinaryProtocoldef main(): # Make socket transport = TSocket.TSocket('localhost', 9090) # Buffering is critical. Raw sockets are very slow transport = TTransport.TBufferedTransport(transport) # Wrap in a protocol protocol = TBinaryProtocol.TBinaryProtocol(transport) # Create a client to use the protocol encoder client = Calculator.Client(protocol) # Connect! transport.open() client.ping() print('ping()') sum_ = client.add(1, 1) print('1+1=%d' % sum_) work = Work() work.op = Operation.DIVIDE work.num1 = 1 work.num2 = 0 try: quotient = client.calculate(1, work) print('Whoa? You know how to divide by zero?') print('FYI the answer is %d' % quotient) except InvalidOperation as e: print('InvalidOperation: %r' % e) work.op = Operation.SUBTRACT work.num1 = 15 work.num2 = 10 diff = client.calculate(1, work) print('15-10=%d' % diff) log = client.getStruct(1) print('Check log: %s' % log.value) # Close! transport.close()if __name__ == \"__main__\": main() server.py文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import globimport sys# sys.path.append('gen-py')# sys.path.insert(0, glob.glob('../../lib/py/build/lib*')[0])from tutorial import Calculatorfrom tutorial.ttypes import InvalidOperation, Operationfrom shared.ttypes import SharedStructfrom thrift.transport import TSocketfrom thrift.transport import TTransportfrom thrift.protocol import TBinaryProtocolfrom thrift.server import TServerclass CalculatorHandler: def __init__(self): self.log = &#123;&#125; def ping(self): print('ping()') def add(self, n1, n2): print('add(%d,%d)' % (n1, n2)) return n1 + n2 def calculate(self, logid, work): print('calculate(%d, %r)' % (logid, work)) if work.op == Operation.ADD: val = work.num1 + work.num2 elif work.op == Operation.SUBTRACT: val = work.num1 - work.num2 elif work.op == Operation.MULTIPLY: val = work.num1 * work.num2 elif work.op == Operation.DIVIDE: if work.num2 == 0: x = InvalidOperation() x.whatOp = work.op x.why = 'Cannot divide by 0' raise x val = work.num1 / work.num2 else: x = InvalidOperation() x.whatOp = work.op x.why = 'Invalid operation' raise x log = SharedStruct() log.key = logid log.value = '%d' % (val) self.log[logid] = log return val def getStruct(self, key): print('getStruct(%d)' % (key)) return self.log[key] def zip(self): print('zip()')if __name__ == '__main__': handler = CalculatorHandler() processor = Calculator.Processor(handler) transport = TSocket.TServerSocket(port=9090) tfactory = TTransport.TBufferedTransportFactory() pfactory = TBinaryProtocol.TBinaryProtocolFactory() server = TServer.TSimpleServer(processor, transport, tfactory, pfactory) print('Starting the server...') server.serve() print('done.') 在根目录下按顺序运行： 1python gen-py/server.py 和 1python gen-py/client.py 效果如下： server: client:","categories":[{"name":"web后端","slug":"web后端","permalink":"https://nullcc.github.io/categories/web后端/"}],"tags":[{"name":"后端架构","slug":"后端架构","permalink":"https://nullcc.github.io/tags/后端架构/"}]},{"title":"互联网创业公司技术团队的管理","slug":"互联网创业公司技术团队的管理","date":"2017-11-27T16:00:00.000Z","updated":"2022-04-15T03:41:13.027Z","comments":true,"path":"2017/11/28/互联网创业公司技术团队的管理/","link":"","permalink":"https://nullcc.github.io/2017/11/28/互联网创业公司技术团队的管理/","excerpt":"这几年基本上是在几家互联网创业公司度过，从最开始的iOS + 后端开发到最后专注于后端开发，有过一些感悟，也踩过一些坑，对此总结了一些经验和失败教训，这里简单记录一下。","text":"这几年基本上是在几家互联网创业公司度过，从最开始的iOS + 后端开发到最后专注于后端开发，有过一些感悟，也踩过一些坑，对此总结了一些经验和失败教训，这里简单记录一下。 互联网创业公司的技术团队内都是比较朝气蓬勃的年轻人，初期的一般的构成是一到两个相对资深的程序员和一些资历尚浅的应届生或有1-3年工作经验的程序员。下面主要从技术选型、日常开发、代码审查、项目进度管理、部门间沟通和协作、团队建设等方面来细说。 技术选型互联网创业公司成立初期，甚至还没正式成立之前，技术人员就必须考虑技术选型的问题，因为这涉及到今后整个技术团队的发展方向。互联网创业公司大部分是以web或者app的形式面向用户，当然还有一些其他的形式，比如智能硬件或者多者结合，这里主要谈谈web和app的形式。在技术选型方面，要充分考虑该领域比较成熟的技术方案，成熟的技术都有很多现成的库可以使用，当你遇到问题时，可以快速找到答案，因为这些问题很多人已经遇到过了。另外使用成熟的技术时，招聘上也有优势，你不需要因为采用了非常小众的技术而费尽心思地去挖人和招聘，这有助于降低前期成本和HR的压力。 当在团队内部推进一项技术方案前，请尽量调研市面上已经成功的案例，分析自己的业务为何需要采用新的技术方案，预估新方案的成本、开发难度、开发周期和可能对现有业务造成的副作用，不要根据个人喜好或仅仅是因为想尝鲜来推进/替换一个技术方案。 当遇到一个比较难以解决的问题时，一般都有两条路：一是用一种比较暴力的方式去解决，二是充分分析这个问题，找到一个完美的解决方案。前者速度快，能够在一定程度上“解决”问题，后者消耗人力和时间都多，但效果最好。选择哪个方式需要看具体情况，要考虑这个功能是不是业务的核心功能，后期是否会有更复杂的需求和并发量，如果回答是肯定的，就需要认认真真地分析这个问题，否则可以依情况采用一些取巧的方式快速解决。 技术和成本的关系也很密切，我们是使用云服务还是自己搭建服务器托管在运营商机房？需要为服务器的CPU、内存、硬盘、网络带宽等资源支多少费用？需要几台服务器？这些数据不能单靠想，最好要有测试数据支撑。建立一套性能测试标准，对你的系统进行测试，你才能知道什么时候需要扩容和缩容。 日常开发代码版本控制是必须的，重要性无须多说。且应该是你在写代码之前就已经准备好的，不管是使用代码版本控制的云服务还是自建代码版本控制仓库，总之必须要做代码版本控制。 程序员们经常陷入一种自我矛盾中：到底要不要写测试代码，单元测试、集成测试、性能测试这些东西如果真的要全部实现，工作量会非常大，所有模块都配备完整的测试代码并不现实，所以我们需要甄别哪些模块今后可能迭代会非常频繁，针对这些模块最好要有相应的单元测试和集成测试，当你为这些模块的迭代忙得不可开交时你会感谢自己之前写过的测试代码。 建立一套运行持续集成（CI）系统也很重要，在每次代码分支合并，都自动运行代码拉取、自动部署、自动运行测试代码，这不仅有利于解放双手减少机械劳动，还有利于快速发现问题和修复。另外在测试过程中发现的bug，如果可能的话，尽量将其测试脚本化后加入到现有的测试脚本库中，时间久了以后你就会拥有一个测试宝库。 团队内需要制定代码规范，最好能写成具体的文档，每个新来的成员入职后阅读。在代码审查中，也需要互相监督代码规范的问题。代码规范的另一个很好的监督办法是使用一些LINT工具，在配置文件内设置代码规范，比如Python的PEP-8规范，使用CI实现自动化代码规范检查。 代码审查代码审查是经常被忽略的一件事，有时候为了求快，做完一个功能就立刻提交到版本控制系统中，然后就测试。这其实并不是一个好习惯，这会让一些新手程序员养成很随意的风格：只要能实现功能就提交测试，出错了再改。理论上来说，每一次提交都应该很慎重，特别是项目参与者比较多和项目复杂以后。适当地引入代码审查机制是有好处的，可以在组内互相审查，提出改进意见。越多人审阅过你的代码，上线的时候出错的概率就越小，员工今后也会慢慢变得更加谨慎和深思熟虑。 项目进度管理项目进度管理不能仅仅是项目经理的口头过问和项目进度邮件中的文字这种相对静态的东西，而应该是更加动态实时的。尝试使用一些项目进度管理工具，项目管理人员，包括项目经理、各个team leader需要时刻掌握这些信息，迭代项目进度预估的数据，使项目进度管理不是一纸空文。 部门间沟通和协作互联网创业公司肯定不止一个技术部门，但技术部门都是非常核心的，所有需求的最终实现者。技术部门要配合的部门很多：产品、设计、运营、商务、财务，当然还有老板本人。做为最忙碌的部门之一，技术部门和其他部门的沟通和协作至关重要。 一般来说，建议在部门内设置小组，比如：web前端小组、web后端小组、app开发组等，小组内还可以继续细分，如果人员本来就非常少，就不需要分得太细，也可以按照项目来分，具体方式可以自己把握。每个小组设置一个组长（可能会有一个人身兼多个小组长的可能），组长会辛苦一点，日常组间沟通、和技术部门领导沟通、任务分配和时间预估等事项需要小组长参与，其余组员不太需要参与，可以专心做业务。另外在人员数量较多的情况下，为了减轻组长的压力也可以设置两个组长互为主备，日常切换由组内自己定夺。 团队建设技术团队内可以组织一些技术分享会，可以由几个活跃分子牵头，将开发中遇到的问题总结出来，或者最近研究的技术心得，向部门的成员分享。这个活动应该是充分自由的，不应该称为团队成员的负担。 定期向员工征集书单（工作相关的），公司购买后员工可借阅。 制定导师计划，当新入职的成员是应届生或者毕业后工作不满2年（这个时间可以视情况而定），最好制定一个导师进行一段时间的指导。导师需要了解新人之前工作的一些情况，带他熟悉项目代码，加快新员工的熟悉进度，尽快投入到实际工作中。 不建议HR部门过多干涉技术部门员工的事情，比如KPI。技术部门的业绩应该有自己独立的一套评定体系。 在互联网创业公司中，技术部门的每个人都是多面手，经常要同时处理很多项目，因此对员工的自觉性、学习能力和学习态度有比较高的要求。要充分利用员工的试用期（考核期），在考核期内，导师必须观察和评定新人的能力和不足，帮助新人进步。一段时间内表现欠佳的，需要谈话，如果考核期到但表现还是没有进步的，要毫不犹豫辞退，这样也是对其他员工的公平。","categories":[{"name":"技术管理","slug":"技术管理","permalink":"https://nullcc.github.io/categories/技术管理/"}],"tags":[{"name":"技术管理","slug":"技术管理","permalink":"https://nullcc.github.io/tags/技术管理/"}]},{"title":"Web后端系统架构漫谈(4)——LVS的三种工作方式","slug":"Web后端系统架构漫谈(4)——LVS的三种工作方式","date":"2017-11-26T16:00:00.000Z","updated":"2022-04-15T03:41:13.023Z","comments":true,"path":"2017/11/27/Web后端系统架构漫谈(4)——LVS的三种工作方式/","link":"","permalink":"https://nullcc.github.io/2017/11/27/Web后端系统架构漫谈(4)——LVS的三种工作方式/","excerpt":"LVS的全称是”Linux Virtual Server”，即Linux虚拟服务器，该项目项目于1998年由章文嵩博士创立。LVS实现了IP层的数据包转发和负载均衡，常被用来进行后端服务集群的虚拟化，对外提供一个统一的IP入口，使整个后端服务集群对外部用户完全透明，就好像是一个台服务器对外提供服务一样。另外使用LVS可以很方便地在后端服务器集群中增删服务节点来提供高可扩展性，LVS也提供健康检查来实现集群的高可用性。","text":"LVS的全称是”Linux Virtual Server”，即Linux虚拟服务器，该项目项目于1998年由章文嵩博士创立。LVS实现了IP层的数据包转发和负载均衡，常被用来进行后端服务集群的虚拟化，对外提供一个统一的IP入口，使整个后端服务集群对外部用户完全透明，就好像是一个台服务器对外提供服务一样。另外使用LVS可以很方便地在后端服务器集群中增删服务节点来提供高可扩展性，LVS也提供健康检查来实现集群的高可用性。 虚拟服务器的工作示意图（图片来自http://www.linuxvirtualserver.org/）： LVS有三种工作方式： VS-NAT (网络地址转换) VS-DR (直接路由) VS-TUN (隧道) 1. VS-NATVS-NAT的工作示意图（图片来自http://www.linuxvirtualserver.org/）： 由于全世界范围内IPv4的资源紧张，但是又有大量的主机需要连接到Internet，所以要想让所有想连接到Internet上的主机都有一个IPv4的公网地址不大现实。于是人们就想到了NAT这种方式。NAT全称是”Network Address Translation”，即网络地址转换。在NAT方式中，LVS的作用是负载均衡和双向请求转发。一个外部请求到达LVS后，LVS检查数据包的目的地址和端口号，如果能在LVS的路由表中找到匹配项，就使用调度算法选择其中一服务器，然后将数据包的目的地址和端口号改写成被选中的服务器的内网IP地址和端口号，接着将数据包转发到这台服务器上，与此同时LVS还会将这条已经建立的连接记录在内部一个哈希表中。之后如果传入的数据包属于这个连接时，LVS在哈希表中就可以直接找到这台服务器，并将数据包包改写后转发过去。当收到服务器应答数据包后，LVS会改写数据包的源地址和源端口号为LVS的外网IP地址和端口号，再将数据包发送给外部的请求方。之后会将这条连接的信息从内部哈希表中删除。 下面用图示的方式看一下NAT的工作方式，假设一个网络的内部结构如下（图片来自http://www.linuxvirtualserver.org/）： 路由表如下： Protocol Virtual IP Address Port Real IP Address Port Weight TCP 202.103.106.5 80 172.16.0.2 80 1 TCP 202.103.106.5 80 172.16.0.3 8000 2 TCP 202.103.106.5 21 172.16.0.3 21 1 当一个外部请求到来时，它的目的IP地址是202.103.106.5，目的端口号为80，下面是该请求在这个网络中的处理步骤： 请求到达LVS，LVS检查路由表，发现满足端口号为80的服务器有两台，使用调度算法选择一台，假设这里选择权重较高的172.16.0.3，端口号为8000。 将数据包的目的地址和目的端口号分别改为172.16.0.3和8000，将该连接加入内部哈希表中，然后将请求包转发到172.16.0.3的8000端口。后续在该连接上如果还有数据包到来，直接从哈希表中取出连接，修改数据包并转发。 172.16.0.3收到数据包，处理后将应答数据包回发给LVS。 LVS收到应答数据包，将数据包中的源地址和源端口号改写成LVS的外网IP地址和外网端口号，最后将数据包发送给外部的请求方。 2. VS-DRDR是”Direct Routing”的意思，即直接路由。 在VS-DR模式中，LVS和真实服务器共享一个虚拟IP地址，即LVS的公网IP地址。当LVS收到一个请求时，会查询路由表找到一台真实服务器，将数据帧的MAC地址改为该真实服务器的MAC地址，然后将数据包转发到该真实服务器，并将这个连接加入内部一个哈希表中，之后这个连接的其他数据包也会被修改后转发到对应的真实服务器。真实服务器处理完后，直接将应答数据包通过Internet发送给外部请求方。 需要注意的是，VS-DR模式要求LVS和所有真实服务器必须处于一个相连接的物理网段中。这是因为DR模式下LVS只会修改数据帧的MAC地址为真实服务器的MAC地址，这个MAC地址这是通过ARP协议获得的，ARP协议通过IP地址获取MAC地址。 VS-DR的工作示意图（图片来自http://www.linuxvirtualserver.org/）： 下面是LVS-DR模式下数据帧中MAC地址的修改过程（图片来自http://www.linuxvirtualserver.org/）： 3. VS-TUNVS-TUN模式使用了IP隧道技术。IP隧道技术能够将一个IP数据包包装在一个新的IP数据包中，然后转发到另一个IP地址。 在VS-TUN模式中，真实服务器都有它们自己的配置了虚拟IP地址并绑定了本地socket的non-arp网络设备，当请求被转发到真实服务器的虚拟IP地址上时，它们会在本地处理这个请求。 当LVS收到一个请求时，会查询路由表找到一台真实服务器，然后使用IP隧道技术将数据包发送给真实服务器，并将这个连接加入内部一个哈希表中，之后这个连接的其他数据包也会被通过IP隧道技术转发到对应的真实服务器，真实服务器收到数据包后需要解封数据包，处理后直接发送给外部请求方。 VS-TUN的工作示意图（图片来自http://www.linuxvirtualserver.org/）：","categories":[{"name":"web后端","slug":"web后端","permalink":"https://nullcc.github.io/categories/web后端/"}],"tags":[{"name":"后端架构","slug":"后端架构","permalink":"https://nullcc.github.io/tags/后端架构/"}]},{"title":"Web后端系统架构漫谈(3)——轮询和加权轮询","slug":"Web后端系统架构漫谈(3)——轮询和加权轮询","date":"2017-11-23T16:00:00.000Z","updated":"2022-04-15T03:41:13.022Z","comments":true,"path":"2017/11/24/Web后端系统架构漫谈(3)——轮询和加权轮询/","link":"","permalink":"https://nullcc.github.io/2017/11/24/Web后端系统架构漫谈(3)——轮询和加权轮询/","excerpt":"轮询轮询是最简单的一种负载均衡手段，是非智能的，轮询按照一个固定的顺序依次将负载分配到下游服务器上去。","text":"轮询轮询是最简单的一种负载均衡手段，是非智能的，轮询按照一个固定的顺序依次将负载分配到下游服务器上去。 请求的编号n(n&gt;=0)、下游服务器的数量x(x&gt;0)和请求被路由到的服务器编号i(i&gt;=0)之间有如下关系： 1i = n % x 这种轮询很简单，但缺点也没明显，无法根据下游服务器的处理能力和实时的负载情况合理分配负载。因此人们又设计了加权轮询算法。 加权轮询加权轮询的优点是可以根据服务器处理能力的不同分配不同比例的请求。每台服务器的权重一般是根据一些指标来综合计算的，常见的指标有： CPU处理能力（CPU频率，高速缓存大小） 内存大小和频率 网络带宽 当前处理的连接数 这里还可以分为静态权重和动态权重两种，静态权重是给每台服务器配置一个固定不变的权重值，动态权重则会根据每台服务器当前的状态来动态计算出一个权重值。两种方式各有利弊，静态权重实现简单但不一定能适应所有情况，动态权重能根据实际情况动态调整，使负载更加均匀，服务器使用效率更高，但实现起来也复杂。 下面要介绍两种加权轮询算法，最大公约数算法法和平滑加权轮询算法。 1. 最大公约数算法最大公约数算法将计算所有服务器的权重值的最大公约数，然后将每台服务器的权重值和它们的最大公约数的比值相加得到数字x，然后继续使用公式算出一个值： 1i = n % x 但此时i并不能直接作为服务器的编号使用，而是要判断i的范围。看一个例子： 假设有3台服务器： A: weight: 4 B: weight: 2 C: weight: 1 它们权重值的最大公约数是1，将每台服务器的权重值和它们的最大公约数的比值相加：4+2+1=7。得到规则： i∈[0, 4)的请求路由到A i∈[4, 6)的请求路由到B i∈[6, 7)的请求路由到C 于是请求编号i∈[0, 7)的请求路由到的服务器编号序列为： [A, A, A, A, B, B, C] 最大公约数算法可以根据每台服务器的权重值来将请求按比例分配给下游服务器，但也有一个缺点，上面i∈[0, 7)的请求，前4个请求都分配给了A，中间两个请求给了B，最后一个请求分配给C。这种算法在数量上是按照权重分配的，但是请求的分布很不均匀。只有等高权重的服务器都分配过一遍后，才轮到低权重的服务器分配。不过这种算法已经基本做到了按权限分配，可用性还是很高的。 2. 平滑加权轮询算法还是假设有3台服务器： A: weight: 4 B: weight: 2 C: weight: 1 在平滑加权轮询算法中，旭要维护每台服务器的当前权重值weight_n和一个当前所有服务器的总权重值current_total_weight。每个请求进来时，先计算current_total_weight，然后将每台服务器的当前权重值加上自己的权重值，将请求分配给当前权重值最大的那台，请求处理完成后，将台服务器的当前权重值减去current_total_weight。周而复始地这样下去。下表记录了前7次请求的理由过程和每台服务器在每个请求时的当前权重值情况： 请求 请求处理前服务器的当前权重值 总权重值 选择的服务器 请求处理后服务器的当前权重值 1 [4, 2, 1] 7 A [-3, 2, 1] 2 [1, 4, 2] 7 B [1, -3, 2] 3 [5, -1, 3] 7 A [-2, -1, 3] 4 [2, 1, 4 ] 7 C [2, 1, -3] 5 [6, 3, -2] 7 A [-1, 3, -2] 6 [3, 5, -1] 7 B [3, -2, -1] 7 [7, 0, 0] 7 A [0, 0, 0] 前7次请求路由的服务器序列为[A, B, A, C, A, B, A]，A被分配了4次，B被分配了2次，C被分配了1次，权重比例和最大公约数算法是一样的，不同的是分配的分布比较均匀了。","categories":[{"name":"web后端","slug":"web后端","permalink":"https://nullcc.github.io/categories/web后端/"}],"tags":[{"name":"后端架构","slug":"后端架构","permalink":"https://nullcc.github.io/tags/后端架构/"}]},{"title":"Web后端系统架构漫谈(2)——一致性hash算法","slug":"Web后端系统架构漫谈(2)——一致性hash算法","date":"2017-11-22T16:00:00.000Z","updated":"2022-04-15T03:41:13.022Z","comments":true,"path":"2017/11/23/Web后端系统架构漫谈(2)——一致性hash算法/","link":"","permalink":"https://nullcc.github.io/2017/11/23/Web后端系统架构漫谈(2)——一致性hash算法/","excerpt":"一致性hash算法使用场景设想一个场景，有n个cache db，以key-value pair的形式存储数据，一个指定的key-value，我们首先对它的key计算哈希值，然后将这个哈希值对n取模，结果就是这个key-value被路由到的cache db。通过这种方式可以将所有的key-value pair路由到对应的cache db中。这个过程用伪代码表示为：","text":"一致性hash算法使用场景设想一个场景，有n个cache db，以key-value pair的形式存储数据，一个指定的key-value，我们首先对它的key计算哈希值，然后将这个哈希值对n取模，结果就是这个key-value被路由到的cache db。通过这种方式可以将所有的key-value pair路由到对应的cache db中。这个过程用伪代码表示为： 12345route_cache_db(key, value, cache_dbs, n)&#123; key_hash = hash_func(key) idx = key_hash % n cache_dbs[idx].set(key, value)&#125; 现在考虑两种情况，一是新增一台cache db，二是其中一台cache db宕机，会发生什么？ 不管是哪一种情况，可用的cache db数量n都会发生改变，route_cache_db中入参n的变化会导致key_hash % n的结果发生变化，因此对任意一个key-value pair都会被路由到和之前不同的cache db中。这意味着一旦cache db的数量发生变化，所有key都需要重建。这无疑是一个巨大的开销，有的系统甚至无法承受这样的开销。并且如果只是使用上面这种算法，我们就无法平滑扩容或缩容这些cache db。 于是有人提出了一致性hash算法，一致性hash能大大缓解上述情况带来的副作用。先来看看什么是一致性hash。 设计一个圆环，假设其数值范围是0~2^32-1，有4个cache db节点。我们使用hash函数对这些cache db的某些信息（如IP、主机名或这些信息的组合）计算哈希值，然后对2^32取模运算，得到这些节点在圆环中的位置。当有key-value pair进来时，使用同样的hash函数计算key的哈希值，然后对2^32取模运算，得到这些key在圆环中的位置。然后按照顺时针方向（递增方向），寻找key在圆环上遇到的第一个cache db节点。示意图如下： 回到刚才讨论的两种情况，新增一台cache db和其中一台发生宕机。 一致性hash算法新增一台cache db的情况 新增一台cache db，在图中表示为node 5，落在node 3到node 5之间的key会被存储到node 5上，因此key 5需要被存储在node 5上。且并不会对其他节点的数据造成影响。如下图： 其中一台发生cache db宕机的情况 其中一台发生cache db宕机，假设node 3发生宕机，node 2到node 3之间的key会被存储到node 4上，因此key 3会被存储在node 4上。且并不会对其他节点的数据造成影响。如下图： hash平衡性当只有少数的cache db时，可能出现两个node在圆环上分布过于靠近的情况，这会导致hash不平衡的情况： 上图这种情况下，node 2会比node 1接受更多的key，造成不平衡。 一种解决方法是，设置多个虚拟node，使node分布更为均匀。比如上图中的情况，我们给每个node都设置3个虚拟node: node-1：node-1#1 node-1#2 node-1#3 node-2：node-2#1 node-2#2 node-3#3 这样就有6个node了。还需要设置这些虚拟node到真实node的对应关系，这在代码中都可以很方便地实现。设置虚拟node后结构如下：","categories":[{"name":"web后端","slug":"web后端","permalink":"https://nullcc.github.io/categories/web后端/"}],"tags":[{"name":"后端架构","slug":"后端架构","permalink":"https://nullcc.github.io/tags/后端架构/"}]},{"title":"Web后端系统架构漫谈(1)——负载均衡","slug":"Web后端系统架构漫谈(1)——负载均衡","date":"2017-11-22T16:00:00.000Z","updated":"2022-04-15T03:41:13.022Z","comments":true,"path":"2017/11/23/Web后端系统架构漫谈(1)——负载均衡/","link":"","permalink":"https://nullcc.github.io/2017/11/23/Web后端系统架构漫谈(1)——负载均衡/","excerpt":"在有一定规模的分布式web系统中，负载均衡是必不可少的。负载均衡这四个字已经完美解释了它的含义：将负载（请求/数据）均匀地分配到多个业务系统中。这里要讨论的是web后端架构设计中使用到的负载均衡技术。","text":"在有一定规模的分布式web系统中，负载均衡是必不可少的。负载均衡这四个字已经完美解释了它的含义：将负载（请求/数据）均匀地分配到多个业务系统中。这里要讨论的是web后端架构设计中使用到的负载均衡技术。 DNS轮询在做域名解析的时候针对一个域名一般都可以添加多条A记录，A记录可以指定一个域名所指向的IP地址，DNS在对一个域名进行解析时，会轮询式地返回其A记录中的任意一个IP地址。以这种方式可以实现DNS级别的负载均衡。 DNS轮询是一种非常简单的负载均衡手段，只需要对域名配置多条A记录即可，很多web系统在最顶层都会使用DNS轮询做负载均衡。但是DNS轮询也有一些缺点，比如它是无脑式轮询，并不会根据每台服务器的负载能力来分配请求。这可能造成负载能力强的服务器只分配到比较少的请求，而负载能力弱的服务器分配到较多请求导致处理不过来。另外由于各大网络运营商都会缓存DNS信息以快速响应DNS解析请求，因此有可能出现某条A记录对应IP的服务器宕机，但DNS缓存还把域名指向这个IP，一些用户无法访问网站的情况。而且就算删除了宕机服务器对应的A记录，DNS信息生效也需要时间，从几分钟到几小时不等，在这段时间内，已经缓存了宕机服务器IP的计算机还是会继续访问这些有问题的服务器，没有缓存DNS信息的计算机可能会从运营商处获取到宕机服务器的IP，同样也是无法访问。 现在很多域名运营商还提供智能DNS轮询，所谓智能，就是返回一个离客户端“最近”的服务器。一般来说，服务器都会部署在各大网络运营商的机房，比如移动机房、联通机房、电信机房。不同运营商之间的网络通信要比同运营商内的网络通信要快（因为不同运营商之间的网络要通信要跨越主干网，更加耗时）。如果识别到客户端是中国移动的，那就返回一个移动机房的服务器IP。这种做法实现起来不难，只要在DNS服务器内部维护一个区域/运营商IP范围列表就可以了。 鉴于DNS轮询的这些缺点，很少有web系统会只使用DNS轮询来作为系统负载均衡的方式。比较常见的组合有DNS轮询+LVS、DNS轮询+Nginx。 接入层负载均衡——Nginx在接入层一般采用Nginx做反向代理来实现负载均衡，对一个域名配置的A记录指向的IP一般就是接入层Nginx反向代理的外网IP了。在Nginx的配置文件nginx.conf中，可以配置下游服务器的负载均衡方式。一般来说有下列方式： 给每台下游服务器配置权重然后按照权重来分配请求。 使用轮询的方式分配请求。 将请求分配给当前最少连接数的服务器。 对请求的IP进行哈希计算，对服务器数量取模后路由到相应的服务器。这种方式会使同一个IP的请求都路由到同一台服务器上。 在Nginx服务器的下游，一般是各种web-server，这些web-server一般会调用下游的service来处理请求。 service层负载均衡在互联网后端架构设计中，web-server的下游是各种service，service层负责对外提供服务，比如用户账户service、商品信息service、订单service、消息推送service等等。在web-server使用服务连接池可以实现web-server层到service层的负载均衡。web-server的服务连接池会建立和下游的service的多个连接，每次随机从服务连接池中取出一个连接来使用，这就实现了负载均衡。 数据层负载均衡数据层中主要是各种db和cache，在互联网web后端中，单表数据量往往很大，需要对db进行水平切分。一般来说db水平切分有两种方式： 按id范围切分 按id哈希切分 举个简单的例子，一个用户表有1亿行数据。 如果按id范围切分，我们可以将其按照每1000W数据一个分库来做水平切分，于是就会有user_0~user_9一共10个分库。然后我们根据请求的id范围来路由到对应的db中。按id范围切分的优点是扩展简单，我们只需要新增一个user_10的库，就可以存放1亿之后的数据，但也有一个问题就是请求可能不均匀，因为一般来说新数据要比老数据活跃。 如果按id哈希切分，先计算id的哈希值，再将这个哈希值对db的个数去模获得对应的db。这种做法的数据均匀性会比较好，但是在扩容的时候稍微麻烦一些，需要数据迁移等操作。 web后端系统架构负载均衡体系图示说了这么多，还是用一幅图来更直观地说明下：","categories":[{"name":"web后端","slug":"web后端","permalink":"https://nullcc.github.io/categories/web后端/"}],"tags":[{"name":"后端架构","slug":"后端架构","permalink":"https://nullcc.github.io/tags/后端架构/"}]},{"title":"垃圾回收(GC)算法介绍(3)——GC复制算法","slug":"垃圾回收(GC)算法介绍(3)——GC复制算法","date":"2017-11-20T16:00:00.000Z","updated":"2022-04-15T03:41:13.031Z","comments":true,"path":"2017/11/21/垃圾回收(GC)算法介绍(3)——GC复制算法/","link":"","permalink":"https://nullcc.github.io/2017/11/21/垃圾回收(GC)算法介绍(3)——GC复制算法/","excerpt":"GC复制算法概述GC复制算法的基本思想是将一个堆分成两个大小完全相等的两个空间：from和to。在分配内存时，总是从from空间中分配，当from空间满无法分配时，将from空间中的所有活动对象都复制到to空间中，复制完毕后回收from空间中的所有对象，最后交换from空间和to空间，如此往复下去。","text":"GC复制算法概述GC复制算法的基本思想是将一个堆分成两个大小完全相等的两个空间：from和to。在分配内存时，总是从from空间中分配，当from空间满无法分配时，将from空间中的所有活动对象都复制到to空间中，复制完毕后回收from空间中的所有对象，最后交换from空间和to空间，如此往复下去。 将活跃对象从from复制到to中需要一个copying函数： 1234567copying()&#123; $free = $to_heap_start for(obj in $root) obj = copy(obj) swap($from_heap_start, $to_heap_start)&#125; 在copying函数中，$free是空闲空间的的头部，一开始被设置成to空间的头部。然后从根上遍历所有能从根引用到的对象，将其复制到to空间中，并递归地将它的所有子对象也复制到to空间中。注意copy函数的返回值是参数对象在to空间的新指针。最后交换from空间和to空间。 这里的核心在于copy函数： 12345678910111213copy(obj)&#123; if(obj.tag != COPIED)&#123; copy_data($free, obj. obj.size) obj.tag = COPIED obj.forwarding = $free $free += obj.size &#125; for(child in obj.forwarding.children) child = copy(child) return obj.forwarding&#125; 我们为每个对象都设置了tag和forwarding域，tag用来标识from空间中的一个对象是否被复制过，forwarding是对象在to空间的对象指针。如果一个对象没有被复制过，我们将它复制到to空间中，并将其tag设为COPIED表示已复制，还需要将它的forwarding指向新空间的那个对象，之后更新$free空闲空间头部地址。此时这个对象的子对象的指针们可能还是指向from空间里的对象，需要递归地复制到to空间中，这里有一个关键点，因为copy函数会返回to空间中新对象的指针，所以： 12for(child in obj.forwarding.children) child = copy(child) 这段代码不但把obj的子对象们复制过去，还将其指针也一并更新了。copy函数的最后返回obj在to空间中的指针。 我们可以借助示意图来更加直观地理解整个复制过程： 假设初始状态如下，现在需要将from空间的活跃对象都复制到to空间中。 先将A复制到to空间，设置A的tag为COPIED，设置A的forwarding为to空间的对象： 然后递归调用copy函数，将A’的子对象C和D复制到to空间，同时设置其tag和forwarding，并将A’指向C和D的原指针更新为指向to空间的新的C’和D’： 当from空间的活动对象都被复制到to空间中后，from空间中的所有对象将被回收。 复制完毕后，回收from空间中所有对象并交换from空间和to空间： 在创建对象的时候，会有条件地执行GC复制算法： 123456789101112new_obj(size)&#123; if($free + size &gt; $from_start + HEAP_SIZE/2)&#123; copying() if($free + size &gt; $from_start + HEAP_SIZE/2) allocation_failed() &#125; obj = $free obj.size = size $free += size return obj&#125; new_obj函数收首先检查from空间是否有足够的空间进行分配，如果没有就执行一次copying函数，然后再次判断是否有足够空间分配，若还是没有就内存分配失败。在实际分配阶段，直接取$free，顺序分配一块内存给新对象，并更新$free指针。这里需要注意，GC复制算法在分配内存时没有遍历空闲链表这种操作，因为可以直接从from空间顺序地划拨一块内存出来，在不执行copying的时候（from空间够），GC复制算法的内存分配效率相当高。 优点和缺点GC复制算法从根出发寻找和复制活跃对象，和堆的大小无关，之和堆上活动对象的多少有关，因此其吞吐量很不错。它还可以实现高速内存分配，因为GC复制算法维护一个from空间可以进行顺序的内存分配，无须遍历空闲链表，因此内存分配效率很高。GC复制算法也不会造成堆的碎片化，因为经过一次GC之后，所有对象都被紧密得排列在堆上，一个对象引用的其他对象也会在堆上紧密排列，它们在内存上邻近，对高速缓存比较友好。 GC复制算法的缺点也很明显，堆使用效率低，只能利用堆的一半大小。另外由于需要移动对象到堆的其他位置，所以不兼容保守式GC。在copy函数内部，会对一个对象递归调用copy，这也是一种开销，如果对象的引用层次过深，可能有栈溢出的危险。 优化方案1. GC广度优先复制算法在概述中秒数的GC复制算法是深度优先的，它会优先对一个对象的所有子对象做复制，在copy函数中递归调用自身来完成，刚才说到这种方案可能引发递归层次过深导致栈溢出。于是有人提出了用迭代的方式替代递归，这样就可以避免递归层次过深导致的栈溢出的问题。伪代码如下： 12345678910111213141516171819202122copying()&#123; scan = $free = $to_heap_start for(obj in $root) obj = copy(obj) while(scan != $free)&#123; for(child in scan.children)&#123; chile = copy(child) &#125; scan += scan.size &#125; swap($from_heap_start, $to_heap_start)&#125;copy(obj)&#123; if(is_obj_in_heap(obj.forwarding, $to_heap_start, HEAP_SIZE/2) == FALSE)&#123; copy_data($free, obj, obj.size) obj.forwarding = $free $free += obj.size &#125; return obj.forwarding&#125; 广度优先复制的思想也不难理解，首先找出所有从根直接引用的对象，将它们全部复制到to空间中。然后从to空间的头部开始遍历对象，将每个对象的子对象复制到to空间，直至scan指针和$free指针相等位置。scan指针指向to空间中当前搜索的对象，$free指针指向to空间中空闲块的头部。我们借助图示来理解这个过程，假设一个堆的初始状态如下： 首先将所有从根直接引用的对象，将它们全部复制到to空间中： 所有根直接引用的对象都复制到to空间后，scan指针在to空间中进行遍历，首先移动A’的子对象到to空间中： 注意在A’的子对象复制完毕后，scan指针指向B’，$free指针指向D’的后面，然后就需要移动B’的子对象到to空间中： 复制完B’的子对象F’后，scan指针指向C’,$free指针指向F’的后面。在这之后，继续将scan指针往前移动，遇到C’、D’和F’，由于这三个对象都没有子对象，不进行复制操作，最终scan指针和$free指针将指向同一处，接着交换from和to空间，复制结束： 优点和缺点GC广度优先复制算法避免了GC深度优先复制算法可能造成过深的递归调用导致栈溢出的问题，如果仔细观察，会发现这个算法将to空间的堆当做一个队列在使用，这非常巧妙。 GC广度优先复制算法的缺点是不像GC深度优先复制算法是高速缓存友好的，GC深度优先复制算法会使一个对象和它的子对象们在堆上彼此相邻，但在广度优先的情况下就不是这样了。 2. GC近似深度优先搜索算法由于广度优先搜索算法存在不能让有引用关系的对象在内存中相邻（或者说在同一个内存页内）的问题，有人开发了GC近似深度优先搜索算法。我们先来看一个示例堆： 假设这里的每个对象都是2个字，一个内存页6个字，也就是说一个内存页最多可以存放3个对象。如果使用广度优先搜索算法，堆上的内存分配情况如下： 灰色矩形框代表内存页，它右上角的数字是内存页的编号，同一个编号的内存页是同一个内存页。通过观察可以发现，除了0号内存页中A和B、C具有引用关系以外，其他内存页中的对象都没有引用关系，因此无法很好地使用高速缓存。 GC近似深度优先搜索算法中有几个很重要的变量： $page：我们将一个堆分割成一个个内存页，$page是这些内存页的数组，$page[i]表示堆上连续的第i个内存页。 $local_scan：每个内存页都有一个当前搜索指针，$local_scan是这些指针的数组，$local_scan[i]表示第i个内存页下一个要搜索的元素指针。 $major_scan：搜索尚未完成的内存页首地址的指针。 $free：空闲分块头部的指针。 下面详细了解一下GC近似深度优先搜索的执行过程。to空间的初始状态如下，此时$local_scan[0]、$major_scan和$free都指向$page[0]的头部： 第一步复制A，然后搜索A，将A的子对象B和C也一起复制过来，完成后$local_scan[0]指向B，表示当前内存页（$page[0]）下一个要搜索的对象是B，由于$page[0]还未搜索完成，所以$major_scan指针不变，$free指针也移动到了C之后，指向$page[1]的头部地址： 现在由于$page[0]指向B，所以开始搜索B，先复制B引用的D： 由于$page[1]已满，D会被复制到$page[1]中，另外$page[0]还未搜索完成，所以$major_scan指针不变，$page[0]中的B也还未所搜索完成，所以$local_scan[0]指针也不变，由于复制了D，$free指针要相应后移。在$page[1]中，还未开始搜索，所以$local_scan[1]指针指向D。 还有一个关键点，该算法在对象被复制到新的内存页时，会使用新页面的$local_scan来搜索，此时会暂停之前的内存页的搜索。 根据这个规则，接下来就要对D引用的对象H、I进行复制了： 此时由于$page[0]还未搜索完成，所以$major_scan指针不变，$page[0]中的B也还未所搜索完成，所以$local_scan[0]指针也不变，由于复制了H和I，$free指针要相应后移。在$page[1]中，D已经搜索完毕，所以$local_scan[1]指针指向H。 接着往下，由于上一次复制过程中并没有对象被复制到新的内存页中，所以回到$marjor_scan指针指向的内存页$page[0]，此时$local_scan[0]指向B，轮到复制B引用的E对象了： 此时由于$page[0]还未搜索完成，所以$major_scan指针不变，$page[0]B已经搜索完成，所以$local_scan[0]指针指向下一个对象C，因为复制了E，$free指针要相应后移。在$page[1]中，H尚未搜索完毕，所以$local_scan[1]指针不变。$page[2]中，E尚未被搜索，所以local_scan[2]指针指向E。 这一步中E被复制到了新的内存页$page[2]中，所以下一次搜索要从$local_scan[2]开始，复制J和K： 此时由于$page[0]还未搜索完成，所以$major_scan指针不变，$page[0]C尚未搜索完成，所以$local_scan[0]指针指向对象C，因为复制了J和K，$free指针要相应后移。在$page[1]中，H尚未搜索完毕，所以$local_scan[1]指针不变。$page[2]中，E已经搜索完毕，所以local_scan[2]指针指向下一个对象J。 按照这个规则一直执行到最后，内存布局如下： GC近似深度优先搜索的内存布局树状图如下： 可以看到互相引用的对象基本都在同一个内存页中了，这可以有效利用高速缓存。 3. 多空间复制算法GC复制算法的一大缺点就是每次只能利用堆空间的一半，有一种算法的思想是这样的，将堆n等分，拿出2个等分的空间用来作为from空间和to空间，以执行GC复制算法，其他空间使用标记-清除算法处理。每次GC都会使to空间和from空间向后移动一个等分。让我们用图示来了解一下这个过程。我们将堆4等分，刚开始的时候，to空间为$heap[0]，from空间为$heap[1]： 执行第一次GC后，堆的布局如下： 将$heap[1]的活跃对象移动到$heap[0]中，其余空间使用标记-清除算法，将清除出来的空间链接到空闲链表中，之后to空间和from空间都向后移动一等分。 第一次GC后，程序继续执行，一段事件后可用空间又满了，执行第二次GC： 多空间复制算法的优点是将原来不能使用的空间从1/2降低到1/n，提高了堆的利用率。缺点是除了2/n部分使用GC复制算法，(n-2)/n的空间使用标记-清除算法处理，会降低分配速度（分配空间时要遍历空闲链表），而且还会造成堆的碎片化。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"垃圾回收","slug":"垃圾回收","permalink":"https://nullcc.github.io/tags/垃圾回收/"}]},{"title":"Trie树(单词查找树)简介","slug":"Trie树(单词查找树)简介","date":"2017-11-17T16:00:00.000Z","updated":"2022-04-15T03:41:13.021Z","comments":true,"path":"2017/11/18/Trie树(单词查找树)简介/","link":"","permalink":"https://nullcc.github.io/2017/11/18/Trie树(单词查找树)简介/","excerpt":"Trie树经常被用来保存单词，所以又被称为单词查找树。它有几个特点：","text":"Trie树经常被用来保存单词，所以又被称为单词查找树。它有几个特点： 根节点不保存字符，其余的每个节点都保存一个字符。 包括根节点在内，所有节点都有一个字符数组用来保存下一个字符的节点。 从根节点出发，沿着子节点一直往下直到某个叶子节点，可以得到一个保存在Trie树中的单词（字符串）。 先来看看Trie树的结构： 这棵Trie树中保存了bat bag bay bear beat bed nice night一共八个单词。 下面来看看Trie树的C语言简单实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#define MAX 26#define MAXLEN 100#define FALSE 0#define TRUE 1// Trie节点结构typedef struct TrieNode&#123; char c; // 每个节点保存的单个字符 int count; // 以从根节点到当前节点作为前缀的字符串个数 struct TrieNode *next[MAX]; // 下一个节点的字符数组 int exist; // 表示到该字符为止是否，搜索过程是否获得一个已存在的字符串&#125;TrieNode; // 创建节点TrieNode *createTrieNode()&#123; TrieNode *node = (TrieNode *)malloc(sizeof(TrieNode)); node-&gt;count = 0; node-&gt;exist = 0; // 初始时节点的next数组的元素都为NULL for(int i = 0; i &lt; MAX; i++)&#123; node-&gt;next[i] = NULL; &#125; return node;&#125;// 插入一个单词void Insert(char *word, TrieNode *root)&#123; int i; TrieNode *cur; if(word[0] == '\\0') // 不处理空字符串 return; cur = root; for(i = 0; word[i] != '\\0'; i++) &#123; int id = word[i] - 'a'; // 计算字符在next数组的下标 if(cur-&gt;next[id] == NULL) // 如果这个字符不存在于next数组，创建该字符的节点 &#123; TrieNode *newNode = createTrieNode(); newNode-&gt;c = word[i]; cur-&gt;next[id] = newNode; &#125; cur = cur-&gt;next[id]; // 从这个新节点往下搜索 &#125; cur-&gt;count++; cur-&gt;exist = 1; return;&#125;// 遍历树，打印树中所有单词void Traverse(TrieNode *cur)&#123; static char theWord[MAXLEN]; static int pos = 0; int i; if(cur == NULL) return; if(cur-&gt;count) &#123; theWord[pos++] = cur-&gt;c; theWord[pos] = '\\0'; printf(\"%s\\n\", theWord); &#125; if(cur-&gt;c == '\\0')&#123; // 根节点 for(i = 0; i &lt; MAX; i++) &#123; if (cur-&gt;next[i] == NULL)&#123; continue; &#125; pos = 0; Traverse(cur-&gt;next[i]); &#125; &#125; else &#123; for(i = 0; i &lt; MAX; i++) // 非根节点 &#123; if (cur-&gt;next[i] == NULL)&#123; continue; &#125; if(!cur-&gt;count)&#123; theWord[pos++] = cur-&gt;c; &#125; Traverse(cur-&gt;next[i]); pos -= 2; &#125; &#125; return;&#125;// 查找一个单词是不是在树中int Find(TrieNode *root, char *word)&#123; int i; TrieNode *cur; cur = root; // 遍历单词中的每一个字符 for(i = 0; word[i] != '\\0'; i++) &#123; int id = word[i] - 'a'; // 计算字符在next数组中的下标 if(cur-&gt;next[id] == NULL) &#123; return FALSE; &#125; cur = cur-&gt;next[id]; &#125; if(cur-&gt;count) return TRUE; else return FALSE;&#125;// 输入单词，在Trie树中创建字符串，直到输入*号停止输入void Construct(TrieNode *root)&#123; char inStr[MAXLEN]; int size = 0; while(1) &#123; scanf(\"%s\",inStr); if(strcmp(inStr,\"*\")==0) break; Insert(inStr, root); &#125; printf(\"树中的所有单词：\\n\"); Traverse(root); return;&#125;int main() &#123; TrieNode *root = createTrieNode(); root-&gt;c = '\\0'; char str[MAXLEN]; Construct(root); printf(\"\\n\"); while(1) &#123; printf(\"请输入需要查找的单词：\\n\"); scanf(\"%s\", str); if(strcmp(str,\"*\") ==0) break; printf(\"%s:%d\\n\", str, Find(root, str)); &#125; return 0;&#125; 简单说明一下，TrieNode结构体中的next数组长度为26，对应26个小写英文字母，保存了下一个字符的TrieNode节点。其中的遍历和查找都是一个递归过程。 上面的代码实现了输入一系列单词来构建Trie树，然后遍历Trie树输出所有单词，最后用户可以查询某个单词是否在Trie树中。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://nullcc.github.io/categories/数据结构/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://nullcc.github.io/tags/数据结构/"}]},{"title":"Redis中的底层数据结构(7)——跳跃表(zskiplist)","slug":"Redis中的底层数据结构(7)——跳跃表(zskiplist)","date":"2017-11-16T16:00:00.000Z","updated":"2022-04-15T03:41:13.020Z","comments":true,"path":"2017/11/17/Redis中的底层数据结构(7)——跳跃表(zskiplist)/","link":"","permalink":"https://nullcc.github.io/2017/11/17/Redis中的底层数据结构(7)——跳跃表(zskiplist)/","excerpt":"本文将详细说明Redis中跳跃表的实现。 在Redis源码（这里使用3.2.11版本）中，跳跃表的实现在server.h（2.8版本之前是redis.h）中的zskiplist结构和zskiplistNode结构，以及t_zset.c中所有以zsl开头的函数。","text":"本文将详细说明Redis中跳跃表的实现。 在Redis源码（这里使用3.2.11版本）中，跳跃表的实现在server.h（2.8版本之前是redis.h）中的zskiplist结构和zskiplistNode结构，以及t_zset.c中所有以zsl开头的函数。 跳跃表概述Redis使用跳跃表来作为zset的底层数据结构。跳跃表是一种随机化的数据结构，其内部是多个链表的并联。在插入、查找、删除等操作上都有不错的效率，而且实现起来比红黑树要简单。 123456789101112131415161718/* ZSET（有序集合）使用的特殊版本的跳跃表 */// 有序集合跳跃表节点结构typedef struct zskiplistNode &#123; robj *obj; // 节点数据对象，指向一个sds字符串对象 double score; // 节点分值，跳跃表中的所有节点都按照分值从小到大排序 struct zskiplistNode *backward; // 前置节点 struct zskiplistLevel &#123; struct zskiplistNode *forward; // 后置节点 unsigned int span; // 该层跨越的节点数量 &#125; level[]; // 跳跃表层结构，一个zskiplistLevel数组&#125; zskiplistNode;// 有序集合跳跃表结构typedef struct zskiplist &#123; struct zskiplistNode *header, *tail; // 跳跃表表头节点指针和表尾节点指针 unsigned long length; // 跳跃表的长度，即跳跃表当前的节点数量（表头节点不算在内） int level; // 当前跳跃表中层数最大的节点的层数（表头节点的层数不算在内）&#125; zskiplist; 解释一下zskiplist： header：跳跃表头节点指针，跳跃表头节点是一个特殊的节点，它不保存实际分值和对象，它的level数组长度为32。tail：跳跃表尾节点指针，跳跃表尾节点是一个真实的节点，这点和头节点不同。length：跳跃表长度，即跳跃表当节点数量，不包括表头节点。level：跳跃表当前节点中的最大层数，不包括表头节点。 再看看zskiplistNode： obj：节点保存的对象，是一个sds字符串对象。score：节点分值，跳跃表中所有节点都按照分值从小到大排序。backward：指向前驱节点。level[]：zskiplistLevel结构体的数组，数组中的每个zskiplistLevel元素称为“层”。每层中保存了后继节点的指针forward和一个span，span表示当前节点到forward指向的后继节点之间需要跨越多少个节点。 Redis zskiplist的数据结构示意图： 跳跃表查找的原理： 12345678910111213141516171819202122unsigned long zslGetRank(zskiplist *zsl, double score, robj *o) &#123; zskiplistNode *x; unsigned long rank = 0; int i; x = zsl-&gt;header; for (i = zsl-&gt;level-1; i &gt;= 0; i--) &#123; while (x-&gt;level[i].forward &amp;&amp; (x-&gt;level[i].forward-&gt;score &lt; score || (x-&gt;level[i].forward-&gt;score == score &amp;&amp; compareStringObjects(x-&gt;level[i].forward-&gt;obj,o) &lt;= 0))) &#123; rank += x-&gt;level[i].span; x = x-&gt;level[i].forward; &#125; /* x might be equal to zsl-&gt;header, so test if obj is non-NULL */ if (x-&gt;obj &amp;&amp; equalStringObjects(x-&gt;obj,o)) &#123; return rank; &#125; &#125; return 0;&#125; 以上图为例，跳跃表中有4个节点，score分别为1，2，3，4。假设现在我想查询score为3，obj为”c”的节点在跳跃表中的排名。Redis会从level[]从后往前遍历，也就是从跳跃表当前的最大层数向最小层数遍历。从头节点出发，首先遍历第5层，直接找到score等于4的节点，但这个节点并不满足x-&gt;level[i].forward-&gt;score &lt; score这个条件，因此跳过这层。接着还是从头节点触发遍历第4层，首先来到score=2的节点，这个节点满足while循环的条件，因此rank加上上一个节点到score为2这个节点的span，值为2，且x当前指向score为2这个节点。在第4层继续查找，来到score为4的节点，还是不满足x-&gt;level[i].forward-&gt;score &lt; score这个条件，第4层查找接触。此时从score为2这个节点的第3层开始查找，下一个节点来到score为3的节点，这个节点满足while循环的条件，rank加上score为2的节点在第3层的span值，为1，此时rank值等于3，而且score为3这个节点不但score相等，obj成员也相等，我们找到了我们想要的节点了，它在跳跃表中的排名是3。 需要特别说明的是，由于头节点的存在，跳跃表排名是从1开始的，要注意这和数组下标以0开始的区别。 上面的跳跃表查找过程图示如下： 跳跃表实现zslCreateNode函数创建有序集合跳跃表节点。 1234567zskiplistNode *zslCreateNode(int level, double score, robj *obj) &#123; // level为跳跃表节点的层数 zskiplistNode *zn = zmalloc(sizeof(*zn)+level*sizeof(struct zskiplistLevel)); zn-&gt;score = score; // 设置节点分值 zn-&gt;obj = obj; // 设置节点数据 return zn;&#125; zslCreate函数创建有序集合跳跃表。 123456789101112131415161718zskiplist *zslCreate(void) &#123; int j; zskiplist *zsl; zsl = zmalloc(sizeof(*zsl)); zsl-&gt;level = 1; // 初始化时，节点的最大层数只有1 zsl-&gt;length = 0; // 初始化时，跳跃表中没有节点，长度为0 // 创建表头节点，表头的level为32 zsl-&gt;header = zslCreateNode(ZSKIPLIST_MAXLEVEL,0,NULL); // 初始化表头节点的各层，初始化后置节点为NULL，跨度为0 for (j = 0; j &lt; ZSKIPLIST_MAXLEVEL; j++) &#123; zsl-&gt;header-&gt;level[j].forward = NULL; zsl-&gt;header-&gt;level[j].span = 0; &#125; zsl-&gt;header-&gt;backward = NULL; // 表头节点的前置节点为NULL zsl-&gt;tail = NULL; // 初始化时表尾节点为NULL return zsl;&#125; zslFreeNode函数释放有序集合跳跃表节点。 1234void zslFreeNode(zskiplistNode *node) &#123; decrRefCount(node-&gt;obj); // 减少节点数据对象的引用计数 zfree(node);&#125; zslFree函数释放有序集合跳跃表。 1234567891011void zslFree(zskiplist *zsl) &#123; zskiplistNode *node = zsl-&gt;header-&gt;level[0].forward, *next; zfree(zsl-&gt;header); // 释放表头 while(node) &#123; // 遍历跳跃表，释放所有节点 next = node-&gt;level[0].forward; zslFreeNode(node); node = next; &#125; zfree(zsl); // 释放跳跃表&#125; zslRandomLevel函数在创建新跳跃表节点时，为它设置一个随机的层数。此函数的返回值介于1到ZSKIPLIST_MAXLEVEL(32)之间（包含1和32）。采用幂率分布的方式，获得越高层级的level概率越低。 1234567int zslRandomLevel(void) &#123; int level = 1; // 每次有0.25的概率对level+1 while ((random()&amp;0xFFFF) &lt; (ZSKIPLIST_P * 0xFFFF)) level += 1; return (level&lt;ZSKIPLIST_MAXLEVEL) ? level : ZSKIPLIST_MAXLEVEL;&#125; zslInsert函数在有序集合跳跃表中插入一个节点，节点的分值为score，数据对象为obj。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566zskiplistNode *zslInsert(zskiplist *zsl, double score, robj *obj) &#123; // update数组用来存放新节点在跳跃表每一层的前置节点 zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x; unsigned int rank[ZSKIPLIST_MAXLEVEL]; int i, level; serverAssert(!isnan(score)); x = zsl-&gt;header; // 跳跃表头节点 // 遍历各层寻找新节点的插入位置 for (i = zsl-&gt;level-1; i &gt;= 0; i--) &#123; /* 大于当前跳跃表节点最大层数的层（这些层没有数据），rank值为0 */ rank[i] = i == (zsl-&gt;level-1) ? 0 : rank[i+1]; /* 如果当前节点的后置节点存在且给定的score大于当前节点的后置节点的score， * 或给定的score等于当前节点的后置节点的score且给定的obj等于当前节点的后置节点的obj， * 将当前节点的span值加到当前层的rank上，且更新x指向当前层的下一个节点。 */ while (x-&gt;level[i].forward &amp;&amp; (x-&gt;level[i].forward-&gt;score &lt; score || (x-&gt;level[i].forward-&gt;score == score &amp;&amp; compareStringObjects(x-&gt;level[i].forward-&gt;obj,obj) &lt; 0))) &#123; rank[i] += x-&gt;level[i].span; // 记录在该层跨越了多少节点 x = x-&gt;level[i].forward; // 移动到后置节点 &#125; update[i] = x; // 获得新节点在跳跃表每一层的前置节点 &#125; /* we assume the key is not already inside, since we allow duplicated * scores, and the re-insertion of score and redis object should never * happen since the caller of zslInsert() should test in the hash table * if the element is already inside or not. */ level = zslRandomLevel(); // 随机获取一个值作为新节点的层数 /* 如果层数大于跳跃表节点中最大的层数，初始化表头节点中那些未使用的层（共level - zsl-&gt;level个）， * 设置其rank为0，设置其span为跳跃表长度（因为表头节点的层指针直接就指向表尾节点的相应层，接着就指向NULL了， * 相当于跨越了整个跳跃表） * */ if (level &gt; zsl-&gt;level) &#123; for (i = zsl-&gt;level; i &lt; level; i++) &#123; rank[i] = 0; update[i] = zsl-&gt;header; update[i]-&gt;level[i].span = zsl-&gt;length; &#125; zsl-&gt;level = level; // 更新跳跃表的level属性 &#125; x = zslCreateNode(level,score,obj); // 创建一个新节点 // 遍历新节点的所有层，建立该节点在每个层在跳跃表中的前后关系 for (i = 0; i &lt; level; i++) &#123; x-&gt;level[i].forward = update[i]-&gt;level[i].forward; // 建立新节点和后置节点的关系 update[i]-&gt;level[i].forward = x; // 建立新节点和前置节点的关系 /* 更新新节点的span以及它的后置节点的span */ x-&gt;level[i].span = update[i]-&gt;level[i].span - (rank[0] - rank[i]); update[i]-&gt;level[i].span = (rank[0] - rank[i]) + 1; &#125; /* 由于新节点中从level到zsl-&gt;level层的存在，它的前置节点相应层的span需要+1 */ for (i = level; i &lt; zsl-&gt;level; i++) &#123; update[i]-&gt;level[i].span++; &#125; // 设置新节点的前置节点 x-&gt;backward = (update[0] == zsl-&gt;header) ? NULL : update[0]; if (x-&gt;level[0].forward) x-&gt;level[0].forward-&gt;backward = x; // 新节点有后置节点，设置它后置节点的前置节点为它自己 else zsl-&gt;tail = x; // 新节点没有后置节点，它就是尾节点 zsl-&gt;length++; // 更新跳跃表节点数量 return x;&#125; zslDeleteNode函数是zslDelete、zslDeleteByScore和zslDeleteByRank函数内部使用的函数，删除一个指定的跳跃表节点。 1234567891011121314151617181920212223void zslDeleteNode(zskiplist *zsl, zskiplistNode *x, zskiplistNode **update) &#123; // zsl为跳跃表，x为要删除的节点，update为指向保存在每一层中要删除节点的前置节点的数组 int i; // 更新删除点上每一层的的节点关系和跨度 for (i = 0; i &lt; zsl-&gt;level; i++) &#123; if (update[i]-&gt;level[i].forward == x) &#123; update[i]-&gt;level[i].span += x-&gt;level[i].span - 1; update[i]-&gt;level[i].forward = x-&gt;level[i].forward; &#125; else &#123; update[i]-&gt;level[i].span -= 1; &#125; &#125; // 更新被删除节点的前置和后置指针 if (x-&gt;level[0].forward) &#123; x-&gt;level[0].forward-&gt;backward = x-&gt;backward; &#125; else &#123; zsl-&gt;tail = x-&gt;backward; &#125; // 更新跳跃表的最大层数 while(zsl-&gt;level &gt; 1 &amp;&amp; zsl-&gt;header-&gt;level[zsl-&gt;level-1].forward == NULL) zsl-&gt;level--; zsl-&gt;length--; // 跳跃表节点数量-1&#125; zslDelete函数从有序集合跳跃表中删除一个具有指定score和object的节点。 12345678910111213141516171819202122int zslDelete(zskiplist *zsl, double score, robj *obj) &#123; zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x; int i; x = zsl-&gt;header; // 表头节点 for (i = zsl-&gt;level-1; i &gt;= 0; i--) &#123; while (x-&gt;level[i].forward &amp;&amp; (x-&gt;level[i].forward-&gt;score &lt; score || (x-&gt;level[i].forward-&gt;score == score &amp;&amp; compareStringObjects(x-&gt;level[i].forward-&gt;obj,obj) &lt; 0))) x = x-&gt;level[i].forward; update[i] = x; // 获得指定节点在跳跃表每一层的前置节点 &#125; /* 可能有多个节点具有相同的分值，需要找到分值和对象都相等的节点。 */ x = x-&gt;level[0].forward; if (x &amp;&amp; score == x-&gt;score &amp;&amp; equalStringObjects(x-&gt;obj,obj)) &#123; // score和obj都相等 zslDeleteNode(zsl, x, update); // 删除跳跃表节点 zslFreeNode(x); // 释放跳跃表节点 return 1; &#125; return 0; /* not found */&#125; zslValueGteMin函数判断给定值value是否大于或等于范围spec中的min，返回1表示value大于或等于min，否则返回0。 123static int zslValueGteMin(double value, zrangespec *spec) &#123; return spec-&gt;minex ? (value &gt; spec-&gt;min) : (value &gt;= spec-&gt;min);&#125; zslValueLteMax函数判断给定值value是否小于或等于范围spec中的max，返回1表示value小于或等于max，否则返回0。 123int zslValueLteMax(double value, zrangespec *spec) &#123; return spec-&gt;maxex ? (value &lt; spec-&gt;max) : (value &lt;= spec-&gt;max);&#125; zslIsInRange函数判断给定的分值范围range是否在跳跃表的分值范围之内，在返回1，否则返回0。 1234567891011121314151617int zslIsInRange(zskiplist *zsl, zrangespec *range) &#123; zskiplistNode *x; // 先排除总为空的范围值 if (range-&gt;min &gt; range-&gt;max || (range-&gt;min == range-&gt;max &amp;&amp; (range-&gt;minex || range-&gt;maxex))) return 0; // 跳跃表尾部节点是跳跃表分值的上限 x = zsl-&gt;tail; if (x == NULL || !zslValueGteMin(x-&gt;score,range)) return 0; // 跳跃表头部节点的下一个节点是跳跃表分值的下限 x = zsl-&gt;header-&gt;level[0].forward; if (x == NULL || !zslValueLteMax(x-&gt;score,range)) return 0; return 1;&#125; zslFirstInRange函数在跳跃表中查找第一个被包含在指定范围内的节点，如果没找到返回NULL。 1234567891011121314151617181920212223zskiplistNode *zslFirstInRange(zskiplist *zsl, zrangespec *range) &#123; zskiplistNode *x; int i; /* 如果给定的分值范围不在跳跃表分值范围之内，直接返回 */ if (!zslIsInRange(zsl,range)) return NULL; x = zsl-&gt;header; // 表头节点 for (i = zsl-&gt;level-1; i &gt;= 0; i--) &#123; /* Go forward while *OUT* of range. */ while (x-&gt;level[i].forward &amp;&amp; !zslValueGteMin(x-&gt;level[i].forward-&gt;score,range)) x = x-&gt;level[i].forward; &#125; /* This is an inner range, so the next node cannot be NULL. */ x = x-&gt;level[0].forward; serverAssert(x != NULL); /* 判断节点分值是否小于或等于给定范围range的上限 */ if (!zslValueLteMax(x-&gt;score,range)) return NULL; return x;&#125; zslLastInRange函数在跳跃表中查找最后一个被包含在指定范围内的节点，如果没找到返回NULL。 12345678910111213141516171819202122zskiplistNode *zslLastInRange(zskiplist *zsl, zrangespec *range) &#123; zskiplistNode *x; int i; /* If everything is out of range, return early. */ if (!zslIsInRange(zsl,range)) return NULL; x = zsl-&gt;header; for (i = zsl-&gt;level-1; i &gt;= 0; i--) &#123; /* Go forward while *IN* range. */ while (x-&gt;level[i].forward &amp;&amp; zslValueLteMax(x-&gt;level[i].forward-&gt;score,range)) x = x-&gt;level[i].forward; &#125; /* This is an inner range, so this node cannot be NULL. */ serverAssert(x != NULL); /* 判断节点分值是否大于或等于给定范围range的下限 */ if (!zslValueGteMin(x-&gt;score,range)) return NULL; return x;&#125;","categories":[{"name":"源码分析","slug":"源码分析","permalink":"https://nullcc.github.io/categories/源码分析/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://nullcc.github.io/tags/Redis/"},{"name":"数据结构","slug":"数据结构","permalink":"https://nullcc.github.io/tags/数据结构/"}]},{"title":"Redis中的底层数据结构(4)——整数集合(intset)","slug":"Redis中的底层数据结构(4)——整数集合(intset)","date":"2017-11-15T16:00:00.000Z","updated":"2022-04-15T03:41:13.019Z","comments":true,"path":"2017/11/16/Redis中的底层数据结构(4)——整数集合(intset)/","link":"","permalink":"https://nullcc.github.io/2017/11/16/Redis中的底层数据结构(4)——整数集合(intset)/","excerpt":"本文将详细说明Redis中整数集合的实现。 在Redis源码（这里使用3.2.11版本）中，整数集合的实现在intset.h和intset.c中。","text":"本文将详细说明Redis中整数集合的实现。 在Redis源码（这里使用3.2.11版本）中，整数集合的实现在intset.h和intset.c中。 整数集合概述整数集合中保存有序、不重复的多个整数，Redis会根据添加进集合中的数的大小来确定集合的编码方式。比如一个添加到一个集合中的所有数字都可以用int16_t来保存，那么这个整数集合的encoding就是INTSET_ENC_INT16。如果新添加一个数时，发现这个数字无法用int16_t保存，而是要用int32_t，Redis会先把这个整数集合的编码升级为INTSET_ENC_INT32，这需要把集合中所有数字都改为int32_t类型的，然后再添加这个新数。在intset.c中有如下代码：12345/* 注意下面这些编码是有顺序的，关系如下： * INTSET_ENC_INT16 &lt; INTSET_ENC_INT32 &lt; INTSET_ENC_INT64. */#define INTSET_ENC_INT16 (sizeof(int16_t))#define INTSET_ENC_INT32 (sizeof(int32_t))#define INTSET_ENC_INT64 (sizeof(int64_t)) 整数集合支持三种编码：INTSET_ENC_INT16、INTSET_ENC_INT32和INTSET_ENC_INT64。在整数集合升级后，集合中原来的数字的值大小不变，变化的只是其数据类型，升级后整数集合所占用的空间会变大。一种比较极端的例子是，一个包含10000个数字的整数集合，只有一个数字是需要用int64_t来编码的，其余数字只需int16_t编码即可，尽管只有一个数字需要int64_t，我们还是必须将整数集合升级到INTSET_ENC_INT64编码。需要特别注意的是，整数集合只支持升级不支持降级。 Redis整数集合的数据结构示意图： 整数集合中的数据结构下面是intset.h的全部内容： 123456789101112131415/* 整数集合结构 */typedef struct intset &#123; uint32_t encoding; // 编码方式 uint32_t length; // 集合数组中元素个数 int8_t contents[]; // 集合数组&#125; intset;intset *intsetNew(void); // 创建一个新整数集合intset *intsetAdd(intset *is, int64_t value, uint8_t *success); // 向一个整数集合中加入元素intset *intsetRemove(intset *is, int64_t value, int *success); // 从一个整数集合中移除元素uint8_t intsetFind(intset *is, int64_t value); // 在一个整数集合中查找元素int64_t intsetRandom(intset *is); // 从一个整数集合中随机返回一个元素uint8_t intsetGet(intset *is, uint32_t pos, int64_t *value); // 获取整数集合中指定位置上的元素uint32_t intsetLen(intset *is); // 获取整数集合的长度size_t intsetBlobLen(intset *is); // 获取整数集合以字节为单位的大小 整数集合的实现整数集合中的三种编码： 12345/* 注意下面这些编码是有顺序的，关系如下： * INTSET_ENC_INT16 &lt; INTSET_ENC_INT32 &lt; INTSET_ENC_INT64. */#define INTSET_ENC_INT16 (sizeof(int16_t))#define INTSET_ENC_INT32 (sizeof(int32_t))#define INTSET_ENC_INT64 (sizeof(int64_t)) _intsetValueEncoding函数返回给定数字需要的编码方式。 12345678static uint8_t _intsetValueEncoding(int64_t v) &#123; if (v &lt; INT32_MIN || v &gt; INT32_MAX) return INTSET_ENC_INT64; else if (v &lt; INT16_MIN || v &gt; INT16_MAX) return INTSET_ENC_INT32; else return INTSET_ENC_INT16;&#125; _intsetGetEncoded函数根据给定的索引值和编码获取整数集合中的元素。 12345678910111213141516171819static int64_t _intsetGetEncoded(intset *is, int pos, uint8_t enc) &#123; int64_t v64; int32_t v32; int16_t v16; if (enc == INTSET_ENC_INT64) &#123; memcpy(&amp;v64,((int64_t*)is-&gt;contents)+pos,sizeof(v64)); memrev64ifbe(&amp;v64); return v64; &#125; else if (enc == INTSET_ENC_INT32) &#123; memcpy(&amp;v32,((int32_t*)is-&gt;contents)+pos,sizeof(v32)); memrev32ifbe(&amp;v32); return v32; &#125; else &#123; memcpy(&amp;v16,((int16_t*)is-&gt;contents)+pos,sizeof(v16)); memrev16ifbe(&amp;v16); return v16; &#125;&#125; _intsetGet函数根据给定的索引值获取整数集合中的元素，编码使用整数集合的encoding。 123static int64_t _intsetGet(intset *is, int pos) &#123; return _intsetGetEncoded(is,pos,intrev32ifbe(is-&gt;encoding));&#125; _intsetSet函数设置整数集合指定索引值上的元素值，编码使用整数集合的encoding。 1234567891011121314static void _intsetSet(intset *is, int pos, int64_t value) &#123; uint32_t encoding = intrev32ifbe(is-&gt;encoding); if (encoding == INTSET_ENC_INT64) &#123; ((int64_t*)is-&gt;contents)[pos] = value; memrev64ifbe(((int64_t*)is-&gt;contents)+pos); &#125; else if (encoding == INTSET_ENC_INT32) &#123; ((int32_t*)is-&gt;contents)[pos] = value; memrev32ifbe(((int32_t*)is-&gt;contents)+pos); &#125; else &#123; ((int16_t*)is-&gt;contents)[pos] = value; memrev16ifbe(((int16_t*)is-&gt;contents)+pos); &#125;&#125; intsetNew函数创建一个空的整数集合。 123456intset *intsetNew(void) &#123; intset *is = zmalloc(sizeof(intset)); is-&gt;encoding = intrev32ifbe(INTSET_ENC_INT16); // 默认编码是INTSET_ENC_INT16 is-&gt;length = 0; // 集合长度初始化为0 return is;&#125; intsetResize函数调整整数集合大小。 12345static intset *intsetResize(intset *is, uint32_t len) &#123; uint32_t size = len*intrev32ifbe(is-&gt;encoding); // 计算新的集合空间大小 is = zrealloc(is,sizeof(intset)+size); // realloc新的空间，size是集合数组contents的大小，所以还要加上整数集合结构的其他成员空间大小 return is;&#125; intsetSearch函数查找”value”的位置。当找到这个值时返回1且将”pos”指向的值设置为这个位置。当没有找到这个值时返回0且将”pos”指向的值设置为插入”value”到这个整数集合时所在的位置。 123456789101112131415161718192021222324252627282930313233343536373839404142static uint8_t intsetSearch(intset *is, int64_t value, uint32_t *pos) &#123; int min = 0, max = intrev32ifbe(is-&gt;length)-1, mid = -1; int64_t cur = -1; /* 整数集合为空时，不可能找到该值的位置 */ if (intrev32ifbe(is-&gt;length) == 0) &#123; if (pos) *pos = 0; // 向一个空的整数集合加入元素时当然是放在索引为0的位置了 return 0; &#125; else &#123; /* 当在集合中找不到该值时，我们会知道它的插入位置。 */ if (value &gt; _intsetGet(is,intrev32ifbe(is-&gt;length)-1)) &#123; // 整数集合中数组的最后一个值最大，如果value大于这个最大值，插入索引就是当前数组长度 if (pos) *pos = intrev32ifbe(is-&gt;length); return 0; &#125; else if (value &lt; _intsetGet(is,0)) &#123; // 整数集合中数组的第一个值最小，如果value小于这个最小值，插入索引就是0 if (pos) *pos = 0; return 0; &#125; &#125; /* 插入索引在中间的情况，使用二分查找法找到插入位置 */ while(max &gt;= min) &#123; mid = ((unsigned int)min + (unsigned int)max) &gt;&gt; 1; cur = _intsetGet(is,mid); if (value &gt; cur) &#123; min = mid+1; &#125; else if (value &lt; cur) &#123; max = mid-1; &#125; else &#123; break; &#125; &#125; if (value == cur) &#123; // 在整数集合中找到了value，将pos指向的值设为value在数组中的索引 if (pos) *pos = mid; return 1; &#125; else &#123; // 在整数集合中没有找到value，获得它的插入位置，复制给pos指向的值 if (pos) *pos = min; return 0; &#125;&#125; intsetUpgradeAndAdd函数将整数集合升级到一个更大的编码上然后添加给定的整数。 1234567891011121314151617181920212223242526272829/* 将整数集合升级到一个更大的编码上然后添加给定的整数。 */static intset *intsetUpgradeAndAdd(intset *is, int64_t value) &#123; uint8_t curenc = intrev32ifbe(is-&gt;encoding); // 当前整数编码 uint8_t newenc = _intsetValueEncoding(value); // 新的整数编码 int length = intrev32ifbe(is-&gt;length); // 当前整数集合中元素数量 /* 由于需要升级编码，待添加的数一定是大于或者等于当前集合中的所有元素， * 因此只有可能在整数集合数组的头部或尾部添加 */ int prepend = value &lt; 0 ? 1 : 0; /* 设置新整数编码，调整整数集合大小 */ is-&gt;encoding = intrev32ifbe(newenc); is = intsetResize(is,intrev32ifbe(is-&gt;length)+1); /* 以从尾至头的方向处理不会覆盖原来的值。 * 注意\"prepend\"变量是用来判断是否需要在整数集合的开始或结束处保留一个空白空间。 * 如果prepend值为1，意味着要在集合数组头部添加新值，则所有元素要后移一个位置，所以 * _intsetSet的第二个参数是length+prepend；在末尾添加时prepend为0，所有元素不用移位。 * _intsetGetEncoded(is,length,curenc)获取整数集合指定位置上的元素值。 */ while(length--) _intsetSet(is,length+prepend,_intsetGetEncoded(is,length,curenc)); if (prepend) _intsetSet(is,0,value); // 在整数集合数组头部插入元素 else _intsetSet(is,intrev32ifbe(is-&gt;length),value); // 在整数集合数组尾部插入元素 is-&gt;length = intrev32ifbe(intrev32ifbe(is-&gt;length)+1); // 更新整数集合元素数量 return is;&#125; intsetMoveTail函数将整数集合数组中from位置后的数据移动到to位置。 123456789101112131415161718192021static void intsetMoveTail(intset *is, uint32_t from, uint32_t to) &#123; void *src, *dst; // 源数据指针和目标数据指针 uint32_t bytes = intrev32ifbe(is-&gt;length)-from; // 要移动的元素个数 uint32_t encoding = intrev32ifbe(is-&gt;encoding); // 整数集合编码 /* 根据整数集合编码计算要移动的字节数，要移动的字节数 = 要移动的元素个数 * 编码类型单位大小 */ if (encoding == INTSET_ENC_INT64) &#123; src = (int64_t*)is-&gt;contents+from; dst = (int64_t*)is-&gt;contents+to; bytes *= sizeof(int64_t); &#125; else if (encoding == INTSET_ENC_INT32) &#123; src = (int32_t*)is-&gt;contents+from; dst = (int32_t*)is-&gt;contents+to; bytes *= sizeof(int32_t); &#125; else &#123; src = (int16_t*)is-&gt;contents+from; dst = (int16_t*)is-&gt;contents+to; bytes *= sizeof(int16_t); &#125; memmove(dst,src,bytes); // 移动数据&#125; intsetAdd函数向整数集合中添加一个整数。 1234567891011121314151617181920212223242526intset *intsetAdd(intset *is, int64_t value, uint8_t *success) &#123; uint8_t valenc = _intsetValueEncoding(value); // 计算要添加数值需要的编码方式 uint32_t pos; if (success) *success = 1; /* 如果需要就升级整数集合编码方式。当我们要升级时， * 我们可以判断出是要在数组首部（value&lt;0）还是尾部（value&gt;0）插入这个新值， * 因为很容易知道新值对于原集合编码是上溢还是下溢。 */ if (valenc &gt; intrev32ifbe(is-&gt;encoding)) &#123; return intsetUpgradeAndAdd(is,value); // 升级集合编码并插入 &#125; else &#123; /* 如果value已经存在于集合中则终止。 * 当集合中不存在value时，pos指向的值表示value的插入位置。 */ if (intsetSearch(is,value,&amp;pos)) &#123; if (success) *success = 0; // 集合中已经存在value return is; &#125; is = intsetResize(is,intrev32ifbe(is-&gt;length)+1); // 扩充集合大小 if (pos &lt; intrev32ifbe(is-&gt;length)) intsetMoveTail(is,pos,pos+1); // 把pos位置之后的数据移动到pos+1处 &#125; _intsetSet(is,pos,value); // 插入value到pos处 is-&gt;length = intrev32ifbe(intrev32ifbe(is-&gt;length)+1); // 更新数组长度 return is;&#125; intsetRemove函数从整数集合种删除元素。 12345678910111213141516171819intset *intsetRemove(intset *is, int64_t value, int *success) &#123; uint8_t valenc = _intsetValueEncoding(value); // 计算要添加数值需要的编码方式 uint32_t pos; if (success) *success = 0; /* 只有要删除数字的编码不大于当前集合编码且存在于集合中时才去做真正的删除操作， * 对于整数集合来说，大于当前集合编码方式的数字不可能存在于这个时刻的集合中。 */ if (valenc &lt;= intrev32ifbe(is-&gt;encoding) &amp;&amp; intsetSearch(is,value,&amp;pos)) &#123; uint32_t len = intrev32ifbe(is-&gt;length); // 当前集合元素数量 if (success) *success = 1; /* 把pos+1之后的数据移动到pos,覆盖掉要删除的元素 */ if (pos &lt; (len-1)) intsetMoveTail(is,pos+1,pos); is = intsetResize(is,len-1); // 调整集合大小 is-&gt;length = intrev32ifbe(len-1); // 更新数组长度 &#125; return is;&#125; intsetFind函数判断value是否存在于集合中。 1234uint8_t intsetFind(intset *is, int64_t value) &#123; uint8_t valenc = _intsetValueEncoding(value); // 计算要添加数值需要的编码方式 return valenc &lt;= intrev32ifbe(is-&gt;encoding) &amp;&amp; intsetSearch(is,value,NULL); // 只是判断元素存在性，因此intsetSearch的pos参数为NULL&#125; intsetRandom函数随机返回集合中一个元素。 123int64_t intsetRandom(intset *is) &#123; return _intsetGet(is,rand()%intrev32ifbe(is-&gt;length));&#125; intsetGet函数返回集合中指定位置处的元素，当pos超出范围时返回0，否则返回1。value指向的地址保存获取到的元素。 1234567uint8_t intsetGet(intset *is, uint32_t pos, int64_t *value) &#123; if (pos &lt; intrev32ifbe(is-&gt;length)) &#123; *value = _intsetGet(is,pos); return 1; &#125; return 0;&#125; intsetLen函数返回整数集合中元素数量。 123uint32_t intsetLen(intset *is) &#123; return intrev32ifbe(is-&gt;length);&#125; intsetBlobLen函数返回整数集合以字节为单位的大小。 123size_t intsetBlobLen(intset *is) &#123; return sizeof(intset)+intrev32ifbe(is-&gt;length)*intrev32ifbe(is-&gt;encoding);&#125;","categories":[{"name":"源码分析","slug":"源码分析","permalink":"https://nullcc.github.io/categories/源码分析/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://nullcc.github.io/tags/Redis/"},{"name":"数据结构","slug":"数据结构","permalink":"https://nullcc.github.io/tags/数据结构/"}]},{"title":"Redis中的底层数据结构(6)——压缩字典(zipmap)","slug":"Redis中的底层数据结构(6)——压缩字典(zipmap)","date":"2017-11-15T16:00:00.000Z","updated":"2022-04-15T03:41:13.020Z","comments":true,"path":"2017/11/16/Redis中的底层数据结构(6)——压缩字典(zipmap)/","link":"","permalink":"https://nullcc.github.io/2017/11/16/Redis中的底层数据结构(6)——压缩字典(zipmap)/","excerpt":"本文将详细说明Redis中——压缩字典的实现。 在Redis源码（这里使用3.2.11版本）中，压缩字典的实现在zipmap.h和zipmap.c中。","text":"本文将详细说明Redis中——压缩字典的实现。 在Redis源码（这里使用3.2.11版本）中，压缩字典的实现在zipmap.h和zipmap.c中。 Redis压缩字典概述1.Redis压缩字典的数据结构示意图： 在zipmap.c中提到： zipmap是一个空间效率非常高的数据结构，它的key查找时间复杂度是O(n)。 举个例子，一个zipmap，其中的key-value映射关系为：”foo” =&gt; “bar”, “hello” =&gt; “world”，它的内存布局如下： \\&lt;zmlen>\\&lt;len>“foo”\\&lt;len>\\&lt;free>“bar”\\&lt;len>“hello”\\&lt;len>\\&lt;free>“world” 一个zipmap由下面几个部分组成： \\&lt;zmlen>：保存zipmap当前的元素数量，占用1字节。当zipmap中元素数量大于或等于254时，这个字段不再有效，取而代之的是我们需要遍历整个zipmap来计算它的元素数量。 \\&lt;len>：保存了它后面的字符串(key或value)的长度。\\&lt;len>的长度是1字节或5字节。如果它的第一个字节的值在0~253之间，它是的值就是这个字节的大小。如果第一个字节是254，则它的大小是后面四个字节表示的值。单字节值为255则表示zipmap的结束。 \\&lt;free>：表示字符串后面未被使用的空闲字节，修改一个key的value会产生空闲字节。比如先把key “foo”的值设为”bar”，然后再把key “foo”的值设为”hi”，就会产生一个空闲字节。\\&lt;free>总是一个8 bit的无符号数，因为如果一个更新操作产生了很多空闲字节，zipmap将重新对这个字符串分配内存以确保空间的紧凑性。 以上两个元素在哈希表中最紧凑的表达方式实际上是： “\\x02\\x03foo\\x03\\x00bar\\x05hello\\x05\\x00world\\xff” 需要注意的是，由于key和value都有一个长度前缀，因此在zipmap中查找一个key的时间复杂度是O(N)，其中N为zipmap中元素的数量，而不是zipmap所占用的字节数。这将大大降低key查找的开销。 压缩字典的数据结构和函数原型1234567891011121314151617181920// 创建一个新的zipmapunsigned char *zipmapNew(void);// 对zipmap设置key-valueunsigned char *zipmapSet(unsigned char *zm, unsigned char *key, unsigned int klen, unsigned char *val, unsigned int vlen, int *update);// 删除zipmap中的指定keyunsigned char *zipmapDel(unsigned char *zm, unsigned char *key, unsigned int klen, int *deleted);// 在使用zipmapNext()函数遍历zipmap之前调用，用于跳过zipmap开头1字节的zmlenunsigned char *zipmapRewind(unsigned char *zm);// 获取当前key的value，并返回下一个节点的地址unsigned char *zipmapNext(unsigned char *zm, unsigned char **key, unsigned int *klen, unsigned char **value, unsigned int *vlen);// 获取指定key对应的valueint zipmapGet(unsigned char *zm, unsigned char *key, unsigned int klen, unsigned char **value, unsigned int *vlen);// 查询是否存在指定keyint zipmapExists(unsigned char *zm, unsigned char *key, unsigned int klen);// 获取zipmap节点数量unsigned int zipmapLen(unsigned char *zm);// 获取zipmap占用的字节数量size_t zipmapBlobLen(unsigned char *zm);// zipmap信息可读化输出void zipmapRepr(unsigned char *p); 压缩字典的实现zipmap.c中的一些宏： 123456789101112// zipmap最大的节点数量#define ZIPMAP_BIGLEN 254// zipmap尾部标志#define ZIPMAP_END 255/* 表示&lt;free&gt;字段的最大值，该字段表示value后面的空闲字节数，&lt;free&gt;占用1字节， * 当用1字节无法表示时，zipmap会重新分配内存，以保证字符串尽量紧凑。 */#define ZIPMAP_VALUE_MAX_FREE 4/* 以下的宏返回编码整数_l的长度需要的字节数，当长度小于ZIPMAP_BIGLEN时返回1，其他值返回5。 */#define ZIPMAP_LEN_BYTES(_l) (((_l) &lt; ZIPMAP_BIGLEN) ? 1 : sizeof(unsigned int)+1) zipmapNew函数创建一个空的zipmap。 12345678unsigned char *zipmapNew(void) &#123; unsigned char *zm = zmalloc(2); // 为zipmap分配空间 // \b空的zipmap只需要zmlen和end两个字节 zm[0] = 0; /* Length */ zm[1] = ZIPMAP_END; return zm;&#125; zipmapDecodeLength函数返回p指向的数据的长度。 12345678static unsigned int zipmapDecodeLength(unsigned char *p) &#123; unsigned int len = *p; if (len &lt; ZIPMAP_BIGLEN) return len; // 如果长度小于ZIPMAP_BIGLEN，编码该长度只需要1字节，直接返回该长度 memcpy(&amp;len,p+1,sizeof(unsigned int)); // 否则编码该长度需要5字节 memrev32ifbe(&amp;len); return len;&#125; zipmapEncodeLength函数计算编码len长度需要的字节数，并保存在p中。如果p为NULL，直接返回这个字节数。 123456789101112131415static unsigned int zipmapEncodeLength(unsigned char *p, unsigned int len) &#123; if (p == NULL) &#123; return ZIPMAP_LEN_BYTES(len); // p为NULL时直接返回编码len所需的字节数 &#125; else &#123; if (len &lt; ZIPMAP_BIGLEN) &#123; // len小于ZIPMAP_BIGLEN时，编码只需要1字节 p[0] = len; return 1; &#125; else &#123; // len大于或等于ZIPMAP_BIGLEN时，编码需要5字节 p[0] = ZIPMAP_BIGLEN; memcpy(p+1,&amp;len,sizeof(len)); memrev32ifbe(p+1); return 1+sizeof(len); &#125; &#125;&#125; zipmapLookupRaw函数在zipmap中查找匹配的key，找到就返回该节点的指针，否则返回NULL。如果没有找到（返回NULL）且totlen不为NULL，就把totlen设置为zipmap占用的字节数，这样调用者就可以对原zipmap进行realloc使得它可以容纳更多元素。 12345678910111213141516171819202122232425262728293031static unsigned char *zipmapLookupRaw(unsigned char *zm, unsigned char *key, unsigned int klen, unsigned int *totlen) &#123; // p: 当前节点指针，k: 匹配到的节点指针 unsigned char *p = zm+1, *k = NULL; unsigned int l,llen; while(*p != ZIPMAP_END) &#123; // 遍历整个zipmap unsigned char free; /* Match or skip the key */ l = zipmapDecodeLength(p); // 计算p指向节点的key的长度 llen = zipmapEncodeLength(NULL,l); // 编码l需要的字节数 // p+llen是当前节点key的地址，l是当前节点的key的长度 if (key != NULL &amp;&amp; k == NULL &amp;&amp; l == klen &amp;&amp; !memcmp(p+llen,key,l)) &#123; /* total不为NULL时，用户需要知道zipmap占用的字节数， * 因此需要继续往下遍历，所以用k先保存匹配的节点的指针，p用于继续往下遍历。 */ if (totlen != NULL) &#123; k = p; &#125; else &#123; // total为NULL时，说明用户不关心zipmap占用的字节数，直接返回找到节点指针即可 return p; &#125; &#125; p += llen+l; // llen+l = 编码当前节点key需要的字节数+key长度，更新以后p指向当前节点的value_len字段 /* 跳过当前节点的value字段 */ l = zipmapDecodeLength(p); // 计算p指向节点的value的长度 p += zipmapEncodeLength(NULL,l); // 计算编码p指向节点的value的长度所需的字节数，并更新p指向当前节点的free free = p[0]; // 获取free的大小 p += l+1+free; // 跳过当前节点的free字段、free指明的空闲大小和value的长度 &#125; if (totlen != NULL) *totlen = (unsigned int)(p-zm)+1; // 赋值totlen return k;&#125; zipmapRequiredLength函数计算以klen为长度的key和以vlen为长度的value的节点需要的空间大小。 12345678static unsigned long zipmapRequiredLength(unsigned int klen, unsigned int vlen) &#123; unsigned int l; l = klen+vlen+3; // 编码key_len需要的空间(最少1字节) + 编码value_len需要的空间(最少1字节) + free(1字节) if (klen &gt;= ZIPMAP_BIGLEN) l += 4; // key_len所能编码的大小超过ZIPMAP_BIGLEN时，需要扩容到5字节 if (vlen &gt;= ZIPMAP_BIGLEN) l += 4; // value_len所能编码的大小超过ZIPMAP_BIGLEN时，需要扩容到5字节 return l;&#125; zipmapRawKeyLength函数返回指定节点的key占用的空间大小。 1234static unsigned int zipmapRawKeyLength(unsigned char *p) &#123; unsigned int l = zipmapDecodeLength(p); // key本身的长度 return zipmapEncodeLength(NULL,l) + l; // key占用的空间大小 = key本身的长度 + 编码key的长度所需的字节数&#125; zipmapRawValueLength函数返回value占用的总空间（value_len + free（本身和其表示的大小之和） + value）。 12345678static unsigned int zipmapRawValueLength(unsigned char *p) &#123; unsigned int l = zipmapDecodeLength(p); // value本身的长度 unsigned int used; // value使用的空间大小 used = zipmapEncodeLength(NULL,l); // 编码value长度所需的字节数，即value_len used += p[used] + 1 + l; // p[used]表示free字段中保存的value后的空闲空间大小，1表示free本身占用1字节，l为value长度 return used;&#125; zipmapRawEntryLength函数，如果p指向一个key，此函数返回该节点占用的空间大小（节点占用空间 = key占用大小 + value占用大小 + 尾部空闲空间大小）。 1234static unsigned int zipmapRawEntryLength(unsigned char *p) &#123; unsigned int l = zipmapRawKeyLength(p); // key占用的空间大小 return l + zipmapRawValueLength(p+l); // value占用的空间大小（已经包括尾部空闲空间）&#125; zipmapResize函数调整zipmap空间大小，len是新的大小。 123456/* 调整zipmap空间大小，len是新的大小 */static inline unsigned char *zipmapResize(unsigned char *zm, unsigned int len) &#123; zm = zrealloc(zm, len); zm[len-1] = ZIPMAP_END; return zm;&#125; zipmapSet函数对指定的key设置value，如果key不存在就创建key。如果update非空且key已经存在， *update被设置为1（意味着是更新key而不是创建key），否则为0。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566unsigned char *zipmapSet(unsigned char *zm, unsigned char *key, unsigned int klen, unsigned char *val, unsigned int vlen, int *update) &#123; unsigned int zmlen, offset; // reqlen: key-value对占用的空间 unsigned int freelen, reqlen = zipmapRequiredLength(klen,vlen); unsigned int empty, vempty; unsigned char *p; freelen = reqlen; if (update) *update = 0; p = zipmapLookupRaw(zm,key,klen,&amp;zmlen); // 查找指定key，zmlen中保存了zipmap占用的空间大小 if (p == NULL) &#123; /* 没有找到key，扩大zipmap大小 */ zm = zipmapResize(zm, zmlen+reqlen); p = zm+zmlen-1; zmlen = zmlen+reqlen; // 新的zipmap大小 = 原zipmap大小 + 新key-value pair大小 /* 增加zipmap节点数量 */ if (zm[0] &lt; ZIPMAP_BIGLEN) zm[0]++; &#125; else &#123; /* 找到key的节点，需要判断是否有足够空间存放新的value */ if (update) *update = 1; freelen = zipmapRawEntryLength(p); // 计算找到的节点的总长度 if (freelen &lt; reqlen) &#123; offset = p-zm; // 保存这个节点相对于zipmap首地址的偏移量 zm = zipmapResize(zm, zmlen-freelen+reqlen); // 扩容zipmap，增加reqlen-freelen大小的空间 p = zm+offset; // 恢复这个节点的指针 /* 当前节点后面的节点地址为p+freelen，把它移动到新的位置(p+reqlen) */ memmove(p+reqlen, p+freelen, zmlen-(offset+freelen+1)); zmlen = zmlen-freelen+reqlen; // 新的zipmap占用空间大小 freelen = reqlen; &#125; &#125; /* 现在我们有足够的空间来容纳key-value pair了。此时如果空闲空间太多， * 需要把后面的节点前移，并且缩小zipmap的大小以让空间更加紧凑。 */ empty = freelen-reqlen; if (empty &gt;= ZIPMAP_VALUE_MAX_FREE) &#123; // 空闲空间大于预设值，freelen &gt; reqlen /* 首先，把节点尾部的空闲字节 */ offset = p-zm; // 当前节点相对于zipmap的偏移量 /* p+reqlen: 被更新节点的尾指针 * p+freelen: 如果原zipmap中key不存在，则在此处reqlen=freelen， * 如果是更新key的value，执行到这里说明之前节点长度大于或等于reqlen， * 下面的memmove操作相当于把这个节点之后的所有数据前移freelen-reqlen个字节。 */ memmove(p+reqlen, p+freelen, zmlen-(offset+freelen+1)); zmlen -= empty; // 更新zipmap的长度（减去压缩的空闲空间大小） zm = zipmapResize(zm, zmlen); // 调整zipmap大小 p = zm+offset; // 恢复p指向当前节点 vempty = 0; // 经过数据迁移调整以后，这个节点已经没有空闲空间了 &#125; else &#123; vempty = empty; // 这个节点的空闲空间大小 &#125; /* Key: */ /* 设置key */ p += zipmapEncodeLength(p,klen); // 编码klen长度需要的字节数，p跳过这个长度 memcpy(p,key,klen); // 设置key p += klen; // 跳过key的长度 /* Value: */ /* 设置Value */ p += zipmapEncodeLength(p,vlen); // 编码vlen长度需要的字节数，p跳过这个长度 *p++ = vempty; // 设置value，同事p跳过free的长度（1字节） memcpy(p,val,vlen); // 设置value return zm;&#125; zipmapDel函数删除指定的key，如果deleted非空且没有找到指定key，则把*deleted设置为0，如果找到此key且成功删除则设为1。 123456789101112131415161718unsigned char *zipmapDel(unsigned char *zm, unsigned char *key, unsigned int klen, int *deleted) &#123; unsigned int zmlen, freelen; // 查找指定key，并获取zipmap占用的空间大小 unsigned char *p = zipmapLookupRaw(zm,key,klen,&amp;zmlen); if (p) &#123; // 找到该key freelen = zipmapRawEntryLength(p); // 目标节点的长度 memmove(p, p+freelen, zmlen-((p-zm)+freelen+1)); // 把目标节点后的所有数据迁移到目标节点的首地址，覆盖数据 zm = zipmapResize(zm, zmlen-freelen); // 调整zipmap大小 /* Decrease zipmap length */ if (zm[0] &lt; ZIPMAP_BIGLEN) zm[0]--; // 减少zipmap节点数量 if (deleted) *deleted = 1; // 删除了1个节点 &#125; else &#123; if (deleted) *deleted = 0; // 没有找到key &#125; return zm;&#125; zipmapRewind函数在使用zipmapNext()函数遍历zipmap之前调用，用于跳过zipmap开头1字节的zmlen。 123unsigned char *zipmapRewind(unsigned char *zm) &#123; return zm+1;&#125; zipmapNext函数遍历整个zipmap的所有元素。一次调用时，第一个参数指向的是zipmap+1。接下来的所有调用的返回值会作为下一次调用时的第一个参数。例子： 12345unsigned char *i = zipmapRewind(my_zipmap);while((i = zipmapNext(i,&amp;key,&amp;klen,&amp;value,&amp;vlen)) != NULL) &#123; printf(\"%d bytes key at $p\\n\", klen, key); printf(\"%d bytes value at $p\\n\", vlen, value);&#125; 12345678910111213141516unsigned char *zipmapNext(unsigned char *zm, unsigned char **key, unsigned int *klen, unsigned char **value, unsigned int *vlen) &#123; if (zm[0] == ZIPMAP_END) return NULL; // 已经到zipmap尾部，结束遍历，返回NULL if (key) &#123; *key = zm; *klen = zipmapDecodeLength(zm); // 获取当前节点key的长度 *key += ZIPMAP_LEN_BYTES(*klen); // 跳过编码klen所需要的字节数，此时key指向当前节点key的地址 &#125; zm += zipmapRawKeyLength(zm); // 更新zm指针，跳过当前节点的key占用的空间大小，此时zm指向当前节点value_len的地址 if (value) &#123; *value = zm+1; // 跳过free（1字节） *vlen = zipmapDecodeLength(zm); // 获取当前节点value的长度 *value += ZIPMAP_LEN_BYTES(*vlen); // 跳过编码vlen所需要的字节数，此时value指向当前节点value的地址 &#125; zm += zipmapRawValueLength(zm); // 更新zm指针，跳过当前节点的value占用的空间大小，此时zm指向下一个节点的首地址 return zm;&#125; zipmapGet函数在zipmap中查找指定key，并获取它的value和value的长度，如果找到这个key返回1，否则返回0。 123456789int zipmapGet(unsigned char *zm, unsigned char *key, unsigned int klen, unsigned char **value, unsigned int *vlen) &#123; unsigned char *p; if ((p = zipmapLookupRaw(zm,key,klen,NULL)) == NULL) return 0; // 找不到指定key，返回0，否则返回目标节点的指针 p += zipmapRawKeyLength(p); // 跳过key_len字段 *vlen = zipmapDecodeLength(p); // 获取目标节点value_len *value = p + ZIPMAP_LEN_BYTES(*vlen) + 1; // 计算编码目标节点value_len所需字节数，跳过这个字节数和free的1字节，此时指向value return 1;&#125; zipmapExists函数查询key是或否存在，存在返回1，否则返回0。 123int zipmapExists(unsigned char *zm, unsigned char *key, unsigned int klen) &#123; return zipmapLookupRaw(zm,key,klen,NULL) != NULL; // 内部调用zipmap的key查找函数&#125; zipmapLen函数返回zipmap的节点数量。 123456789101112unsigned int zipmapLen(unsigned char *zm) &#123; unsigned int len = 0; if (zm[0] &lt; ZIPMAP_BIGLEN) &#123; // zm_len小于ZIPMAP_BIGLEN时，节点数量就是它的值 len = zm[0]; &#125; else &#123; // 否则需要遍历zipmap计算节点数量 unsigned char *p = zipmapRewind(zm); while((p = zipmapNext(p,NULL,NULL,NULL,NULL)) != NULL) len++; if (len &lt; ZIPMAP_BIGLEN) zm[0] = len; // 遍历完更新zm_len &#125; return len;&#125; zipmapBlobLen函数返回zipmap占用的字节数，我们可以把zipmap序列化到磁盘（或者其他什么地方），只需要以zipmap头部指针为开始，顺序把它所占用字节数存储起来即可。 12345size_t zipmapBlobLen(unsigned char *zm) &#123; unsigned int totlen; zipmapLookupRaw(zm,NULL,0,&amp;totlen); return totlen;&#125;","categories":[{"name":"源码分析","slug":"源码分析","permalink":"https://nullcc.github.io/categories/源码分析/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://nullcc.github.io/tags/Redis/"},{"name":"数据结构","slug":"数据结构","permalink":"https://nullcc.github.io/tags/数据结构/"}]},{"title":"Redis中的底层数据结构(5)——压缩链表(ziplist)","slug":"Redis中的底层数据结构(5)——压缩链表(ziplist)","date":"2017-11-15T16:00:00.000Z","updated":"2022-04-15T03:41:13.019Z","comments":true,"path":"2017/11/16/Redis中的底层数据结构(5)——压缩链表(ziplist)/","link":"","permalink":"https://nullcc.github.io/2017/11/16/Redis中的底层数据结构(5)——压缩链表(ziplist)/","excerpt":"本文将详细说明Redis中压缩链表的实现。 在Redis源码（这里使用3.2.11版本）中，整数集合的实现在ziplist.h和ziplist.c中。","text":"本文将详细说明Redis中压缩链表的实现。 在Redis源码（这里使用3.2.11版本）中，整数集合的实现在ziplist.h和ziplist.c中。 压缩链表概述ziplist是Redis列表键和哈希键的底层实现之一。当一个列表的每个列表项都是较小的整数或较短的字符串时，Redis会使用ziplist作为底层实现。当一个哈希键只包含少量key-value pair，且每个key-value pair的key和value为较小的整数或较短的字符串时，Redis会使用ziplist作为底层实现。 在ziplist.c的注释中，我们可以知道ziplist的大致结构： \\&lt;zlbytes>\\&lt;zltail>\\&lt;zllen>\\&lt;entry>\\&lt;entry>\\&lt;zlend> 这几个部分的含义如下： \\&lt;zlbytes>：一个无符号整数，表示ziplist所占用的字节数。这个值让我们在调整ziplist的大小时无须先遍历它获得其大小。 \\&lt;zltail>：链表中最后一个元素的偏移量。保存这个值可以让我们从链表尾弹出元素而无须遍历整个链表找到最后一个元素的位置。 \\&lt;zllen>：链表中的元素个数。当这个值大于2**16-2时，我们需要遍历真个链表计算出链表中的元素数量。 \\&lt;entry>：链表中的节点。稍后会详细说明节点的数据结构。 \\&lt;zlend>：一个拥有特殊值255的字节，它标识链表结束。 同样，在ziplist.c的注释中，我们还可以发现下面几个宏，在注释中我们知道ziplist的结构细节： 12345678910111213141516// 获取ziplist占用的总字节数，ziplist在zip header的第0~3个字节保存了ZIP_BYTES#define ZIPLIST_BYTES(zl) (*((uint32_t*)(zl)))// 获取ziplist的尾节点偏移量，ziplist在zip header的第4~7个字节保存了ZIP_TAIL#define ZIPLIST_TAIL_OFFSET(zl) (*((uint32_t*)((zl)+sizeof(uint32_t))))// 获取ziplist的节点数量，ziplist在zip header的第8~9个字节保存了ZIP_LENGTH#define ZIPLIST_LENGTH(zl) (*((uint16_t*)((zl)+sizeof(uint32_t)*2)))// 获取ziplist的header大小，zip header中保存了ZIP_BYTES(uint32_t)、ZIP_TAIL(uint32_t)和ZIP_LENGTH(uint16_t)，一共10字节#define ZIPLIST_HEADER_SIZE (sizeof(uint32_t)*2+sizeof(uint16_t))// 获取ziplist的ZIP_END大小，是一个uint8_t类型，1字节#define ZIPLIST_END_SIZE (sizeof(uint8_t))// 获取ziplist ZIP_ENTRY头节点地址，ZIP_ENTRY头指针 = ziplist首地址 + head大小#define ZIPLIST_ENTRY_HEAD(zl) ((zl)+ZIPLIST_HEADER_SIZE)// 获取ziplist ZIP_ENTRY尾节点地址，ZIP_ENTRY尾指针 = ziplist首地址 + 尾节点偏移量#define ZIPLIST_ENTRY_TAIL(zl) ((zl)+intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl)))// 获取ziplist尾指针（ZIP_END），ziplist尾指针 = ziplist首地址 + ziplist占用的总字节数 - 1#define ZIPLIST_ENTRY_END(zl) ((zl)+intrev32ifbe(ZIPLIST_BYTES(zl))-1) 上述代码告诉我们，\\&lt;zlbytes>占用4字节，\\&lt;zltail>占用4字节，\\&lt;zllen>占用2字节，\\\\占用1字节，另外，\\&lt;zlbytes> + \\&lt;zltail> + \\&lt;zllen>合起来称为ziplist header，固定占用10字节，ZIPLIST_ENTRY_HEAD为头节点地址，ZIPLIST_ENTRY_TAIL为尾节点地址，ZIPLIST_ENTRY_END为尾指针，指向ziplist的\\&lt;zlend>。我们根据这些细节可以画出ziplist更详细的结构图： Redis ziplist的数据结构示意图： 再来看看ziplist节点zlentry的定义： 12345678910111213141516171819// 压缩链表节点结构typedef struct zlentry &#123; // prevrawlensize: 上一个节点的长度所占的字节数 // prevrawlen: 上一个节点的长度 unsigned int prevrawlensize, prevrawlen; // lensize: 编码当前节点长度len所需要的字节数 // len: 当前节点长度 unsigned int lensize, len; // 当前节点的header大小，headersize = lensize + prevrawlensize unsigned int headersize; // 当前节点的编码格式 unsigned char encoding; // 当前节点指针 unsigned char *p;&#125; zlentry; 可以得到zlentry的数据结构示意图： 关于ziplist的zlentry，ziplist.c中提到： ziplist中每个节点都有一个header作为前缀，其中包含了两个字段。首先是前一个节点的长度，这个信息可以允许我们从后向前遍历ziplist。第二个字段是节点的编码和节点存储的字符串长度。 前一个节点的长度使用如下方式来编码： 如果前一个节点的长度小于254字节，保存前一个节点的长度只需消耗1字节，长度值就是它的值。如果前一个节点长度大于或等于254，编码它将占用5字节。其中第一个字节的值是254，用来标识后面有一个更大的值，其余4个字节的值就表示前一个节点的长度。 header中另一个字段的值依赖于节点的值。当节点的值是一个字符串，前两个bit将保存用于存储字符串长度的编码类型，后面是字符串的实际长度。当节点的值是一个整数时，前两个bit都为1。之后的两个bit用来指出节点header后保存的整数的类型。下面是不同类型和编码的一个概括： |00pppppp| - 1 byte 长度小于或等于63字节(2^6-1字节)的字符串，保存其长度需要6 bits。 |01pppppp|qqqqqqqq| - 2 bytes 长度小于或等于16383字节(2^14-1字节)的字符串，保存其长度需要14 bits。 |10______|qqqqqqqq|rrrrrrrr|ssssssss|tttttttt| - 5 bytes 长度大于或等于16384字节的字符串，第一个byte的第3~8个bit的值没有含义，第一个byte后的2~5个bytes保存了其长度。 |11000000| - 1 byte 使用int16_t编码的整数，这个整数占用2字节。 |11010000| - 1 byte 使用int32_t编码的整数，这个整数占用4字节。 |11100000| - 1 byte 使用int64_t编码的整数，这个整数占用8字节。 |11110000| - 1 byte 使用24 bits编码的整数，这个整数占用3字节。 |11111110| - 1 byte 使用8 bits编码的整数，这个整数占用1字节。 |1111xxxx| - (其中xxxx的取值在0000~1101之间) 表示一个4 bit整数立即编码，表示的无符号整数范围为0~12。但实际能编码的值为1(0001)~13(1101)，因为0000和1111不能使用。 |11111111| - ziplist的结束符 注意：所有整数都已小端字节序表示。 zlentry实际结构： 压缩链表数据结构下面是ziplist.h中的函数原型： 1234567891011121314unsigned char *ziplistNew(void); // 创建一个压缩链表unsigned char *ziplistMerge(unsigned char **first, unsigned char **second); // 合并两个压缩链表unsigned char *ziplistPush(unsigned char *zl, unsigned char *s, unsigned int slen, int where); // 向表头/表尾添加一个节点unsigned char *ziplistIndex(unsigned char *zl, int index); // 获取索引值为index的节点unsigned char *ziplistNext(unsigned char *zl, unsigned char *p); // 获取指定节点的下一个节点unsigned char *ziplistPrev(unsigned char *zl, unsigned char *p); // 获取指定节点的上一个节点unsigned int ziplistGet(unsigned char *p, unsigned char **sval, unsigned int *slen, long long *lval); // 获取指定节点的信息unsigned char *ziplistInsert(unsigned char *zl, unsigned char *p, unsigned char *s, unsigned int slen); // 在指定节点后插入节点unsigned char *ziplistDelete(unsigned char *zl, unsigned char **p); // 删除指定节点unsigned char *ziplistDeleteRange(unsigned char *zl, int index, unsigned int num); // 从指定的下标开始，删除num个节点unsigned int ziplistCompare(unsigned char *p, unsigned char *s, unsigned int slen); // 比较两个节点的值unsigned char *ziplistFind(unsigned char *p, unsigned char *vstr, unsigned int vlen, unsigned int skip); // 查找指定节点unsigned int ziplistLen(unsigned char *zl); // 获取链表长度size_t ziplistBlobLen(unsigned char *zl); // 获取链表占用的总字节数 压缩链表实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123// ziplist结束标识#define ZIP_END 255#define ZIP_BIGLEN 254/* 不同的编码/长度 */// 字符串掩码 11000000#define ZIP_STR_MASK 0xc0// 整数掩码 00110000#define ZIP_INT_MASK 0x30// 字符串编码，后6位做为长度，字符串长度len&lt;2^6 00XXXXXX，占用1字节#define ZIP_STR_06B (0 &lt;&lt; 6)// 字符串编码，后14位做为长度，字符串长度len&lt;2^14 01XXXXXX XXXXXXXX，占用2字节#define ZIP_STR_14B (1 &lt;&lt; 6)// 字符串编码，后32位做为长度，字符串长度len&lt;2^32 10000000 XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX，占用5字节#define ZIP_STR_32B (2 &lt;&lt; 6)// 16位整数编码，占用2字节，存储结构：11000000，范围-2^16~2^16-1#define ZIP_INT_16B (0xc0 | 0&lt;&lt;4)// 32位整数编码，占用4字节，存储结构：11010000，范围-2^32~2^32-1#define ZIP_INT_32B (0xc0 | 1&lt;&lt;4)// 64位整数编码，占用8字节，存储结构：11100000，范围-2^64~2^64-1#define ZIP_INT_64B (0xc0 | 2&lt;&lt;4)// 24位整数编码，占用3字节，存储结构：11110000，范围-2^24~2^24-1#define ZIP_INT_24B (0xc0 | 3&lt;&lt;4)// 8位整数编码，占用1字节，存储结构：11111110，范围-2^8~2^8-1#define ZIP_INT_8B 0xfe/* 4bit整数立即编码 */// 4bit编码整数立即编码掩码 00001111#define ZIP_INT_IMM_MASK 0x0f// 4bit编码整数立即编码最小值 00001111#define ZIP_INT_IMM_MIN 0xf1 /* 11110001 */#define ZIP_INT_IMM_MAX 0xfd /* 11111101 */// 获取4bit编码整数的值#define ZIP_INT_IMM_VAL(v) (v &amp; ZIP_INT_IMM_MASK)// 24位整数最大值#define INT24_MAX 0x7fffff// 24位整数最小值#define INT24_MIN (-INT24_MAX - 1)// 决定字符串类型的宏#define ZIP_IS_STR(enc) (((enc) &amp; ZIP_STR_MASK) &lt; ZIP_STR_MASK)/* 工具宏 */// 获取ziplist占用的总字节数，ziplist在zip header的第0~3个字节保存了ZIP_BYTES#define ZIPLIST_BYTES(zl) (*((uint32_t*)(zl)))// 获取ziplist的尾节点偏移量，ziplist在zip header的第4~7个字节保存了ZIP_TAIL#define ZIPLIST_TAIL_OFFSET(zl) (*((uint32_t*)((zl)+sizeof(uint32_t))))// 获取ziplist的节点数量，ziplist在zip header的第8~9个字节保存了ZIP_LENGTH#define ZIPLIST_LENGTH(zl) (*((uint16_t*)((zl)+sizeof(uint32_t)*2)))// 获取ziplist的header大小，zip header中保存了ZIP_BYTES(uint32_t)、ZIP_TAIL(uint32_t)和ZIP_LENGTH(uint16_t)，一共10字节#define ZIPLIST_HEADER_SIZE (sizeof(uint32_t)*2+sizeof(uint16_t))// 获取ziplist的ZIP_END大小，是一个uint8_t类型，1字节#define ZIPLIST_END_SIZE (sizeof(uint8_t))// 获取ziplist ZIP_ENTRY头节点地址，ZIP_ENTRY头指针 = ziplist首地址 + head大小#define ZIPLIST_ENTRY_HEAD(zl) ((zl)+ZIPLIST_HEADER_SIZE)// 获取ziplist ZIP_ENTRY尾节点地址，ZIP_ENTRY尾指针 = ziplist首地址 + 尾节点偏移量#define ZIPLIST_ENTRY_TAIL(zl) ((zl)+intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl)))// 获取ziplist尾指针（ZIP_END），ziplist尾指针 = ziplist首地址 + ziplist占用的总字节数 - 1#define ZIPLIST_ENTRY_END(zl) ((zl)+intrev32ifbe(ZIPLIST_BYTES(zl))-1)/* ziplist节点数量的正增量只能是1（删除节点时，负增量有可能小于-1），因为每次只能添加一个元素到ziplist中。 */#define ZIPLIST_INCR_LENGTH(zl,incr) &#123; \\ if (ZIPLIST_LENGTH(zl) &lt; UINT16_MAX) \\ ZIPLIST_LENGTH(zl) = intrev16ifbe(intrev16ifbe(ZIPLIST_LENGTH(zl))+incr); \\&#125;// 压缩链表节点结构typedef struct zlentry &#123; // prevrawlensize: 上一个节点的长度所占的字节数 // prevrawlen: 上一个节点的长度 unsigned int prevrawlensize, prevrawlen; // lensize: 编码当前节点长度len所需要的字节数 // len: 当前节点长度 unsigned int lensize, len; // 当前节点的header大小，headersize = lensize + prevrawlensize unsigned int headersize; // 当前节点的编码格式 unsigned char encoding; // 当前节点指针 unsigned char *p;&#125; zlentry;// 重置压缩链表节点#define ZIPLIST_ENTRY_ZERO(zle) &#123; \\ (zle)-&gt;prevrawlensize = (zle)-&gt;prevrawlen = 0; \\ (zle)-&gt;lensize = (zle)-&gt;len = (zle)-&gt;headersize = 0; \\ (zle)-&gt;encoding = 0; \\ (zle)-&gt;p = NULL; \\&#125;/* 提取ptr指向的字节的编码并把其编码设置为encoding指定的值 */#define ZIP_ENTRY_ENCODING(ptr, encoding) do &#123; \\ (encoding) = (ptr[0]); \\ if ((encoding) &lt; ZIP_STR_MASK) (encoding) &amp;= ZIP_STR_MASK; \\&#125; while(0) zipIntSize函数返回存储一个以encoding为编码的整型需要的字节数。 123456789101112unsigned int zipIntSize(unsigned char encoding) &#123; switch(encoding) &#123; case ZIP_INT_8B: return 1; case ZIP_INT_16B: return 2; case ZIP_INT_24B: return 3; case ZIP_INT_32B: return 4; case ZIP_INT_64B: return 8; default: return 0; /* 4 bit immediate */ &#125; assert(NULL); // 不应该到达这里 return 0;&#125; zipEncodeLength函数计算新节点的长度和编码所占用的字节数，并存储在’p’中。如果p为NULL就返回编码这样的长度所需要的字节数量。 12345678910111213141516171819202122232425262728293031323334unsigned int zipEncodeLength(unsigned char *p, unsigned char encoding, unsigned int rawlen) &#123; // len: 需要的字节数量 // buf: 字符串长度存储结构 unsigned char len = 1, buf[5]; if (ZIP_IS_STR(encoding)) &#123; /* \b\b虽然给定了编码，但，所以我们这里使用原始长度来判断编码类型 */ if (rawlen &lt;= 0x3f) &#123; // 长度小于2^6，使用1个字节保存 if (!p) return len; // p为NULL则直接返回1（字节） buf[0] = ZIP_STR_06B | rawlen; &#125; else if (rawlen &lt;= 0x3fff) &#123; // 长度小于2^14，使用2个字节保存 len += 1; if (!p) return len; buf[0] = ZIP_STR_14B | ((rawlen &gt;&gt; 8) &amp; 0x3f); buf[1] = rawlen &amp; 0xff; &#125; else &#123; // 其他情况，使用4个字节保存 len += 4; if (!p) return len; buf[0] = ZIP_STR_32B; buf[1] = (rawlen &gt;&gt; 24) &amp; 0xff; buf[2] = (rawlen &gt;&gt; 16) &amp; 0xff; buf[3] = (rawlen &gt;&gt; 8) &amp; 0xff; buf[4] = rawlen &amp; 0xff; &#125; &#125; else &#123; /* 整数编码，存储结构长度总是1 */ if (!p) return len; buf[0] = encoding; &#125; /* p存储字符串长度存储结构 */ memcpy(p,buf,len); return len; // 返回存储数据的长度需要的字节数量&#125; ZIP_DECODE_LENGTH宏解码ptr中被编码的长度。encoding变量保存了节点的编码，lensize变量保存了编码节点长度所需要的字节数，len变量保存节点长度。 123456789101112131415161718192021222324#define ZIP_DECODE_LENGTH(ptr, encoding, lensize, len) do &#123; \\ ZIP_ENTRY_ENCODING((ptr), (encoding)); \\ if ((encoding) &lt; ZIP_STR_MASK) &#123; \\ if ((encoding) == ZIP_STR_06B) &#123; \\ (lensize) = 1; \\ (len) = (ptr)[0] &amp; 0x3f; \\ &#125; else if ((encoding) == ZIP_STR_14B) &#123; \\ (lensize) = 2; \\ (len) = (((ptr)[0] &amp; 0x3f) &lt;&lt; 8) | (ptr)[1]; \\ &#125; else if (encoding == ZIP_STR_32B) &#123; \\ (lensize) = 5; \\ (len) = ((ptr)[1] &lt;&lt; 24) | \\ ((ptr)[2] &lt;&lt; 16) | \\ ((ptr)[3] &lt;&lt; 8) | \\ ((ptr)[4]); \\ &#125; else &#123; \\ assert(NULL); \\ &#125; \\ &#125; else &#123; \\ /* 整数编码，存储结构长度总是1 */ \\ (lensize) = 1; \\ (len) = zipIntSize(encoding); \\ &#125; \\&#125; while(0); zipPrevEncodeLength函数将p指向的节点的header以len长度进行重新编码，更新prevrawlensize和prevrawlen，len为前一个节点的长度。如果p为NULL就返回存储len长度需要的字节数。 123456789101112131415unsigned int zipPrevEncodeLength(unsigned char *p, unsigned int len) &#123; if (p == NULL) &#123; return (len &lt; ZIP_BIGLEN) ? 1 : sizeof(len)+1; &#125; else &#123; if (len &lt; ZIP_BIGLEN) &#123; p[0] = len; return 1; &#125; else &#123; p[0] = ZIP_BIGLEN; memcpy(p+1,&amp;len,sizeof(len)); memrev32ifbe(p+1); return 1+sizeof(len); &#125; &#125;&#125; zipPrevEncodeLengthForceLarge函数如果p非空，记录编码前一个节点的长度需要的字节数到p中。这个函数只在比较大的encoding时使用（__ziplistCascadeUpdate函数中用到了该函数）。 123456void zipPrevEncodeLengthForceLarge(unsigned char *p, unsigned int len) &#123; if (p == NULL) return; p[0] = ZIP_BIGLEN; memcpy(p+1,&amp;len,sizeof(len)); memrev32ifbe(p+1);&#125; ZIP_DECODE_PREVLENSIZE宏计算ptr指向的节点中存储的上一个节点长度需要的字节数，设置在prevlensize中。 1234567#define ZIP_DECODE_PREVLENSIZE(ptr, prevlensize) do &#123; \\ if ((ptr)[0] &lt; ZIP_BIGLEN) &#123; \\ (prevlensize) = 1; \\ &#125; else &#123; \\ (prevlensize) = 5; \\ &#125; \\&#125; while(0); ZIP_DECODE_PREVLEN宏计算ptr指向的节点的上一个节点的长度（存储在prevlen中）和存储的上一个节点长度需要的字节数（存储在prevlensize中）。 12345678910#define ZIP_DECODE_PREVLEN(ptr, prevlensize, prevlen) do &#123; \\ ZIP_DECODE_PREVLENSIZE(ptr, prevlensize); \\ if ((prevlensize) == 1) &#123; \\ (prevlen) = (ptr)[0]; \\ &#125; else if ((prevlensize) == 5) &#123; \\ assert(sizeof((prevlensize)) == 4); \\ memcpy(&amp;(prevlen), ((char*)(ptr)) + 1, 4); \\ memrev32ifbe(&amp;prevlen); \\ &#125; \\&#125; while(0); zipPrevLenByteDiff函数返回存储长度为len所需的字节数和存储p指向的节点的上一个节点的长度所需的字节数之差（字节）。 12345int zipPrevLenByteDiff(unsigned char *p, unsigned int len) &#123; unsigned int prevlensize; ZIP_DECODE_PREVLENSIZE(p, prevlensize); // 计算保存p指向节点的上一个节点的长度所需的字节数 return zipPrevEncodeLength(NULL, len) - prevlensize;&#125; zipRawEntryLength函数返回p指向的节点所占用的字节数。 1234567unsigned int zipRawEntryLength(unsigned char *p) &#123; // unsigned int prevlensize, encoding, lensize, len; ZIP_DECODE_PREVLENSIZE(p, prevlensize); ZIP_DECODE_LENGTH(p + prevlensize, encoding, lensize, len); return prevlensize + lensize + len;&#125; zipTryEncoding函数检查entry指向的字符串能否被编码成一个整型，能返回1，不能返回0。并保存这个整数在v中，保存这个整数的编码在encoding中。 123456789101112131415161718192021222324int zipTryEncoding(unsigned char *entry, unsigned int entrylen, long long *v, unsigned char *encoding) &#123; long long value; if (entrylen &gt;= 32 || entrylen == 0) return 0; // entry长度大于32位或者为0都不能被编码为一个整型 if (string2ll((char*)entry,entrylen,&amp;value)) &#123; /* 这个字符串可以被编码。判断能够编码它的最小编码类型。 */ if (value &gt;= 0 &amp;&amp; value &lt;= 12) &#123; *encoding = ZIP_INT_IMM_MIN+value; &#125; else if (value &gt;= INT8_MIN &amp;&amp; value &lt;= INT8_MAX) &#123; *encoding = ZIP_INT_8B; &#125; else if (value &gt;= INT16_MIN &amp;&amp; value &lt;= INT16_MAX) &#123; *encoding = ZIP_INT_16B; &#125; else if (value &gt;= INT24_MIN &amp;&amp; value &lt;= INT24_MAX) &#123; *encoding = ZIP_INT_24B; &#125; else if (value &gt;= INT32_MIN &amp;&amp; value &lt;= INT32_MAX) &#123; *encoding = ZIP_INT_32B; &#125; else &#123; *encoding = ZIP_INT_64B; &#125; *v = value; return 1; &#125; return 0;&#125; zipSaveInteger函数把value的值保存在p指向的节点中，其中编码类型为encoding。 12345678910111213141516171819202122232425262728void zipSaveInteger(unsigned char *p, int64_t value, unsigned char encoding) &#123; int16_t i16; int32_t i32; int64_t i64; if (encoding == ZIP_INT_8B) &#123; ((int8_t*)p)[0] = (int8_t)value; &#125; else if (encoding == ZIP_INT_16B) &#123; i16 = value; memcpy(p,&amp;i16,sizeof(i16)); memrev16ifbe(p); &#125; else if (encoding == ZIP_INT_24B) &#123; i32 = value&lt;&lt;8; memrev32ifbe(&amp;i32); memcpy(p,((uint8_t*)&amp;i32)+1,sizeof(i32)-sizeof(uint8_t)); &#125; else if (encoding == ZIP_INT_32B) &#123; i32 = value; memcpy(p,&amp;i32,sizeof(i32)); memrev32ifbe(p); &#125; else if (encoding == ZIP_INT_64B) &#123; i64 = value; memcpy(p,&amp;i64,sizeof(i64)); memrev64ifbe(p); &#125; else if (encoding &gt;= ZIP_INT_IMM_MIN &amp;&amp; encoding &lt;= ZIP_INT_IMM_MAX) &#123; /* 值直接保存在编码本身中，什么也不做。 */ &#125; else &#123; assert(NULL); &#125;&#125; zipLoadInteger函数返回p指向的节点中以encoding为编码的整型数。 123456789101112131415161718192021222324252627282930int64_t zipLoadInteger(unsigned char *p, unsigned char encoding) &#123; int16_t i16; int32_t i32; int64_t i64, ret = 0; if (encoding == ZIP_INT_8B) &#123; ret = ((int8_t*)p)[0]; &#125; else if (encoding == ZIP_INT_16B) &#123; memcpy(&amp;i16,p,sizeof(i16)); memrev16ifbe(&amp;i16); ret = i16; &#125; else if (encoding == ZIP_INT_32B) &#123; memcpy(&amp;i32,p,sizeof(i32)); memrev32ifbe(&amp;i32); ret = i32; &#125; else if (encoding == ZIP_INT_24B) &#123; i32 = 0; memcpy(((uint8_t*)&amp;i32)+1,p,sizeof(i32)-sizeof(uint8_t)); memrev32ifbe(&amp;i32); ret = i32&gt;&gt;8; &#125; else if (encoding == ZIP_INT_64B) &#123; memcpy(&amp;i64,p,sizeof(i64)); memrev64ifbe(&amp;i64); ret = i64; &#125; else if (encoding &gt;= ZIP_INT_IMM_MIN &amp;&amp; encoding &lt;= ZIP_INT_IMM_MAX) &#123; ret = (encoding &amp; ZIP_INT_IMM_MASK)-1; &#125; else &#123; assert(NULL); &#125; return ret;&#125; zipEntry函数将p指向的节点的所有信息存储在压缩链表节点e中。 1234567void zipEntry(unsigned char *p, zlentry *e) &#123; ZIP_DECODE_PREVLEN(p, e-&gt;prevrawlensize, e-&gt;prevrawlen); ZIP_DECODE_LENGTH(p + e-&gt;prevrawlensize, e-&gt;encoding, e-&gt;lensize, e-&gt;len); e-&gt;headersize = e-&gt;prevrawlensize + e-&gt;lensize; e-&gt;p = p;&#125; ziplistNew函数创建一个空的压缩链表。 123456789unsigned char *ziplistNew(void) &#123; unsigned int bytes = ZIPLIST_HEADER_SIZE+1; // 空压缩链表占用的总字节数 = 压缩链表头部大小 + ZIP_END大小（等于1） unsigned char *zl = zmalloc(bytes); // 为压缩链表分配空间 ZIPLIST_BYTES(zl) = intrev32ifbe(bytes); // 填充压缩链表占用的总字节数数据域 ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(ZIPLIST_HEADER_SIZE); // // 填充压缩链表最后一个节点的偏移量数据域 ZIPLIST_LENGTH(zl) = 0; // 填充压缩链表节点数量数据域 zl[bytes-1] = ZIP_END; // 填充压缩链表ZIP_END数据域（ZIP_END在压缩链表的最后一个字节） return zl;&#125; ziplistResize函数调整指定的压缩链表大小。 123456unsigned char *ziplistResize(unsigned char *zl, unsigned int len) &#123; zl = zrealloc(zl,len); // realloc空间 ZIPLIST_BYTES(zl) = intrev32ifbe(len); // 填充压缩链表占用的总字节数数据域 zl[len-1] = ZIP_END; // 填充压缩链表ZIP_END数据域（ZIP_END在压缩链表的最后一个字节） return zl;&#125; __ziplistCascadeUpdate函数执行压缩链表级联更新。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465unsigned char *__ziplistCascadeUpdate(unsigned char *zl, unsigned char *p) &#123; // curlen: 当前压缩链表占用的字节数，rawlen: 节点长度，rawlensize: 保存节点长度需要的字节数 size_t curlen = intrev32ifbe(ZIPLIST_BYTES(zl)), rawlen, rawlensize; size_t offset, noffset, extra; unsigned char *np; zlentry cur, next; while (p[0] != ZIP_END) &#123; // p指向当前节点的首地址 zipEntry(p, &amp;cur); // 将p指向的节点信息初始化到cur指向的zipEntry中 rawlen = cur.headersize + cur.len; // 节点总长度 = 节点头长度 + 当前节点长度 rawlensize = zipPrevEncodeLength(NULL,rawlen); // 计算存储当前节点的长度需要的字节数 /* 如果是最后一个节点，则跳出while循环。 */ if (p[rawlen] == ZIP_END) break; zipEntry(p+rawlen, &amp;next); // p+rawlen为下一个节点的首地址，初始化next为下一个节点 /* 对next节点来说，如果上一个节点（就是cur）的长度没有改变，就不做任何操作。 */ if (next.prevrawlen == rawlen) break; if (next.prevrawlensize &lt; rawlensize) &#123; /* next节点的上一个节点的长度所占的字节数next.prevrawlensize * 小于存储当前节点的长度需要的字节数时，需要扩容 */ offset = p-zl; // 当前节点相对于压缩链表首地址的偏移量 extra = rawlensize-next.prevrawlensize; // 额外需要的字节数 = 存储当前节点的长度需要的字节数（刚计算得出）- next中存储上一个节点的长度所占的字节数 zl = ziplistResize(zl,curlen+extra); // 调整压缩链表长度 p = zl+offset; // 当前节点指针 np = p+rawlen; // next节点新地址 noffset = np-zl; // next节点的偏移量 /* 更新ziplist最后一个节点偏移量，如果next节点是尾部节点就不做更新。 */ if ((zl+intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl))) != np) &#123; ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl))+extra); &#125; /* 移动next节点到新地址，为当前节点cur空出空间。 * */ memmove(np+rawlensize, np+next.prevrawlensize, curlen-noffset-next.prevrawlensize-1); // 将next节点的header以rawlen长度进行重新编码，更新prevrawlensize和prevrawlen zipPrevEncodeLength(np,rawlen); /* 更新当前节点指针 */ p += rawlen; // 指向下一个节点 curlen += extra; // 更新压缩链表占用的总字节数 &#125; else &#123; if (next.prevrawlensize &gt; rawlensize) &#123; /* next节点的上一个节点的长度所占的字节数next.prevrawlensize, * 小于存储当前节点的长度需要的字节数时，这意味着next节点编码前置节点的 * header空间有5字节，而编码rawlen只需要1字节，需要缩容。但应该尽量避免这么做。 * 所以我们用5字节的空间将1字节的编码重新编码 */ zipPrevEncodeLengthForceLarge(p+rawlen,rawlen); &#125; else &#123; // 说明next.prevrawlensize = rawlensize，只需要更新next节点的header zipPrevEncodeLength(p+rawlen,rawlen); &#125; /* Stop here, as the raw length of \"next\" has not changed. */ break; &#125; &#125; return zl;&#125; __ziplistDelete函数从p指向的节点开始，删除num个节点。返回ziplist的指针。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556unsigned char *__ziplistDelete(unsigned char *zl, unsigned char *p, unsigned int num) &#123; unsigned int i, totlen, deleted = 0; size_t offset; int nextdiff = 0; zlentry first, tail; zipEntry(p, &amp;first); // p指向第一个要删除的节点 for (i = 0; p[0] != ZIP_END &amp;&amp; i &lt; num; i++) &#123; // 从p开始遍历num个节点（如果有这么多），统计要删除的节点数量 p += zipRawEntryLength(p); // zipRawEntryLength(p)返回p指向的节点所占用的字节数 deleted++; &#125; totlen = p-first.p; // 总的删除长度 if (totlen &gt; 0) &#123; if (p[0] != ZIP_END) &#123; /* \b如果被删除的最后一个节点不是压缩链表的最后一个节点，说明它后面还有节点A。 * A节点的header部分的大小可能无法容纳新的前置节点B（被删除的第一个节点的前置节点） * 所以这里需要计算这里面的差值。 */ // first.prevrawlen为被删除的第一个节点的前置节点的长度 // p指向被删除的最后一个节点的后置节点 nextdiff = zipPrevLenByteDiff(p,first.prevrawlen); // 差值 p -= nextdiff; // 更新p的指针 zipPrevEncodeLength(p,first.prevrawlen); // 更新被删除的最后一个节点的后置节点的prevrawlensize和prevrawlen /* 更新表尾偏移量，新的表尾偏移量 = 当前表尾偏移量 - 删除的长度 */ ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl))-totlen); zipEntry(p, &amp;tail); // tail为最后一个删除节点的后置节点 // 当被删除的最后一个节点后面有多于一个的节点，需要更新ziplist表尾偏移量，加上修正值 if (p[tail.headersize+tail.len] != ZIP_END) &#123; ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl))+nextdiff); &#125; /* 把tail节点之后的数据移动到被删除的第一个节点的位置 */ memmove(first.p,p, intrev32ifbe(ZIPLIST_BYTES(zl))-(p-zl)-1); &#125; else &#123; /* 把p指向的节点和其后面的所有节点都删除了，无须移动数据，只需要更新ziplist表尾偏移量 */ ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe((first.p-zl)-first.prevrawlen); &#125; offset = first.p-zl; // \b节点结合处偏移量 zl = ziplistResize(zl, intrev32ifbe(ZIPLIST_BYTES(zl))-totlen+nextdiff); // 调整ziplist大小 ZIPLIST_INCR_LENGTH(zl,-deleted); // 调整ziplist节点数量 p = zl+offset; // 节点结合处指针 /* 当nextdiff != 0时，结合处的节点将发生变化（前置节点长度prevrawlen会改变）， * 这里我们需要级联更新ziplist。 */ if (nextdiff != 0) zl = __ziplistCascadeUpdate(zl,p); &#125; return zl;&#125; __ziplistInsert函数在p指向的地方插入元素，元素值为s，值长度为slen。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899unsigned char *__ziplistInsert(unsigned char *zl, unsigned char *p, unsigned char *s, unsigned int slen) &#123; // curlen: 压缩链表占用的字节长度 size_t curlen = intrev32ifbe(ZIPLIST_BYTES(zl)), reqlen; // prevlensize: 保存插入位置处节点的前置节点len所需的字节数，prevlen: 插入位置处节点的前置节点长度 unsigned int prevlensize, prevlen = 0; size_t offset; int nextdiff = 0; unsigned char encoding = 0; long long value = 123456789; /* initialized to avoid warning. Using a value that is easy to see if for some reason we use it uninitialized. */ zlentry tail; /* 找出插入位置的前置节点的长度 */ if (p[0] != ZIP_END) &#123; // 获取插入位置处节点的前置节点长度len所需的字节数和前置节点的长度 ZIP_DECODE_PREVLEN(p, prevlensize, prevlen); &#125; else &#123; // 插入位置为链表尾 unsigned char *ptail = ZIPLIST_ENTRY_TAIL(zl); // ptail为尾节点指针 if (ptail[0] != ZIP_END) &#123; prevlen = zipRawEntryLength(ptail); // 计算尾节点的前置节点的长度 &#125; &#125; /* 检查节点是否可以被编码，并判断编码类型 */ if (zipTryEncoding(s,slen,&amp;value,&amp;encoding)) &#123; /* zipIntSize(encoding)返回编码指定类型整数需要的空间大小 */ reqlen = zipIntSize(encoding); &#125; else &#123; /* 无法用一个整数编码，使用字符串编码，编码长度为入参slen。 */ reqlen = slen; &#125; /* 计算保存前置节点长度需要的空间和保存值需要的空间大小。 */ reqlen += zipPrevEncodeLength(NULL,prevlen); reqlen += zipEncodeLength(NULL,encoding,slen); /* 当不是在链表尾插入时，我们需要保证插入位置的后置节点的空间能够保存这个 * 被插入节点的长度。 */ int forcelarge = 0; nextdiff = (p[0] != ZIP_END) ? zipPrevLenByteDiff(p,reqlen) : 0; // 计算空间差值 if (nextdiff == -4 &amp;&amp; reqlen &lt; 4) &#123; nextdiff = 0; forcelarge = 1; &#125; /* 保存插入位置的偏移量，因为realloc调用有可能会改变ziplist的地址。 */ offset = p-zl; zl = ziplistResize(zl,curlen+reqlen+nextdiff); // ziplist调整大小 p = zl+offset; // 更新插入位置指针 /* Apply memory move when necessary and update tail offset. */ if (p[0] != ZIP_END) &#123; // 不是在链表尾插入 /* 新节点长度为reqlen，将新节点后面的节点都移动到新的位置。 */ memmove(p+reqlen,p-nextdiff,curlen-offset-1+nextdiff); /* 在新节点的后置节点中更新前置节点的信息。 */ if (forcelarge) zipPrevEncodeLengthForceLarge(p+reqlen,reqlen); else zipPrevEncodeLength(p+reqlen,reqlen); /* 更新尾节点偏移量，直接在原来的基础加上新节点长度即可 */ ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl))+reqlen); /* When the tail contains more than one entry, we need to take * \"nextdiff\" in account as well. Otherwise, a change in the * size of prevlen doesn't have an effect on the *tail* offset. */ zipEntry(p+reqlen, &amp;tail); // tail为新节点的后置节点 // 当新节点的后面有多于一个的节点，需要更新ziplist表尾偏移量，加上修正值 if (p[reqlen+tail.headersize+tail.len] != ZIP_END) &#123; ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl))+nextdiff); &#125; &#125; else &#123; // 在链表尾插入，新节点成为新的尾节点，更新尾节点偏移量。 ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(p-zl); &#125; /* 当nextdiff != 0时，结合处的节点将发生变化（前置节点长度prevrawlen会改变）， * 这里我们需要级联更新ziplist。 */ if (nextdiff != 0) &#123; offset = p-zl; zl = __ziplistCascadeUpdate(zl,p+reqlen); p = zl+offset; &#125; /* 真正插入节点 */ p += zipPrevEncodeLength(p,prevlen); // 新节点存储前置节点长度需要的字节数 p += zipEncodeLength(p,encoding,slen); // 新节点编码长度为slen的数据所需要的字节数量 if (ZIP_IS_STR(encoding)) &#123; // 字符串型数据 memcpy(p,s,slen); // 拷贝数据s到指定位置 &#125; else &#123; zipSaveInteger(p,value,encoding); // 保存整数 &#125; ZIPLIST_INCR_LENGTH(zl,1); // 更新链表节点数量 return zl;&#125; ziplistMerge函数合并两个ziplist，把第一个ziplist和第二个ziplist首尾相连。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101unsigned char *ziplistMerge(unsigned char **first, unsigned char **second) &#123; /* 如果所有参数都是NULL，无须合并，直接返回NULL。 */ if (first == NULL || *first == NULL || second == NULL || *second == NULL) return NULL; /* 如果两个ziplist是同一个，也无法合并。 */ if (*first == *second) return NULL; // 第1个ziplist占用的空间大小和节点数量 size_t first_bytes = intrev32ifbe(ZIPLIST_BYTES(*first)); size_t first_len = intrev16ifbe(ZIPLIST_LENGTH(*first)); // 第2个ziplist占用的空间大小和节点数量 size_t second_bytes = intrev32ifbe(ZIPLIST_BYTES(*second)); size_t second_len = intrev16ifbe(ZIPLIST_LENGTH(*second)); int append; unsigned char *source, *target; size_t target_bytes, source_bytes; /* 选择比较大的那个ziplist，这样直接就地扩容比较容易。 * */ if (first_len &gt;= second_len) &#123; /* 以第一个ziplist为target，把第二个ziplist追加到它后面。 */ target = *first; target_bytes = first_bytes; source = *second; source_bytes = second_bytes; append = 1; // 后向追加 &#125; else &#123; /* 以第二个ziplist为target，把第一个ziplist前向追加到它上面。 */ target = *second; target_bytes = second_bytes; source = *first; source_bytes = first_bytes; append = 0; // 前向追加 &#125; /* 计算合并后的ziplist占用的空间大小，需要扣除其中一个ziplist的元数据（zip_header和zip_end）的大小 */ size_t zlbytes = first_bytes + second_bytes - ZIPLIST_HEADER_SIZE - ZIPLIST_END_SIZE; size_t zllength = first_len + second_len; // 合并后的ziplist节点数量 /* 合并后的ziplist节点数量必须限制在UINT16_MAX之内 */ zllength = zllength &lt; UINT16_MAX ? zllength : UINT16_MAX; /* 在操作内存之前先保存两个ziplist的尾节点偏移量。 */ size_t first_offset = intrev32ifbe(ZIPLIST_TAIL_OFFSET(*first)); size_t second_offset = intrev32ifbe(ZIPLIST_TAIL_OFFSET(*second)); /* realloc目标ziplist的空间。 */ target = zrealloc(target, zlbytes); if (append) &#123; /* append == appending to target */ /* Copy source after target (copying over original [END]): * [TARGET - END, SOURCE - HEADER] */ /* target = ziplist_1 &lt;- ziplist_2 */ memcpy(target + target_bytes - ZIPLIST_END_SIZE, source + ZIPLIST_HEADER_SIZE, source_bytes - ZIPLIST_HEADER_SIZE); &#125; else &#123; /* !append == prepending to target */ /* Move target *contents* exactly size of (source - [END]), * then copy source into vacataed space (source - [END]): * [SOURCE - END, TARGET - HEADER] */ /* target = ziplist_1 -&gt; ziplist_2 */ memmove(target + source_bytes - ZIPLIST_END_SIZE, target + ZIPLIST_HEADER_SIZE, target_bytes - ZIPLIST_HEADER_SIZE); memcpy(target, source, source_bytes - ZIPLIST_END_SIZE); &#125; /* 更新目标ziplist header元数据 */ ZIPLIST_BYTES(target) = intrev32ifbe(zlbytes); // 更新目标ziplist占用的字节数 ZIPLIST_LENGTH(target) = intrev16ifbe(zllength); // 更新目标ziplist节点数量 /* 新的尾节点偏移量计算方式： * + N 字节：第一个ziplist的总字节数 * - 1 字节：第一个ziplist的ZIP_END * + M 字节：第二个ziplist原来的尾节点偏移量 * - J 字节：第二个ziplist的header的字节数 */ ZIPLIST_TAIL_OFFSET(target) = intrev32ifbe( (first_bytes - ZIPLIST_END_SIZE) + (second_offset - ZIPLIST_HEADER_SIZE)); /* 在接合处级联更新目标ziplist */ target = __ziplistCascadeUpdate(target, target+first_offset); /* Now free and NULL out what we didn't realloc */ if (append) &#123; // target = ziplist_1 &lt;- ziplist_2，释放第二个ziplist的空间，并更新ziplist指针 zfree(*second); *second = NULL; *first = target; &#125; else &#123; // target = ziplist_1 -&gt; ziplist_2，释放第一个ziplist的空间，并更新ziplist指针 zfree(*first); *first = NULL; *second = target; &#125; return target;&#125; ziplistPush函数向ziplist中插入元素，只能在首尾添加。 12345unsigned char *ziplistPush(unsigned char *zl, unsigned char *s, unsigned int slen, int where) &#123; unsigned char *p; p = (where == ZIPLIST_HEAD) ? ZIPLIST_ENTRY_HEAD(zl) : ZIPLIST_ENTRY_END(zl); // 获取插入位置指针 return __ziplistInsert(zl,p,s,slen);&#125; ziplistIndex函数根据给定的索引值返回一个节点的指针。当给定的索引值为负时，从后向前遍历。当链表在给定的索引值上没有节点时返回NULL。 1234567891011121314151617181920212223unsigned char *ziplistIndex(unsigned char *zl, int index) &#123; unsigned char *p; unsigned int prevlensize, prevlen = 0; if (index &lt; 0) &#123; // 从后向前遍历链表 index = (-index)-1; p = ZIPLIST_ENTRY_TAIL(zl); // 尾节点指针 if (p[0] != ZIP_END) &#123; ZIP_DECODE_PREVLEN(p, prevlensize, prevlen); while (prevlen &gt; 0 &amp;&amp; index--) &#123; p -= prevlen; ZIP_DECODE_PREVLEN(p, prevlensize, prevlen); &#125; &#125; &#125; else &#123; // 从前向后遍历链表 p = ZIPLIST_ENTRY_HEAD(zl); // 头节点指针 while (p[0] != ZIP_END &amp;&amp; index--) &#123; p += zipRawEntryLength(p); &#125; &#125; return (p[0] == ZIP_END || index &gt; 0) ? NULL : p; // 没有找到相应的节点，返回NULL，否则返回这个节点的指针&#125; ziplistNext函数返回ziplist中当前节点的后置节点指针，如果当前节点是尾节点则返回NULL。 1234567891011121314151617/* 返回ziplist中当前节点的后置节点指针，如果当前节点是尾节点则返回NULL。 */unsigned char *ziplistNext(unsigned char *zl, unsigned char *p) &#123; ((void) zl); /* 由于调用ziplistDelete函数，p有可能等于ZIP_END， * 这时应该返回NULL。否则，当后置节点为ZIP_END时返回NULL。 */ if (p[0] == ZIP_END) &#123; return NULL; &#125; p += zipRawEntryLength(p); // p加上当前节点长度为它的后置节点的地址 if (p[0] == ZIP_END) &#123; return NULL; &#125; return p;&#125; ziplistPrev函数返回ziplist当前节点的前置节点指针。 123456789101112131415161718unsigned char *ziplistPrev(unsigned char *zl, unsigned char *p) &#123; unsigned int prevlensize, prevlen = 0; /* 从ZIP_END开始向前迭代会返回尾节点。当p指向链表头节点时，返回NULL。 */ if (p[0] == ZIP_END) &#123; // p指向ZIP_END时，返回链表尾节点 p = ZIPLIST_ENTRY_TAIL(zl); return (p[0] == ZIP_END) ? NULL : p; &#125; else if (p == ZIPLIST_ENTRY_HEAD(zl)) &#123; // p指向链表头节点时，返回NULL return NULL; &#125; else &#123; // 获得p指向节点的前置节点长度，p减该长度即为当前节点前置节点 ZIP_DECODE_PREVLEN(p, prevlensize, prevlen); assert(prevlen &gt; 0); return p-prevlen; &#125;&#125; ziplistGet函数获取p指向的节点的数据，根据其编码决定数据保存在sstr（字符串）还是sval（整数）中。\\sstr刚开始总是被设置为NULL。当p指向ziplist的尾部（ZIP_END）时返回0，否则返回1。 123456789101112131415161718unsigned int ziplistGet(unsigned char *p, unsigned char **sstr, unsigned int *slen, long long *sval) &#123; zlentry entry; if (p == NULL || p[0] == ZIP_END) return 0; if (sstr) *sstr = NULL; zipEntry(p, &amp;entry); // 初始化entry为当前节点 if (ZIP_IS_STR(entry.encoding)) &#123; // 当前节点为字符串，数据保存在*sstr if (sstr) &#123; *slen = entry.len; *sstr = p+entry.headersize; &#125; &#125; else &#123; // 当前节点为整数，数据保存在*sval if (sval) &#123; *sval = zipLoadInteger(p+entry.headersize,entry.encoding); &#125; &#125; return 1;&#125; ziplistInsert函数向ziplist中p指向的节点处插入一个节点。 123unsigned char *ziplistInsert(unsigned char *zl, unsigned char *p, unsigned char *s, unsigned int slen) &#123; return __ziplistInsert(zl,p,s,slen);&#125; ziplistDelete函数从ziplist中删除p指向的节点。还就地更新了*p，以使得在删除节点的时候还能迭代ziplist。 12345678unsigned char *ziplistDelete(unsigned char *zl, unsigned char **p) &#123; size_t offset = *p-zl; // 当前节点的偏移量 zl = __ziplistDelete(zl,*p,1); // 从ziplist中删除当前节点，由于ziplistDelete会调用realloc，zl有可能会发生变化 /* 事先在p中保存当前元素的指针，因为ziplistDelete会调用realloc，有可能会导致zl指针发生变化。 */ *p = zl+offset; // 更新了*p，此时*p指向的是被删除节点的后置节点，可以继续使用这个指针进行迭代。 return zl;&#125; ziplistDeleteRange函数删除ziplist中一个范围内的节点。 1234unsigned char *ziplistDeleteRange(unsigned char *zl, int index, unsigned int num) &#123; unsigned char *p = ziplistIndex(zl,index); return (p == NULL) ? zl : __ziplistDelete(zl,p,num);&#125; ziplistCompare函数比较p指向节点的值和sstr指向的长度为slen的数据，当相等时返回1，否则返回0。 1234567891011121314151617181920212223unsigned int ziplistCompare(unsigned char *p, unsigned char *sstr, unsigned int slen) &#123; zlentry entry; unsigned char sencoding; long long zval, sval; if (p[0] == ZIP_END) return 0; zipEntry(p, &amp;entry); // entry为p指向的节点 if (ZIP_IS_STR(entry.encoding)) &#123; /* entry的值是字符串 */ if (entry.len == slen) &#123; return memcmp(p+entry.headersize,sstr,slen) == 0; &#125; else &#123; return 0; &#125; &#125; else &#123; /* entry的值是整数，此时不比较编码类型，因为不同编码类型的位数不同，只比较值是否相等。 */ if (zipTryEncoding(sstr,slen,&amp;sval,&amp;sencoding)) &#123; zval = zipLoadInteger(p+entry.headersize,entry.encoding); return zval == sval; &#125; &#125; return 0;&#125; ziplistFind函数在ziplist中查找与指定节点相等的节点。每次比较后跳过skip个节点。没有找到相应节点时返回NULL。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758unsigned char *ziplistFind(unsigned char *p, unsigned char *vstr, unsigned int vlen, unsigned int skip) &#123; int skipcnt = 0; // 已经跳过的节点数 unsigned char vencoding = 0; long long vll = 0; while (p[0] != ZIP_END) &#123; unsigned int prevlensize, encoding, lensize, len; unsigned char *q; ZIP_DECODE_PREVLENSIZE(p, prevlensize); // 保存当前节点的前置节点长度所需的字节数 ZIP_DECODE_LENGTH(p + prevlensize, encoding, lensize, len); // 获取当前节点的encoding、lensize和len q = p + prevlensize + lensize; // 当前节点value域指针 if (skipcnt == 0) &#123; /* 比较当前节点和给定节点的值 */ if (ZIP_IS_STR(encoding)) &#123; // 当前节点的值为字符串 if (len == vlen &amp;&amp; memcmp(q, vstr, vlen) == 0) &#123; return p; &#125; &#125; else &#123; /* 判断vstr指向的数据能否被编码成整数，这个操作只做一次， * 一旦判定为可以被编码成整数，vencoding被设置为非0值且vll被设置成一对应的整数。 * 如果不能，vencoding被设置为UCHAR_MAX。 */ if (vencoding == 0) &#123; if (!zipTryEncoding(vstr, vlen, &amp;vll, &amp;vencoding)) &#123; /* If the entry can't be encoded we set it to * UCHAR_MAX so that we don't retry again the next * time. */ vencoding = UCHAR_MAX; &#125; /* Must be non-zero by now */ assert(vencoding); &#125; /* 只有当vencoding != UCHAR_MAX时才能以整数比较当前节点和给定节点的值。 */ if (vencoding != UCHAR_MAX) &#123; // vstr指向的值可以被以整数编码 long long ll = zipLoadInteger(q, encoding); if (ll == vll) &#123; return p; &#125; &#125; &#125; /* Reset skip count */ skipcnt = skip; &#125; else &#123; /* Skip entry */ skipcnt--; &#125; /* 移动到下个节点 */ p = q + len; &#125; return NULL;&#125; ziplistLen函数返回ziplist的节点数量。 1234567891011121314151617unsigned int ziplistLen(unsigned char *zl) &#123; unsigned int len = 0; if (intrev16ifbe(ZIPLIST_LENGTH(zl)) &lt; UINT16_MAX) &#123; // 如果ziplist的节点数量小于UINT16_MAX，直接取ziplist header中存放的节点数量 len = intrev16ifbe(ZIPLIST_LENGTH(zl)); &#125; else &#123; unsigned char *p = zl+ZIPLIST_HEADER_SIZE; // ziplist头节点指针 while (*p != ZIP_END) &#123; // 遍历ziplist计算节点数量 p += zipRawEntryLength(p); len++; &#125; /* 如果实际计算出来的长度小于UINT16_MAX，更新ziplist header中的节点数量 */ if (len &lt; UINT16_MAX) ZIPLIST_LENGTH(zl) = intrev16ifbe(len); &#125; return len;&#125; ziplistBlobLen函数获取链表占用的总字节数。 123size_t ziplistBlobLen(unsigned char *zl) &#123; return intrev32ifbe(ZIPLIST_BYTES(zl));&#125;","categories":[{"name":"源码分析","slug":"源码分析","permalink":"https://nullcc.github.io/categories/源码分析/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://nullcc.github.io/tags/Redis/"},{"name":"数据结构","slug":"数据结构","permalink":"https://nullcc.github.io/tags/数据结构/"}]},{"title":"Redis中的底层数据结构(3)——字典(dict)","slug":"Redis中的底层数据结构(3)——字典(dict)","date":"2017-11-14T16:00:00.000Z","updated":"2022-04-15T03:41:13.019Z","comments":true,"path":"2017/11/15/Redis中的底层数据结构(3)——字典(dict)/","link":"","permalink":"https://nullcc.github.io/2017/11/15/Redis中的底层数据结构(3)——字典(dict)/","excerpt":"本文将详细说明Redis中字典的实现。 在Redis源码（这里使用3.2.11版本）中，字典的实现在dict.h和dict.c中。","text":"本文将详细说明Redis中字典的实现。 在Redis源码（这里使用3.2.11版本）中，字典的实现在dict.h和dict.c中。 Redis字典概述1.Redis字典的数据结构示意图： 2.Redis字典概述dict的type属性是一个dictType类型的结构体指针，dictType结构体包含了字典的哈希函数、复制key、复制value、销毁key、销毁value和比较两个key的函数指针。ht[2]维护了两个dictht，ht[0]和ht[1]。ht[0]主要负责保存字典哈希表的散列数组内容，即一堆dictEntry。ht[1]用于rehash，rehash的内容在之后会详细说明。dictht的table是一个dictEntry指针的数组，这里面的每一个元素都是一个dictEntry，当有新的key-value时，先用哈希函数计算散列值，再将散列值映射到散列数组中，然后用dictEntry包装key-value pair，放入散列数组相应的槽中，如果有多个key的散列值相同，它们将在散列数组中位于同一个槽中，相邻的dictEntry的next指针链接起来，形成一个dictEntry链表，这个链表的头节点保存在散列数组的相应槽中。 3.Redis字典的哈希函数dict.c源码中提供了三种哈希函数，分别是： 1). dictIntHashFunction: Thomas Wang’s 32 bit Mix哈希算法，对一个无符号整型数进行一系列的移位运算，效率较高。2). dictGenHashFunction: Austin Appleby的MurmurHash2算法。3). dictGenCaseHashFunction: 一个对大小写不敏感的哈希函数（基于djb哈希算法）。有兴趣的同学可以直接阅读dict.c的hash functions部分。 4.Rehash过程由于一个字典哈希表的散列数组具有一个初始的大小，这在dict.h中有定义：1#define DICT_HT_INITIAL_SIZE 4 之后如果向该字典中添加更多的key-value pair，就需要扩充散列数组的大小，另外如果一个字典原来有很多key，之后又删除了一部分key，为了节省内存，也会对该字典进行缩容。这就需要进行rehash。rehash将字典原来的key重新计算散列值并映射到一个新的散列数组（大小发生改变）上。rehash赋予字典动态扩容/缩容的能力。 Redis处于非rehash时，字典中的key都保存在ht[0]的散列数组中，当某一时刻需要进行rehash时，会在ht[1]上扩大或缩小散列数组的大小，接着把ht[0]里的所有key重新计算散列值并映射到ht[1]的散列数组中。当完成所有key的rehash后，将ht[0]和ht[1]对调，原来的ht[1]经过rehash成为了新的ht[0]。 Redis的rehash还有一个值得注意的特点，由于rehash可能比较耗时，导致Redis无法处理其他事情，因此Redis不会一次性一个字典做全量rehash，而是把rehash操作分摊到很多时间点上，比如在字典中查找、新增、删除key-value pair时，执行一步rehash过程，这可以称为增量rehash。在后面列出的代码中将会看到这是如何实现的。 字典中的数据结构以下是dict.h中的字典相关的结构体代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/* 保存key-value对的结构体 */typedef struct dictEntry &#123; void *key; // 字典键 union &#123; // value是一个联合，只能存放下列类型值的其中一个 void *val; // 空类型指针一枚 uint64_t u64; // 无符号整型一枚 int64_t s64; // 有符号整型一枚 double d; // 双精度浮点数一枚 &#125; v; struct dictEntry *next; // 指向下一个键值对节点的指针&#125; dictEntry;/* 字典操作的方法 */typedef struct dictType &#123; unsigned int (*hashFunction)(const void *key); // 哈希函数指针，使用key来计算哈希值 void *(*keyDup)(void *privdata, const void *key); // 复制key的函数指针 void *(*valDup)(void *privdata, const void *obj); // 复制value的函数指针 int (*keyCompare)(void *privdata, const void *key1, const void *key2); // 比较两个key的函数指针 void (*keyDestructor)(void *privdata, void *key); // 销毁key的函数指针 void (*valDestructor)(void *privdata, void *obj); // 销毁value的函数指针&#125; dictType;/* 这是我们的哈希表结构。每个字典都有两个这样的结构，因为 * 我们实现了从旧哈希表迁移数据到新哈希表的增量rehash。*//* 哈希表结构 */typedef struct dictht &#123; dictEntry **table; // 散列数组 unsigned long size; // 散列数组长度 unsigned long sizemask; // 散列数组长度掩码 = 散列数组长度-1 unsigned long used; // 散列数组中已经被使用的节点数量&#125; dictht;/* 字典结构 */typedef struct dict &#123; dictType *type; // 字典类型 void *privdata; // 私有数据 dictht ht[2]; // 一个字典中有两个哈希表，原因如上述 long rehashidx; // 数据rehash的当前索引位置 int iterators; // 当前使用的迭代器数量&#125; dict;/* 如果safe被设置成1则表示这是一个安全的迭代器，这意味着你可以在迭代字典时调用 * dictAdd、dictFind等一些函数。否则这是一个不安全的迭代器，只能在迭代时调用 * dictNext()函数。*//* 字典迭代器 */typedef struct dictIterator &#123; dict *d; // 字典指针 long index; // 散列数组的当前索引值 int table, safe; // 哈希表编号（0／1）和安全标志 dictEntry *entry, *nextEntry; // 当前键值对结构体指针，下一个键值对结构体指针 long long fingerprint; // 字典的指纹&#125; dictIterator; 以下是dict.h中的函数原型，宏部分只列出了DICT_HT_INITIAL_SIZE： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/* 散列数组的初始大小 */#define DICT_HT_INITIAL_SIZE 4#define dictHashKey(d, key) (d)-&gt;type-&gt;hashFunction(key) // 获取指定key的哈希值#define dictGetKey(he) ((he)-&gt;key) // 获取指定节点的key#define dictGetVal(he) ((he)-&gt;v.val) // 获取指定节点的value#define dictGetSignedIntegerVal(he) ((he)-&gt;v.s64) // 获取指定节点的value，值为signed int#define dictGetUnsignedIntegerVal(he) ((he)-&gt;v.u64) // 获取指定节点的value，值为unsigned int#define dictGetDoubleVal(he) ((he)-&gt;v.d) // 获取指定节点的value，值为double#define dictSlots(d) ((d)-&gt;ht[0].size+(d)-&gt;ht[1].size) // 获取字典中哈希表的总长度，总长度=哈希表1散列数组长度+哈希表2散列数组长度#define dictSize(d) ((d)-&gt;ht[0].used+(d)-&gt;ht[1].used) // 获取字典中哈希表已被使用的节点数量，已被使用的节点数量=哈希表1散列数组已被使用的节点数量+哈希表2散列数组已被使用的节点数量#define dictIsRehashing(d) ((d)-&gt;rehashidx != -1) // 字典当前是否正在进行rehash操作/* API */dict *dictCreate(dictType *type, void *privDataPtr); // 创建一个字典int dictExpand(dict *d, unsigned long size); // 扩充字典大小int dictAdd(dict *d, void *key, void *val); // 向字典中添加键值对dictEntry *dictAddRaw(dict *d, void *key); // 向字典中添加一个只有key的dictEntryint dictReplace(dict *d, void *key, void *val); // 设置/替换指定key的value（key不存在就设置key-value，存在则替换value）dictEntry *dictReplaceRaw(dict *d, void *key); // 和dictReplace，设置/替换指定key（只设置key）int dictDelete(dict *d, const void *key); // 根据key删除字典中的一个key-value对int dictDeleteNoFree(dict *d, const void *key); // 根据key删除字典中的一个key-value对，但并不释放相应的key和valuevoid dictRelease(dict *d); // 释放一个字典dictEntry * dictFind(dict *d, const void *key); // 根据key在字典中查找一个key-value对void *dictFetchValue(dict *d, const void *key); // 根据key从字典中获取它对应的valueint dictResize(dict *d); // 重新计算并设置字典的哈希数组大小，调整到能包含所有元素的最小大小dictIterator *dictGetIterator(dict *d); // 获取一个字典的普通（非安全）迭代器dictIterator *dictGetSafeIterator(dict *d); // 获取一个字典的安全迭代器dictEntry *dictNext(dictIterator *iter); // 获取迭代器的下一个key-value对void dictReleaseIterator(dictIterator *iter); // 释放字典迭代器dictEntry *dictGetRandomKey(dict *d); // 随机获取字典中的一个key-value对unsigned int dictGetSomeKeys(dict *d, dictEntry **des, unsigned int count); // 从字典中随机取样count个key-value对void dictGetStats(char *buf, size_t bufsize, dict *d); // 获取字典状态unsigned int dictGenHashFunction(const void *key, int len); // 一种哈希算法unsigned int dictGenCaseHashFunction(const unsigned char *buf, int len); // 对大小写不敏感的哈希算法void dictEmpty(dict *d, void(callback)(void*)); // 清空字典数据并调用回调函数void dictEnableResize(void); // 开启字典resizevoid dictDisableResize(void); // 禁用字典resizeint dictRehash(dict *d, int n); // 字典rehashint dictRehashMilliseconds(dict *d, int ms); // 在ms时间内rehash，超过则停止void dictSetHashFunctionSeed(unsigned int initval); // 设置rehash函数种子unsigned int dictGetHashFunctionSeed(void); // 获取rehash函数种子unsigned long dictScan(dict *d, unsigned long v, dictScanFunction *fn, void *privdata); // 遍历整个字典，每次访问一个元素都会调用fn操作其数据/* 哈希表类型 */extern dictType dictTypeHeapStringCopyKey;extern dictType dictTypeHeapStrings;extern dictType dictTypeHeapStringCopyKeyValue; 字典的实现在实现部分，将会列出dict.c中比较重要的一些函数定义。 先看dict.c中的一些静态变量和静态函数声明： 12345678910111213/* dictEnableResize()和dictDisableResize()函数允许我们在需要时启用/禁用哈希表的重新规划空间的 * 功能。这对Redis来说非常重要，因为我们使用写时复制且不希望在有子进程进行保存操作时移动太多内存 * 中的数据。 * * 需要注意的是即使dict_can_resize被设置为0，在某些情况下也会触发字典重新规划空间的操作： * 当一个哈希表中的元素个数和散列数组（桶）的比例大于dict_force_resize_ratio时， * 触发字典重新规划空间的操作。 */static int dict_can_resize = 1; // 字典重新规划空间开关static unsigned int dict_force_resize_ratio = 5; // 字典被强制进行重新规划空间时的（元素个数/桶大小）比例static int _dictExpandIfNeeded(dict *ht); // 判断字典是否需要扩容static unsigned long _dictNextPower(unsigned long size); // 字典扩容的大小（字典的容量都是2的整数次方大小），该函数返回大于或等于size的2的整数次方的数字最小的那个static int _dictKeyIndex(dict *ht, const void *key); // 返回指定key在散列数组中的索引值static int _dictInit(dict *ht, dictType *type, void *privDataPtr); // 初始化一个字典 _dictReset函数重置一个已经被ht_init()函数初始化过的哈希表。注意：这个函数只应该被ht_destroy()函数调用。 1234567static void _dictReset(dictht *ht)&#123; ht-&gt;table = NULL; ht-&gt;size = 0; ht-&gt;sizemask = 0; ht-&gt;used = 0;&#125; dictCreate函数创建一个哈希表。 1234567dict *dictCreate(dictType *type, void *privDataPtr)&#123; dict *d = zmalloc(sizeof(*d)); // 分配内存 _dictInit(d,type,privDataPtr); // 初始化哈希表 return d;&#125; _dictInit函数初始化哈希表。 12345678910int _dictInit(dict *d, dictType *type, void *privDataPtr)&#123; _dictReset(&amp;d-&gt;ht[0]); // 初始化第一个哈希表 _dictReset(&amp;d-&gt;ht[1]); // 初始化第二个哈希表 d-&gt;type = type; // 初始化字典类型 d-&gt;privdata = privDataPtr; // 初始化私有数据 d-&gt;rehashidx = -1; // 初始化rehash索引 d-&gt;iterators = 0; // 初始化字典迭代器 return DICT_OK;&#125; dictResize函数重新计算并设置字典的哈希数组大小，调整到能包含所有元素的最小大小，保持已使用节点数量/桶大小的比率接近&lt;=1。 12345678910int dictResize(dict *d)&#123; int minimal; if (!dict_can_resize || dictIsRehashing(d)) return DICT_ERR; // 禁用字典resize或当前字典正在rehash时返回错误 minimal = d-&gt;ht[0].used; // 已使用节点的数量 if (minimal &lt; DICT_HT_INITIAL_SIZE) // 已使用节点数量小于散列数组的初始大小时，新空间大小设置为散列数组的初始大小 minimal = DICT_HT_INITIAL_SIZE; return dictExpand(d, minimal); // 扩充字典大小&#125; dictExpand函数扩充或创建哈希表。 1234567891011121314151617181920212223242526272829int dictExpand(dict *d, unsigned long size)&#123; dictht n; /* the new hash table */ unsigned long realsize = _dictNextPower(size); // 计算一个合适的哈希表大小，大小为2的整数次方 /* 当字典正在进行rehash或字典哈希表中已使用节点数量大于size都返回错误 */ if (dictIsRehashing(d) || d-&gt;ht[0].used &gt; size) return DICT_ERR; /* 新的空间大小和当前的相同，没必要进行rehash */ if (realsize == d-&gt;ht[0].size) return DICT_ERR; /* 为新的哈希表分配空间然后初始化它的所有指针为NULL */ n.size = realsize; // 新哈希表散列数组长度 n.sizemask = realsize-1; // 新哈希表散列数组长度掩码 n.table = zcalloc(realsize*sizeof(dictEntry*)); // 新哈希表散列数组空间分配 n.used = 0; // 新哈希表已使用节点数量 /* 如果d还未被初始化，就不需要rehash，直接把n赋值给字典的第一个哈希表。 */ if (d-&gt;ht[0].table == NULL) &#123; d-&gt;ht[0] = n; return DICT_OK; &#125; /* 准备第二个哈希表用来进行增量rehash */ d-&gt;ht[1] = n; // 1号哈希表现在是被扩展了，数据会从0号哈希表被移动到1号哈希表 d-&gt;rehashidx = 0; return DICT_OK;&#125; dictRehash函数分N步进行增量rehash。当旧哈希表中还有key没移动到新哈希表时，函数返回1，否则返回0。一次rehash过程包含把一个桶从旧哈希表移动到新哈希表（由于我们在同一个桶中使用链表形式保存key-value对，所以一个桶中可能有一个以上的key需要移动）。然而由于哈希表中可能有一部分是空的，并不能保证每一步能对至少一个桶进行rehash，因此我们规定一步中最多只能访问N*10个空桶，否则这么大量的工作可能会造成一段长时间的阻塞。 123456789101112131415161718192021222324252627282930313233343536373839404142int dictRehash(dict *d, int n) &#123; int empty_visits = n*10; // 一步rehash中最多访问的空桶的次数 if (!dictIsRehashing(d)) return 0; while(n-- &amp;&amp; d-&gt;ht[0].used != 0) &#123; // 分n步进行rehash dictEntry *de, *nextde; /* 注意rehashidx不能越界，因为由于ht[0].used != 0，我们知道还有元素没有被rehash */ assert(d-&gt;ht[0].size &gt; (unsigned long)d-&gt;rehashidx); while(d-&gt;ht[0].table[d-&gt;rehashidx] == NULL) &#123; // 遇到空桶了 d-&gt;rehashidx++; // rehashidx移动到下一个桶 if (--empty_visits == 0) return 1; // 当前一次rehash过程遇到的空桶数量等于n*10则直接结束 &#125; de = d-&gt;ht[0].table[d-&gt;rehashidx]; // 获得当前桶中第一个key-value对的指针 /* 把当前桶中所有的key从旧哈希表移动到新哈希表 */ while(de) &#123; // 遍历桶中的key-value对链表 unsigned int h; nextde = de-&gt;next; // 链表中下一个key-value对的指针 /* 获取key的哈希值并计算其在新哈希表中桶的索引值 */ h = dictHashKey(d, de-&gt;key) &amp; d-&gt;ht[1].sizemask; de-&gt;next = d-&gt;ht[1].table[h]; // 设置当前key-value对的next指针指向1号哈希表相应桶得地址 d-&gt;ht[1].table[h] = de; // 将key-value对移动到1号哈希表中（rehash后的新表不会出现一个桶中有多个元素的情况） d-&gt;ht[0].used--; // 扣减0号哈希表已使用节点的数量 d-&gt;ht[1].used++; // 增加1号哈希表已使用节点的数量 de = nextde; // 移动当前key-value对得指针到链表的下一个元素 &#125; d-&gt;ht[0].table[d-&gt;rehashidx] = NULL; // 当把一个桶中所有得key-value对都rehash以后，设置当前桶指向NULL d-&gt;rehashidx++; &#125; /* 检查我们已经对表中所有元素完成rehash操作 */ if (d-&gt;ht[0].used == 0) &#123; zfree(d-&gt;ht[0].table); // 释放0号哈希表的哈希数组 d-&gt;ht[0] = d-&gt;ht[1]; // 把1号哈希表置为0号 _dictReset(&amp;d-&gt;ht[1]); // 重置1号哈希表 d-&gt;rehashidx = -1; return 0; // 完成整个增量式rehash &#125; return 1; // 还有元素没有被rehash&#125; timeInMilliseconds函数获取当前时间戳，单位毫秒。 123456long long timeInMilliseconds(void) &#123; struct timeval tv; gettimeofday(&amp;tv,NULL); return (((long long)tv.tv_sec)*1000)+(tv.tv_usec/1000);&#125; dictRehashMilliseconds函数在ms时间内rehash，超过则停止。 12345678910int dictRehashMilliseconds(dict *d, int ms) &#123; long long start = timeInMilliseconds(); // 起始时间 int rehashes = 0; // rehash次数 while(dictRehash(d,100)) &#123; // 分100步rehash rehashes += 100; if (timeInMilliseconds()-start &gt; ms) break; // 超过规定时间则停止rehash &#125; return rehashes;&#125; _dictRehashStep函数这个函数会执行一步的rehash操作，只有在哈希表没有安全迭代器时才会使用。当在rehash过程中使用迭代器时，我们不能操作两个哈希表，否则有些元素会被遗漏或者被重复rehash。在字典的键查找或更新操作过程中，如果符合rehash条件，就会触发一次rehash，每次执行一步。 123static void _dictRehashStep(dict *d) &#123; if (d-&gt;iterators == 0) dictRehash(d,1); // 没有迭代器在使用时，执行一次一步的rehash&#125; dictAdd函数向目标哈希表添加一个key-value对。 12345678int dictAdd(dict *d, void *key, void *val)&#123; dictEntry *entry = dictAddRaw(d,key); // 先只添加key if (!entry) return DICT_ERR; dictSetVal(d, entry, val); // 设置value return DICT_OK;&#125; dictAddRaw函数是低级别的字典添加操作。此函数添加一个ket-value结构但并不设置value，然后返回这个结构给用户，这可以确保用户按照自己的意愿设置value。此函数还作为用户级别的API直接暴露出来，这主要是为了在散列值内存储非指针类型的数据，比如： entry = dictAddRaw(dict,mykey); if (entry != NULL) dictSetSignedIntegerVal(entry,1000);返回值： 如果key已经存在返回NULL。 如果成功添加了key，函数返回hash结构供用户操作。 1234567891011121314151617181920212223dictEntry *dictAddRaw(dict *d, void *key)&#123; int index; dictEntry *entry; dictht *ht; if (dictIsRehashing(d)) _dictRehashStep(d); // 字典正在进行rehash时，执行一步增量式rehash过程 /* 获取key对应的索引值，当key已经存在时_dictKeyIndex函数返回-1，添加失败 */ if ((index = _dictKeyIndex(d, key)) == -1) return NULL; /* 为新的key-value对分配内存 * 把新添加的元素放在顶部，这很类似数据库的做法：最近添加的元素有更高的访问频率。 */ ht = dictIsRehashing(d) ? &amp;d-&gt;ht[1] : &amp;d-&gt;ht[0]; // 如果字典正在rehash，直接把新元素添加到1号哈希表中 entry = zmalloc(sizeof(*entry)); // 分配内存 entry-&gt;next = ht-&gt;table[index]; ht-&gt;table[index] = entry; // 把新元素插入哈希表相应索引下链表的头部 ht-&gt;used++; // 增加哈希表已使用元素数量 dictSetKey(d, entry, key); // 设置key return entry;&#125; dictReplace函数向字典添加一个元素，不管指定的key是否存在。key不存在时，添加后函数返回1，否则返回0，dictReplace()函数此时只更新相应的value。 12345678910111213141516171819int dictReplace(dict *d, void *key, void *val)&#123; dictEntry *entry, auxentry; /* 尝试添加元素，如果key不存在dictAdd()函数调用成功，并返回1。 */ if (dictAdd(d, key, val) == DICT_OK) return 1; /* key已经存在，获取key-value对 */ entry = dictFind(d, key); /* 对key-value对设置新value并释放旧value的内存。需要注意的是这个先设置再释放的顺序很重要， * 因为新value很有可能和旧value完全是同一个东西。考虑引用记数的情况，你应该先增加引用记数（设置新value）， * 再减少引用记数（释放旧value），这个顺序不能被颠倒。 */ auxentry = *entry; dictSetVal(d, entry, val); dictFreeVal(d, &amp;auxentry); return 0;&#125; dictReplaceRaw函数是dictAddRaw()的简化版本，它总是返回指定key的key-value对结构，即使key已经存在不能被添加时（这种情况下会直接返回这个已经存在的key的key-value对结构）。 12345dictEntry *dictReplaceRaw(dict *d, void *key) &#123; dictEntry *entry = dictFind(d,key); return entry ? entry : dictAddRaw(d,key); // key存在时返回它的key-value对结构，否则调用dictAddRaw&#125; dictGenericDelete函数查找并移除一个元素。dictDelete函数移除字典中的指定key，并释放相应的key和value。dictDeleteNoFree函数移除字典中的指定key，不释放相应的key和value。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546static int dictGenericDelete(dict *d, const void *key, int nofree)&#123; unsigned int h, idx; dictEntry *he, *prevHe; int table; if (d-&gt;ht[0].size == 0) return DICT_ERR; // 字典0号哈希表大小为0时直接返回错误 if (dictIsRehashing(d)) _dictRehashStep(d); // 如果字典d正在rehash，执行一步的rehash过程 h = dictHashKey(d, key); // 计算key的hash值 for (table = 0; table &lt;= 1; table++) &#123; // 遍历0号和1号哈希表移除元素 idx = h &amp; d-&gt;ht[table].sizemask; // 获取key所在的哈希数组索引值 he = d-&gt;ht[table].table[idx]; // 获取idx索引位置指向的第一个entry prevHe = NULL; while(he) &#123; // 遍历idx索引位置上的entry链表，移除key为指定值的元素 if (key==he-&gt;key || dictCompareKeys(d, key, he-&gt;key)) &#123; // 找到该entry /* Unlink the element from the list */ if (prevHe) prevHe-&gt;next = he-&gt;next; else d-&gt;ht[table].table[idx] = he-&gt;next; if (!nofree) &#123; // nofree标志表示是否需要释放这个entry的key和value dictFreeKey(d, he); // 释放key dictFreeVal(d, he); // 释放value &#125; zfree(he); // 释放enrty d-&gt;ht[table].used--; // 减少已存在的key数量 return DICT_OK; &#125; prevHe = he; // 没找到则向后查找 he = he-&gt;next; &#125; /* 如果字典不是正在进行rehash，直接跳过对1号哈希表的搜索，因为只有在rehash过程中， * 添加的key-value才会直接写到1号哈希表中，其他时候都是直接写0号哈希表。 */ if (!dictIsRehashing(d)) break; &#125; return DICT_ERR; /* not found */&#125;int dictDelete(dict *ht, const void *key) &#123; return dictGenericDelete(ht,key,0);&#125;int dictDeleteNoFree(dict *ht, const void *key) &#123; return dictGenericDelete(ht,key,1);&#125; _dictClear函数销毁整个字典。dictRelease函数清空并释放字典。 1234567891011121314151617181920212223242526272829303132int _dictClear(dict *d, dictht *ht, void(callback)(void *)) &#123; unsigned long i; /* Free all the elements */ for (i = 0; i &lt; ht-&gt;size &amp;&amp; ht-&gt;used &gt; 0; i++) &#123; // 遍历整个哈希表 dictEntry *he, *nextHe; if (callback &amp;&amp; (i &amp; 65535) == 0) callback(d-&gt;privdata); // 销毁私有数据 if ((he = ht-&gt;table[i]) == NULL) continue; // 跳过没有数据的桶 while(he) &#123; // 遍历桶中的entry销毁数据 nextHe = he-&gt;next; dictFreeKey(d, he); dictFreeVal(d, he); zfree(he); ht-&gt;used--; // 递减哈希表中的元素数量 he = nextHe; &#125; &#125; /* 释放哈希表的哈希数组 */ zfree(ht-&gt;table); /* 重置整个哈希表 */ _dictReset(ht); return DICT_OK; /* never fails */&#125;void dictRelease(dict *d)&#123; _dictClear(d,&amp;d-&gt;ht[0],NULL); _dictClear(d,&amp;d-&gt;ht[1],NULL); zfree(d);&#125; dictFind函数查找字典key。 1234567891011121314151617181920dictEntry *dictFind(dict *d, const void *key)&#123; dictEntry *he; unsigned int h, idx, table; if (d-&gt;ht[0].used + d-&gt;ht[1].used == 0) return NULL; // 0号和1号哈希表都没有元素，返回NULL if (dictIsRehashing(d)) _dictRehashStep(d); // 如果字典正在rehash，执行一次一步rehash h = dictHashKey(d, key); // 计算key的哈希值 for (table = 0; table &lt;= 1; table++) &#123; // 在0号和1号哈希表种查找 idx = h &amp; d-&gt;ht[table].sizemask; // 计算索引值 he = d-&gt;ht[table].table[idx]; // 获取哈希数组相应索引的第一个元素 while(he) &#123; // 遍历元素链表，查找key if (key==he-&gt;key || dictCompareKeys(d, key, he-&gt;key)) return he; he = he-&gt;next; &#125; if (!dictIsRehashing(d)) return NULL; // 如果字典不是正在进行rehash，直接跳过对1号哈希表的搜索，并返回NULL &#125; return NULL;&#125; dictFetchValue函数获取字典中指定key的value。 123456void *dictFetchValue(dict *d, const void *key) &#123; dictEntry *he; he = dictFind(d,key); // 用key找到key-value entry return he ? dictGetVal(he) : NULL;&#125; dictFingerprint函数返回字典的指纹。字典的指纹是一个64位的数字，它表示字典在一个给定时间点的状态，其实就是一些字典熟悉的异或结果。当初始化了一个不安全的迭代器时，我们可以拿到字典的指纹，并且在迭代器被释放时检查这个指纹。如果两个指纹不同就表示迭代器的所有者在迭代过程中进行了被禁止的操作。 123456789101112131415161718192021222324252627282930long long dictFingerprint(dict *d) &#123; long long integers[6], hash = 0; int j; integers[0] = (long) d-&gt;ht[0].table; // 0号哈希表 integers[1] = d-&gt;ht[0].size; // 0号哈希表的大小 integers[2] = d-&gt;ht[0].used; // 0号哈希表中元素数量 integers[3] = (long) d-&gt;ht[1].table; // 1号哈希表 integers[4] = d-&gt;ht[1].size; // 1号哈希表的大小 integers[5] = d-&gt;ht[1].used; // 1号哈希表中元素数量 /* 我们对N个整形数计算hash值的方法是连续地把上一个数字的hash值和下一个数相加，形成一个新值， * 再对这个新值计算hash值，以此类推。像这样： * * Result = hash(hash(hash(int1)+int2)+int3) ... * * 用这种方式计算一组整型的hash值时，不同的计算顺序会有不同的结果。 */ for (j = 0; j &lt; 6; j++) &#123; hash += integers[j]; /* 使用Tomas Wang's 64 bit integer哈希算法 */ hash = (~hash) + (hash &lt;&lt; 21); // hash = (hash &lt;&lt; 21) - hash - 1; hash = hash ^ (hash &gt;&gt; 24); hash = (hash + (hash &lt;&lt; 3)) + (hash &lt;&lt; 8); // hash * 265 hash = hash ^ (hash &gt;&gt; 14); hash = (hash + (hash &lt;&lt; 2)) + (hash &lt;&lt; 4); // hash * 21 hash = hash ^ (hash &gt;&gt; 28); hash = hash + (hash &lt;&lt; 31); &#125; return hash;&#125; dictGetIterator函数获取一个字典的不安全迭代器。dictGetSafeIterator函数获取一个字典的安全迭代器。 12345678910111213141516171819dictIterator *dictGetIterator(dict *d)&#123; dictIterator *iter = zmalloc(sizeof(*iter)); // 为迭代器分配空间 iter-&gt;d = d; iter-&gt;table = 0; // 迭代的是0号哈希表 iter-&gt;index = -1; iter-&gt;safe = 0; // 0表示不安全 iter-&gt;entry = NULL; iter-&gt;nextEntry = NULL; return iter;&#125;dictIterator *dictGetSafeIterator(dict *d) &#123; dictIterator *i = dictGetIterator(d); i-&gt;safe = 1; // 1表示安全 return i;&#125; dictNext函数获取迭代器的下一个元素。 123456789101112131415161718192021222324252627282930313233dictEntry *dictNext(dictIterator *iter)&#123; while (1) &#123; if (iter-&gt;entry == NULL) &#123; // 当前桶的entry链表已经迭代完毕 dictht *ht = &amp;iter-&gt;d-&gt;ht[iter-&gt;table]; // 获取迭代器的哈希表指针 if (iter-&gt;index == -1 &amp;&amp; iter-&gt;table == 0) &#123; // 刚开始迭代0号哈希表时 if (iter-&gt;safe) iter-&gt;d-&gt;iterators++; // 如果是安全的迭代器，就将当前使用的迭代器数量+1 else iter-&gt;fingerprint = dictFingerprint(iter-&gt;d); // 不安全迭代器需要设置字典指纹 &#125; iter-&gt;index++; // 移动到下一个桶 if (iter-&gt;index &gt;= (long) ht-&gt;size) &#123; // 迭代器的当前索引值超过哈希表大小 if (dictIsRehashing(iter-&gt;d) &amp;&amp; iter-&gt;table == 0) &#123; // 字典正在rehash且当前是0号哈希表时 iter-&gt;table++; // 开始迭代1号哈希表 iter-&gt;index = 0; // 设置开始迭代索引为0 ht = &amp;iter-&gt;d-&gt;ht[1]; // 更新哈希表指针 &#125; else &#123; break; // 如果字典不在rehash且迭代结束，就跳出并返回NULL，表示没有下一个元素了 &#125; &#125; iter-&gt;entry = ht-&gt;table[iter-&gt;index]; // 获取当前桶上的第一个元素 &#125; else &#123; iter-&gt;entry = iter-&gt;nextEntry; // 获取当前桶中entry的下一个entry &#125; if (iter-&gt;entry) &#123; /* 保存nextEntry指针，因为迭代器用户有可能会删除当前entry */ iter-&gt;nextEntry = iter-&gt;entry-&gt;next; return iter-&gt;entry; &#125; &#125; return NULL;&#125; dictReleaseIterator函数释放字典迭代器。 12345678910void dictReleaseIterator(dictIterator *iter)&#123; if (!(iter-&gt;index == -1 &amp;&amp; iter-&gt;table == 0)) &#123; // 如果当前迭代器时初始化状态且是0号哈希表 if (iter-&gt;safe) // 释放安全迭代器时需要递减当前使用的迭代器数量（安全迭代器只能有一个） iter-&gt;d-&gt;iterators--; else assert(iter-&gt;fingerprint == dictFingerprint(iter-&gt;d)); // 迭代器的字典指纹和实时的字典指纹不符时报错 &#125; zfree(iter);&#125; dictGetRandomKey函数从哈希表中随机返回一个entry。适用于实现随机算法。 12345678910111213141516171819202122232425262728293031323334353637dictEntry *dictGetRandomKey(dict *d)&#123; dictEntry *he, *orighe; unsigned int h; int listlen, listele; if (dictSize(d) == 0) return NULL; // 字典没有元素时直接返回NULL if (dictIsRehashing(d)) _dictRehashStep(d); // 字典在rehash过程中，执行一次一步的rehash if (dictIsRehashing(d)) &#123; // 字典正在rehash do &#123; /* 我们知道0-rehashidx-1之间的索引范围内没有元素 */ h = d-&gt;rehashidx + (random() % (d-&gt;ht[0].size + d-&gt;ht[1].size - d-&gt;rehashidx)); he = (h &gt;= d-&gt;ht[0].size) ? d-&gt;ht[1].table[h - d-&gt;ht[0].size] : d-&gt;ht[0].table[h]; &#125; while(he == NULL); &#125; else &#123; // 字典不在rehash时，随机生成一个索引值，直到此索引值上有entry do &#123; h = random() &amp; d-&gt;ht[0].sizemask; he = d-&gt;ht[0].table[h]; &#125; while(he == NULL); &#125; /* \b我们找到了一个非空的桶，但它是一个链表结构，所以我们要从链表中随机获取一个元素。 * 唯一明智的方式是计算链表长度并随机选择一个索引值。*/ listlen = 0; orighe = he; while(he) &#123; // 计算链表长度 he = he-&gt;next; listlen++; &#125; listele = random() % listlen; he = orighe; while(listele--) he = he-&gt;next; return he;&#125; dictGetSomeKeys函数对字典进行随机采样，从一些随机位置返回一些key。此函数并不保证返回’count’中指定个数的key，并且也不保证不会返回重复的元素，不过函数会尽力做到返回’count’个key和尽量返回重复的key。函数返回指向dictEntry数组的指针。这个数组的大小至少能容纳’count’个元素。函数返回保存在’des’中entry的数量，如果哈希表中的元素小于’count’个，或者在一个合理的时间内没有找到指定个数的元素，这个数字可能会比入参’count’要小。需要注意的是，此函数并不适合当你需要一个数量刚好的采样集合的情况，但当你仅仅需要进行“采样”时来进行一些统计计算时，还是适用的。用函数来获取N个随机key要比执行N次dictGetRandomKey()要快。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263unsigned int dictGetSomeKeys(dict *d, dictEntry **des, unsigned int count) &#123; unsigned long j; // 字典内部的哈希表编号，0或1 unsigned long tables; // \b哈希表数量 unsigned long stored = 0, maxsizemask; // 获取到的随机key数量，掩码 unsigned long maxsteps; // 最大步骤数，考虑到开销问题，超过这个值就放弃继续获取随机key if (dictSize(d) &lt; count) count = dictSize(d); maxsteps = count*10; // 最大步骤数为需要获取的key的数量的10倍 /* 运行count次一步rehash操作 */ for (j = 0; j &lt; count; j++) &#123; if (dictIsRehashing(d)) _dictRehashStep(d); else break; &#125; tables = dictIsRehashing(d) ? 2 : 1; // 字典rehash过程中就有两个哈希表要采样，正常情况下是1个 maxsizemask = d-&gt;ht[0].sizemask; if (tables &gt; 1 &amp;&amp; maxsizemask &lt; d-&gt;ht[1].sizemask) // rehash过程中如果1号哈希表比0号哈希表搭则使用1号哈希表的掩码 maxsizemask = d-&gt;ht[1].sizemask; // /* 获取一个随机索引值 */ unsigned long i = random() &amp; maxsizemask; unsigned long emptylen = 0; // 迄今为止的连续空entry数量 while(stored &lt; count &amp;&amp; maxsteps--) &#123; for (j = 0; j &lt; tables; j++) &#123; /* 和dict.c中的rehash一样： 由于ht[0]正在进行rehash，那里并没有密集的有元素的桶 * 需要访问，我们可以跳过ht[0]中位于0到idx-1之间的桶，idx是字典的数据rehash的当前索引位置 * 这个位置以前的桶中的数据都已经被移动到ht[1]了。 */ if (tables == 2 &amp;&amp; j == 0 &amp;&amp; i &lt; (unsigned long) d-&gt;rehashidx) &#123; /* 此外，在rehash过程中，如果我们获取的随机索引值i大于ht[1]的大小，则ht[0] * 和ht[1]都已经没有可用元素让我们获取，此时我们可以直接跳过。 * （这一版发生在字典空间从大表小的情况下）。 */ if (i &gt;= d-&gt;ht[1].size) i = d-&gt;rehashidx; continue; &#125; if (i &gt;= d-&gt;ht[j].size) continue; //获取的随机索引值i超出范围，直接开始下一次循环 dictEntry *he = d-&gt;ht[j].table[i]; // 获取到一个entry /* 计算连续遇到的空桶的数量，如果到达'count'就跳到其他位置去获取（'count'最小值为5） */ if (he == NULL) &#123; emptylen++; if (emptylen &gt;= 5 &amp;&amp; emptylen &gt; count) &#123; i = random() &amp; maxsizemask; // 重新获取随机值i emptylen = 0; // 重置连续遇到的空桶的数量 &#125; &#125; else &#123; // 遇到了非空桶 emptylen = 0; // 重置连续遇到的空桶的数量 while (he) &#123; /* 把桶中entry链表中的所有元素加入到结果数组中 */ *des = he; des++; he = he-&gt;next; stored++; if (stored == count) return stored; &#125; &#125; &#125; i = (i+1) &amp; maxsizemask; &#125; return stored;&#125; dictScan函数用于迭代字典中的所有元素。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970unsigned long dictScan(dict *d, unsigned long v, dictScanFunction *fn, void *privdata)&#123; dictht *t0, *t1; const dictEntry *de; unsigned long m0, m1; if (dictSize(d) == 0) return 0; if (!dictIsRehashing(d)) &#123; t0 = &amp;(d-&gt;ht[0]); m0 = t0-&gt;sizemask; /* Emit entries at cursor */ de = t0-&gt;table[v &amp; m0]; while (de) &#123; fn(privdata, de); de = de-&gt;next; &#125; &#125; else &#123; t0 = &amp;d-&gt;ht[0]; t1 = &amp;d-&gt;ht[1]; /* Make sure t0 is the smaller and t1 is the bigger table */ if (t0-&gt;size &gt; t1-&gt;size) &#123; t0 = &amp;d-&gt;ht[1]; t1 = &amp;d-&gt;ht[0]; &#125; m0 = t0-&gt;sizemask; m1 = t1-&gt;sizemask; /* Emit entries at cursor */ de = t0-&gt;table[v &amp; m0]; while (de) &#123; fn(privdata, de); de = de-&gt;next; &#125; /* Iterate over indices in larger table that are the expansion * of the index pointed to by the cursor in the smaller table */ do &#123; /* Emit entries at cursor */ de = t1-&gt;table[v &amp; m1]; while (de) &#123; fn(privdata, de); de = de-&gt;next; &#125; /* Increment bits not covered by the smaller mask */ v = (((v | m0) + 1) &amp; ~m0) | (v &amp; m0); /* Continue while bits covered by mask difference is non-zero */ &#125; while (v &amp; (m0 ^ m1)); &#125; /* Set unmasked bits so incrementing the reversed cursor * operates on the masked bits of the smaller table */ v |= ~m0; /* Increment the reverse cursor */ v = rev(v); v++; v = rev(v); return v;&#125; 以下是一些私有函数的实现： _dictExpandIfNeeded函数判断字典是否需要扩容，如果需要则扩容，否则什么也不做。 12345678910111213141516171819static int _dictExpandIfNeeded(dict *d)&#123; /* 正在进行增量式rehash，直接返回 */ if (dictIsRehashing(d)) return DICT_OK; /* 如果哈希表为空（散列数组大小为0），把它的大小扩容到初始状态（散列数组的初始大小） */ if (d-&gt;ht[0].size == 0) return dictExpand(d, DICT_HT_INITIAL_SIZE); /* 如果元素数量和散列数组的比值达到或超过1:1，且我们允许调整哈希表的大小（全局变量dict_can_resize为1） * 或者虽然我们不允许调整哈希表大小（全局变量dict_can_resize为0），但是元素数量/散列数组的值 * 已经超过安全阈值（全局变量dict_force_resize_ratio），我们把哈希表大小调整为当前已使用桶数量的两倍。 */ if (d-&gt;ht[0].used &gt;= d-&gt;ht[0].size &amp;&amp; (dict_can_resize || d-&gt;ht[0].used/d-&gt;ht[0].size &gt; dict_force_resize_ratio)) &#123; return dictExpand(d, d-&gt;ht[0].used*2); &#125; return DICT_OK;&#125; _dictNextPower函数返回大于且最接近size的2的正整数次方的数字，因为哈希表的大小一定是2的正整数次方。 1234567891011static unsigned long _dictNextPower(unsigned long size)&#123; unsigned long i = DICT_HT_INITIAL_SIZE; if (size &gt;= LONG_MAX) return LONG_MAX; // 防止size溢出 while(1) &#123; if (i &gt;= size) return i; i *= 2; &#125;&#125; _dictKeyIndex函数计算一个给定key在字典中的索引值。如果key已经存在，返回-1。需要注意的是如果哈希表正在进行rehash，返回的总是1号哈希表（新哈希表）的索引值。 1234567891011121314151617181920212223static int (dict *d, const void *key)&#123; unsigned int h, idx, table; dictEntry *he; /* 如果需要，扩容字典 */ if (_dictExpandIfNeeded(d) == DICT_ERR) return -1; /* 计算key的哈希值*/ h = dictHashKey(d, key); for (table = 0; table &lt;= 1; table++) &#123; idx = h &amp; d-&gt;ht[table].sizemask; // 计算key的索引值 /* 遍历当前桶的entry链表查找指定的key是否已经存在 */ he = d-&gt;ht[table].table[idx]; // 桶中第一个元素 while(he) &#123; if (key==he-&gt;key || dictCompareKeys(d, key, he-&gt;key)) // 找到此key说明已存在，返回-1 return -1; he = he-&gt;next; // 下一个元素 &#125; if (!dictIsRehashing(d)) break; // 字典不在rehash，只查看0号哈希表即可，跳过1号哈希表 &#125; return idx;&#125; dictEmpty函数清空字典数据并调用回调函数。 123456void dictEmpty(dict *d, void(callback)(void*)) &#123; _dictClear(d,&amp;d-&gt;ht[0],callback); // 清空0号哈希表，并调用回调函数 _dictClear(d,&amp;d-&gt;ht[1],callback); // 清空1号哈希表，并调用回调函数 d-&gt;rehashidx = -1; // 设置不在rehash过程中 d-&gt;iterators = 0; // 设置当前迭代器数量为0&#125; dictEnableResize函数允许调整字典大小。dictDisableResize函数禁止调整字典大小。 1234567void dictEnableResize(void) &#123; dict_can_resize = 1;&#125;void dictDisableResize(void) &#123; dict_can_resize = 0;&#125;","categories":[{"name":"源码分析","slug":"源码分析","permalink":"https://nullcc.github.io/categories/源码分析/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://nullcc.github.io/tags/Redis/"},{"name":"数据结构","slug":"数据结构","permalink":"https://nullcc.github.io/tags/数据结构/"}]},{"title":"Redis的启动过程","slug":"Redis的启动过程","date":"2017-11-12T16:00:00.000Z","updated":"2022-04-15T03:41:13.020Z","comments":true,"path":"2017/11/13/Redis的启动过程/","link":"","permalink":"https://nullcc.github.io/2017/11/13/Redis的启动过程/","excerpt":"Redis的启动过程都写在server.c的main中（这里使用的是Redis-3.2.11版本的源码，早期版本应该在redis.c的main中）。下面就来概括性地看看Redis的启动过程中发生了什么。","text":"Redis的启动过程都写在server.c的main中（这里使用的是Redis-3.2.11版本的源码，早期版本应该在redis.c的main中）。下面就来概括性地看看Redis的启动过程中发生了什么。 由于main函数的头部有一些预编译条件代码，主要是针对测试用的，这里把这部分代码全部删除，以让整个过程更加清晰一点。 main函数的定义如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145int main(int argc, char **argv) &#123; struct timeval tv; int j; setlocale(LC_COLLATE,\"\"); zmalloc_enable_thread_safeness(); zmalloc_set_oom_handler(redisOutOfMemoryHandler); srand(time(NULL)^getpid()); gettimeofday(&amp;tv,NULL); dictSetHashFunctionSeed(tv.tv_sec^tv.tv_usec^getpid()); server.sentinel_mode = checkForSentinelMode(argc,argv); initServerConfig(); // 初始化Redis服务器的各种参数 /* Store the executable path and arguments in a safe place in order * to be able to restart the server later. */ server.executable = getAbsolutePath(argv[0]); server.exec_argv = zmalloc(sizeof(char*)*(argc+1)); server.exec_argv[argc] = NULL; for (j = 0; j &lt; argc; j++) server.exec_argv[j] = zstrdup(argv[j]); /* We need to init sentinel right now as parsing the configuration file * in sentinel mode will have the effect of populating the sentinel * data structures with master nodes to monitor. */ if (server.sentinel_mode) &#123; initSentinelConfig(); initSentinel(); &#125; /* Check if we need to start in redis-check-rdb mode. We just execute * the program main. However the program is part of the Redis executable * so that we can easily execute an RDB check on loading errors. */ if (strstr(argv[0],\"redis-check-rdb\") != NULL) redis_check_rdb_main(argc,argv); if (argc &gt;= 2) &#123; j = 1; /* First option to parse in argv[] */ sds options = sdsempty(); char *configfile = NULL; /* Handle special options --help and --version */ if (strcmp(argv[1], \"-v\") == 0 || strcmp(argv[1], \"--version\") == 0) version(); if (strcmp(argv[1], \"--help\") == 0 || strcmp(argv[1], \"-h\") == 0) usage(); if (strcmp(argv[1], \"--test-memory\") == 0) &#123; if (argc == 3) &#123; memtest(atoi(argv[2]),50); exit(0); &#125; else &#123; fprintf(stderr,\"Please specify the amount of memory to test in megabytes.\\n\"); fprintf(stderr,\"Example: ./redis-server --test-memory 4096\\n\\n\"); exit(1); &#125; &#125; /* First argument is the config file name? */ if (argv[j][0] != '-' || argv[j][1] != '-') &#123; configfile = argv[j]; server.configfile = getAbsolutePath(configfile); /* Replace the config file in server.exec_argv with * its absoulte path. */ zfree(server.exec_argv[j]); server.exec_argv[j] = zstrdup(server.configfile); j++; &#125; /* All the other options are parsed and conceptually appended to the * configuration file. For instance --port 6380 will generate the * string \"port 6380\\n\" to be parsed after the actual file name * is parsed, if any. */ while(j != argc) &#123; if (argv[j][0] == '-' &amp;&amp; argv[j][1] == '-') &#123; /* Option name */ if (!strcmp(argv[j], \"--check-rdb\")) &#123; /* Argument has no options, need to skip for parsing. */ j++; continue; &#125; if (sdslen(options)) options = sdscat(options,\"\\n\"); options = sdscat(options,argv[j]+2); options = sdscat(options,\" \"); &#125; else &#123; /* Option argument */ options = sdscatrepr(options,argv[j],strlen(argv[j])); options = sdscat(options,\" \"); &#125; j++; &#125; if (server.sentinel_mode &amp;&amp; configfile &amp;&amp; *configfile == '-') &#123; serverLog(LL_WARNING, \"Sentinel config from STDIN not allowed.\"); serverLog(LL_WARNING, \"Sentinel needs config file on disk to save state. Exiting...\"); exit(1); &#125; resetServerSaveParams(); loadServerConfig(configfile,options); sdsfree(options); &#125; else &#123; serverLog(LL_WARNING, \"Warning: no config file specified, using the default config. In order to specify a config file use %s /path/to/%s.conf\", argv[0], server.sentinel_mode ? \"sentinel\" : \"redis\"); &#125; server.supervised = redisIsSupervised(server.supervised_mode); int background = server.daemonize &amp;&amp; !server.supervised; if (background) daemonize(); initServer(); if (background || server.pidfile) createPidFile(); redisSetProcTitle(argv[0]); redisAsciiArt(); checkTcpBacklogSettings(); if (!server.sentinel_mode) &#123; /* Things not needed when running in Sentinel mode. */ serverLog(LL_WARNING,\"Server started, Redis version \" REDIS_VERSION); #ifdef __linux__ linuxMemoryWarnings(); #endif loadDataFromDisk(); if (server.cluster_enabled) &#123; if (verifyClusterConfigWithData() == C_ERR) &#123; serverLog(LL_WARNING, \"You can't have keys in a DB different than DB 0 when in \" \"Cluster mode. Exiting.\"); exit(1); &#125; &#125; if (server.ipfd_count &gt; 0) serverLog(LL_NOTICE,\"The server is now ready to accept connections on port %d\", server.port); if (server.sofd &gt; 0) serverLog(LL_NOTICE,\"The server is now ready to accept connections at %s\", server.unixsocket); &#125; else &#123; sentinelIsRunning(); &#125; /* Warning the user about suspicious maxmemory setting. */ if (server.maxmemory &gt; 0 &amp;&amp; server.maxmemory &lt; 1024*1024) &#123; serverLog(LL_WARNING,\"WARNING: You specified a maxmemory value that is less than 1MB (current value is %llu bytes). Are you sure this is what you really want?\", server.maxmemory); &#125; aeSetBeforeSleepProc(server.el,beforeSleep); aeMain(server.el); // 进入主事件循环 aeDeleteEventLoop(server.el); return 0;&#125; 我们现在从头到尾解析这个函数。 1. initServerConfig首先来到initServerConfig();这行代码上，从函数名称上很容易猜想这个函数是用来初始化Redis服务器配置的。这个函数的定义有点长，不过它做的事情并不难理解，就是初始化Redis的部分参数。在initServerConfig中初始化的这部分参数，一般都可以对应到redis.conf中的配置项，比如端口号、rdb文件名等。。initServerConfig中也提供了很多redis.conf配置项的默认参数，如果配置文件没有给出某个配置项的值，initServerConfig就会给出一个默认值。具体来看看这个函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165/* 初始化Redis服务器配置（主要配置文件的一些参数） */void initServerConfig(void) &#123; int j; getRandomHexChars(server.runid,CONFIG_RUN_ID_SIZE); // 设置server的runid，runid用来标识一个特定的唯一的已启动的redis实例 server.configfile = NULL; // 配置文件的绝对路径 server.executable = NULL; // 可执行文件的绝对路径 server.hz = CONFIG_DEFAULT_HZ; // serverCron()的调用频率，单位毫秒 server.runid[CONFIG_RUN_ID_SIZE] = '\\0'; // 设置runid的结束符 server.arch_bits = (sizeof(long) == 8) ? 64 : 32; // server的机器字长 server.port = CONFIG_DEFAULT_SERVER_PORT; // server端口 server.tcp_backlog = CONFIG_DEFAULT_TCP_BACKLOG; // tcp server.bindaddr_count = 0; // server.bindaddr[]的元素个数 server.unixsocket = NULL; // Unix socket路径 server.unixsocketperm = CONFIG_DEFAULT_UNIX_SOCKET_PERM; // Unix socket许可 server.ipfd_count = 0; // ipfd[]的槽数 server.sofd = -1; // Unix socket文件描述符 server.protected_mode = CONFIG_DEFAULT_PROTECTED_MODE; // 保护模式开关，是否允许外部主机连接 server.dbnum = CONFIG_DEFAULT_DBNUM; // Redis server中db的个数 server.verbosity = CONFIG_DEFAULT_VERBOSITY; // 日志级别 server.maxidletime = CONFIG_DEFAULT_CLIENT_TIMEOUT; // 客户端超时时间，客户端空闲时间超过此值时服务器会断开和客户端的连接 server.tcpkeepalive = CONFIG_DEFAULT_TCP_KEEPALIVE; // tcp保活标志，当此值非零时，会设置SO_KEEPALIVE server.active_expire_enabled = 1; /* Can be disabled for testing purposes. */ server.client_max_querybuf_len = PROTO_MAX_QUERYBUF_LEN; // 客户端最大查询缓存大小 server.saveparams = NULL; // rdb的保存点数组 server.loading = 0; // redis从磁盘上加载数据的标志，非零值表示正在从磁盘加载数据 server.logfile = zstrdup(CONFIG_DEFAULT_LOGFILE); // 日志文件路径 server.syslog_enabled = CONFIG_DEFAULT_SYSLOG_ENABLED; // 是否允许syslog server.syslog_ident = zstrdup(CONFIG_DEFAULT_SYSLOG_IDENT); // syslog识别字段 server.syslog_facility = LOG_LOCAL0; // syslog设备 server.daemonize = CONFIG_DEFAULT_DAEMONIZE; // 是否是守护进程 server.supervised = 0; /* 1 if supervised, 0 otherwise. */ server.supervised_mode = SUPERVISED_NONE; /* See SUPERVISED_* */ server.aof_state = AOF_OFF; // AOF的状态，有AOF_(ON|OFF|WAIT_REWRITE)三种 server.aof_fsync = CONFIG_DEFAULT_AOF_FSYNC; // fsync()策略 server.aof_no_fsync_on_rewrite = CONFIG_DEFAULT_AOF_NO_FSYNC_ON_REWRITE; // 进行AOF rewrite时是否允许fsync server.aof_rewrite_perc = AOF_REWRITE_PERC; /* Rewrite AOF if % growth is &gt; M and... */ server.aof_rewrite_min_size = AOF_REWRITE_MIN_SIZE; // AOF文件的最小大小 server.aof_rewrite_base_size = 0; // 上一次rewrite后AOF文件大小 server.aof_rewrite_scheduled = 0; // BGSAVE结束后开始rewrite server.aof_last_fsync = time(NULL); // 上一次fsync()的UNIX时间戳 server.aof_rewrite_time_last = -1; // 上一次AOF rewrite耗时 server.aof_rewrite_time_start = -1; // 当前一次AOF rewrite的开始时间 server.aof_lastbgrewrite_status = C_OK; // 上一次bgrewrite状态，C_OK或C_ERR server.aof_delayed_fsync = 0; // fsync拖延次数 server.aof_fd = -1; // 当前AOF文件的文件描述符 server.aof_selected_db = -1; // 当前AOF选择的db server.aof_flush_postponed_start = 0; // AOF文件延迟刷新的时间 server.aof_rewrite_incremental_fsync = CONFIG_DEFAULT_AOF_REWRITE_INCREMENTAL_FSYNC; // rewrite期间有fsync增量吗 server.aof_load_truncated = CONFIG_DEFAULT_AOF_LOAD_TRUNCATED; /* Don't stop on unexpected AOF EOF. */ server.pidfile = NULL; // redis server的pid文件路径 server.rdb_filename = zstrdup(CONFIG_DEFAULT_RDB_FILENAME); // rdb文件名 server.aof_filename = zstrdup(CONFIG_DEFAULT_AOF_FILENAME); // aof文件名 server.requirepass = NULL; // AUTH命令的密码，为NULL即不需要密码 server.rdb_compression = CONFIG_DEFAULT_RDB_COMPRESSION; // 是否在rdb中使用压缩 server.rdb_checksum = CONFIG_DEFAULT_RDB_CHECKSUM; // 是否使用rdb校验码 server.stop_writes_on_bgsave_err = CONFIG_DEFAULT_STOP_WRITES_ON_BGSAVE_ERROR; // 是否不允许在BGSAVE出错时写入 server.activerehashing = CONFIG_DEFAULT_ACTIVE_REHASHING; // serverCron()时是否可以执行增量哈希 server.notify_keyspace_events = 0; // server.maxclients = CONFIG_DEFAULT_MAX_CLIENTS; // 同时允许多少客户端连接 server.bpop_blocked_clients = 0; // 被列表bpop命令阻塞住的客户端数 server.maxmemory = CONFIG_DEFAULT_MAXMEMORY; // 最大使用内存量（字节） server.maxmemory_policy = CONFIG_DEFAULT_MAXMEMORY_POLICY; // 在内存达到最大值时的key淘汰策略 server.maxmemory_samples = CONFIG_DEFAULT_MAXMEMORY_SAMPLES; server.hash_max_ziplist_entries = OBJ_HASH_MAX_ZIPLIST_ENTRIES; server.hash_max_ziplist_value = OBJ_HASH_MAX_ZIPLIST_VALUE; server.list_max_ziplist_size = OBJ_LIST_MAX_ZIPLIST_SIZE; server.list_compress_depth = OBJ_LIST_COMPRESS_DEPTH; server.set_max_intset_entries = OBJ_SET_MAX_INTSET_ENTRIES; server.zset_max_ziplist_entries = OBJ_ZSET_MAX_ZIPLIST_ENTRIES; server.zset_max_ziplist_value = OBJ_ZSET_MAX_ZIPLIST_VALUE; server.hll_sparse_max_bytes = CONFIG_DEFAULT_HLL_SPARSE_MAX_BYTES; server.shutdown_asap = 0; server.repl_ping_slave_period = CONFIG_DEFAULT_REPL_PING_SLAVE_PERIOD; server.repl_timeout = CONFIG_DEFAULT_REPL_TIMEOUT; server.repl_min_slaves_to_write = CONFIG_DEFAULT_MIN_SLAVES_TO_WRITE; server.repl_min_slaves_max_lag = CONFIG_DEFAULT_MIN_SLAVES_MAX_LAG; server.cluster_enabled = 0; server.cluster_node_timeout = CLUSTER_DEFAULT_NODE_TIMEOUT; server.cluster_migration_barrier = CLUSTER_DEFAULT_MIGRATION_BARRIER; server.cluster_slave_validity_factor = CLUSTER_DEFAULT_SLAVE_VALIDITY; server.cluster_require_full_coverage = CLUSTER_DEFAULT_REQUIRE_FULL_COVERAGE; server.cluster_configfile = zstrdup(CONFIG_DEFAULT_CLUSTER_CONFIG_FILE); server.migrate_cached_sockets = dictCreate(&amp;migrateCacheDictType,NULL); server.next_client_id = 1; /* Client IDs, start from 1 .*/ server.loading_process_events_interval_bytes = (1024*1024*2); server.lua_time_limit = LUA_SCRIPT_TIME_LIMIT; server.lruclock = getLRUClock(); // server的LRU时钟 resetServerSaveParams(); // 保存策略，1小时内至少有1次修改就保存 appendServerSaveParams(60*60,1); /* save after 1 hour and 1 change */ // 保存策略，5分钟内至少有100次修改就保存 appendServerSaveParams(300,100); /* save after 5 minutes and 100 changes */ // 保存策略，1分钟内至少有10000次修改就保存 appendServerSaveParams(60,10000); /* save after 1 minute and 10000 changes */ /* 复制相关 */ server.masterauth = NULL; server.masterhost = NULL; server.masterport = 6379; server.master = NULL; server.cached_master = NULL; server.repl_master_initial_offset = -1; server.repl_state = REPL_STATE_NONE; server.repl_syncio_timeout = CONFIG_REPL_SYNCIO_TIMEOUT; server.repl_serve_stale_data = CONFIG_DEFAULT_SLAVE_SERVE_STALE_DATA; server.repl_slave_ro = CONFIG_DEFAULT_SLAVE_READ_ONLY; server.repl_down_since = 0; /* Never connected, repl is down since EVER. */ server.repl_disable_tcp_nodelay = CONFIG_DEFAULT_REPL_DISABLE_TCP_NODELAY; server.repl_diskless_sync = CONFIG_DEFAULT_REPL_DISKLESS_SYNC; server.repl_diskless_sync_delay = CONFIG_DEFAULT_REPL_DISKLESS_SYNC_DELAY; server.slave_priority = CONFIG_DEFAULT_SLAVE_PRIORITY; server.slave_announce_ip = CONFIG_DEFAULT_SLAVE_ANNOUNCE_IP; server.slave_announce_port = CONFIG_DEFAULT_SLAVE_ANNOUNCE_PORT; server.master_repl_offset = 0; /* 复制部分的重新同步 */ server.repl_backlog = NULL; server.repl_backlog_size = CONFIG_DEFAULT_REPL_BACKLOG_SIZE; server.repl_backlog_histlen = 0; server.repl_backlog_idx = 0; server.repl_backlog_off = 0; server.repl_backlog_time_limit = CONFIG_DEFAULT_REPL_BACKLOG_TIME_LIMIT; server.repl_no_slaves_since = time(NULL); /* 客户端输出缓冲区限制 */ for (j = 0; j &lt; CLIENT_TYPE_OBUF_COUNT; j++) server.client_obuf_limits[j] = clientBufferLimitsDefaults[j]; /* 一些双精度浮点数常量初始化 */ R_Zero = 0.0; // 零 R_PosInf = 1.0/R_Zero; // 正无穷大 R_NegInf = -1.0/R_Zero; // 负无穷大 R_Nan = R_Zero/R_Zero; // 非数值 /* 初始化命令表，由于命令名称有可能在redis.conf中使用重命名命令修改， * 这里我们先初始化它们。 */ server.commands = dictCreate(&amp;commandTableDictType,NULL); server.orig_commands = dictCreate(&amp;commandTableDictType,NULL); populateCommandTable(); server.delCommand = lookupCommandByCString(\"del\"); server.multiCommand = lookupCommandByCString(\"multi\"); server.lpushCommand = lookupCommandByCString(\"lpush\"); server.lpopCommand = lookupCommandByCString(\"lpop\"); server.rpopCommand = lookupCommandByCString(\"rpop\"); server.sremCommand = lookupCommandByCString(\"srem\"); server.execCommand = lookupCommandByCString(\"exec\"); server.expireCommand = lookupCommandByCString(\"expire\"); server.pexpireCommand = lookupCommandByCString(\"pexpire\"); /* 慢操作日志 */ server.slowlog_log_slower_than = CONFIG_DEFAULT_SLOWLOG_LOG_SLOWER_THAN; server.slowlog_max_len = CONFIG_DEFAULT_SLOWLOG_MAX_LEN; /* 延迟监控 */ server.latency_monitor_threshold = CONFIG_DEFAULT_LATENCY_MONITOR_THRESHOLD; /* 调试 */ server.assert_failed = \"&lt;no assertion failed&gt;\"; server.assert_file = \"&lt;no file&gt;\"; server.assert_line = 0; server.bug_report_start = 0; server.watchdog_period = 0;&#125; 这里面东西很多，我们能发现有相当一部分代码是从一些宏定义中读取配置参数并赋值给server的相应属性。另外有几个比较重要的server属性需要说明一下，首先是server.runid，这个属性在每次Redis的启动过程中都会改变，它是一串随机值，用来标识一个特定的Redis运行实例，如果一个客户端两次连接Redis服务器runid不同，有两种可能，要么是连接到了另一个Redis实例，要么是原来的Redis已经重启导致runid改变。另外initServerConfig中还有很多文件路径的配置，比如配置文件路径、日志路径、rdb文件路径，aof文件路径等。还配置了Redis执行AOF的时机，默认的时机有三种：1小时内至少有1次修改就保存、5分钟内至少有100次修改就保存和1分钟内至少有10000次修改就保存，这个配置还可以在配置文件redis.conf中改变。 2. initServerinitServer函数代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137/* 初始化服务器（主要是一些动态属性） */void initServer(void) &#123; int j; signal(SIGHUP, SIG_IGN); signal(SIGPIPE, SIG_IGN); setupSignalHandlers(); // 注册信号处理器 if (server.syslog_enabled) &#123; openlog(server.syslog_ident, LOG_PID | LOG_NDELAY | LOG_NOWAIT, server.syslog_facility); &#125; server.pid = getpid(); // 获取进程pid server.current_client = NULL; // 当前连接的客户端，只用于崩溃报告中 server.clients = listCreate(); // 活动客户端列表 server.clients_to_close = listCreate(); // 需要异步关闭的客户端列表 server.slaves = listCreate(); // 从服务器列表 server.monitors = listCreate(); // 监控服务器列表 server.clients_pending_write = listCreate(); // server.slaveseldb = -1; /* Force to emit the first SELECT command. */ // server.unblocked_clients = listCreate(); // 在下一个事件循环中需要解锁的客户端列表 server.ready_keys = listCreate(); // server.clients_waiting_acks = listCreate(); // 等待响应的客户端列表 server.get_ack_from_slaves = 0; server.clients_paused = 0; // 如果当前客户端暂停则为true server.system_memory_size = zmalloc_get_memory_size(); // 系统报告的总内存 createSharedObjects(); // 创建一些共享对象 adjustOpenFilesLimit(); server.el = aeCreateEventLoop(server.maxclients+CONFIG_FDSET_INCR); // 创建事件循环 server.db = zmalloc(sizeof(redisDb)*server.dbnum); // 创建db实例 /* 在TCP socket上监听用户命令 */ if (server.port != 0 &amp;&amp; listenToPort(server.port,server.ipfd,&amp;server.ipfd_count) == C_ERR) exit(1); /* 打开Unix域socket监听 */ if (server.unixsocket != NULL) &#123; unlink(server.unixsocket); /* don't care if this fails */ server.sofd = anetUnixServer(server.neterr,server.unixsocket, server.unixsocketperm, server.tcp_backlog); if (server.sofd == ANET_ERR) &#123; serverLog(LL_WARNING, \"Opening Unix socket: %s\", server.neterr); exit(1); &#125; anetNonBlock(NULL,server.sofd); &#125; /* 没有监听的socket时程序终止 */ if (server.ipfd_count == 0 &amp;&amp; server.sofd &lt; 0) &#123; serverLog(LL_WARNING, \"Configured to not listen anywhere, exiting.\"); exit(1); &#125; /* 创建Redis数据库，并初始化其内部状态 */ for (j = 0; j &lt; server.dbnum; j++) &#123; server.db[j].dict = dictCreate(&amp;dbDictType,NULL); server.db[j].expires = dictCreate(&amp;keyptrDictType,NULL); server.db[j].blocking_keys = dictCreate(&amp;keylistDictType,NULL); server.db[j].ready_keys = dictCreate(&amp;setDictType,NULL); server.db[j].watched_keys = dictCreate(&amp;keylistDictType,NULL); server.db[j].eviction_pool = evictionPoolAlloc(); server.db[j].id = j; server.db[j].avg_ttl = 0; &#125; server.pubsub_channels = dictCreate(&amp;keylistDictType,NULL); server.pubsub_patterns = listCreate(); listSetFreeMethod(server.pubsub_patterns,freePubsubPattern); listSetMatchMethod(server.pubsub_patterns,listMatchPubsubPattern); server.cronloops = 0; server.rdb_child_pid = -1; server.aof_child_pid = -1; server.rdb_child_type = RDB_CHILD_TYPE_NONE; server.rdb_bgsave_scheduled = 0; aofRewriteBufferReset(); // 重置AOF rewrite buffer server.aof_buf = sdsempty(); server.lastsave = time(NULL); /* At startup we consider the DB saved. */ server.lastbgsave_try = 0; /* At startup we never tried to BGSAVE. */ server.rdb_save_time_last = -1; server.rdb_save_time_start = -1; server.dirty = 0; resetServerStats(); // 重置服务器状态 /* A few stats we don't want to reset: server startup time, and peak mem. */ server.stat_starttime = time(NULL); server.stat_peak_memory = 0; server.resident_set_size = 0; server.lastbgsave_status = C_OK; server.aof_last_write_status = C_OK; server.aof_last_write_errno = 0; server.repl_good_slaves_count = 0; updateCachedTime(); // 更新服务器缓存时间 /* 创建处理后台操作的时间事件 */ if(aeCreateTimeEvent(server.el, 1, serverCron, NULL, NULL) == AE_ERR) &#123; serverPanic(\"Can't create the serverCron time event.\"); exit(1); &#125; /* 创建处理通过TCP和Unix域socket的新连接的事件处理器 */ for (j = 0; j &lt; server.ipfd_count; j++) &#123; if (aeCreateFileEvent(server.el, server.ipfd[j], AE_READABLE, acceptTcpHandler,NULL) == AE_ERR) &#123; serverPanic( \"Unrecoverable error creating server.ipfd file event.\"); &#125; &#125; if (server.sofd &gt; 0 &amp;&amp; aeCreateFileEvent(server.el,server.sofd,AE_READABLE, acceptUnixHandler,NULL) == AE_ERR) serverPanic(\"Unrecoverable error creating server.sofd file event.\"); /* 在需要时打开AOF文件 */ if (server.aof_state == AOF_ON) &#123; server.aof_fd = open(server.aof_filename, O_WRONLY|O_APPEND|O_CREAT,0644); if (server.aof_fd == -1) &#123; serverLog(LL_WARNING, \"Can't open the append-only file: %s\", strerror(errno)); exit(1); &#125; &#125; /* 32位机器侠设置内存最大值为3GB，且不会淘汰key */ if (server.arch_bits == 32 &amp;&amp; server.maxmemory == 0) &#123; serverLog(LL_WARNING,\"Warning: 32 bit instance detected but no memory limit set. Setting 3 GB maxmemory limit with 'noeviction' policy now.\"); server.maxmemory = 3072LL*(1024*1024); /* 3 GB */ server.maxmemory_policy = MAXMEMORY_NO_EVICTION; &#125; if (server.cluster_enabled) clusterInit(); // Redis集群配置 replicationScriptCacheInit(); // 复制脚本缓存初始化 scriptingInit(1); // 复制脚本初始化 slowlogInit(); // 慢操作日志初始化 latencyMonitorInit(); // 延迟监控初始化 bioInit(); // 初始化后台线程&#125; initServer一开始会注册系统信号的处理器，然后初始化Redis服务器的部分参数，这些参数大都是动态的，比如pid、db、活动客户端列表之类的。再创建一些共享对象，这些共享对象被用到的频率很高，所以预先创建好，要使用时直接从内存中获得能提高效率。之后就是在TCP连接和Unix domain socket上监听客户端发来的命令。接着初始化所有db和它们的内部状态，创建后台操作的时间事件、打开AOF和设置内存的最大使用量（只有在32-bit的机器上）。最后就是做一些和集群、复制脚本、慢操作、延迟监控和后台进程相关的初始化操作。 3. aeSetBeforeSleepProc和aeMain在Redis服务器其中之前，还需要设置主事件循环，在这个循环中将会处理Redis的I/O操作。aeSetBeforeSleepProc负责设置事件循环中每次进入事件处理过程之前前调用的函数，aeMain为主事件循环函数。","categories":[{"name":"源码分析","slug":"源码分析","permalink":"https://nullcc.github.io/categories/源码分析/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://nullcc.github.io/tags/Redis/"}]},{"title":"理解Linux中的I/O多路复用","slug":"理解Linux中的IO多路复用","date":"2017-11-12T16:00:00.000Z","updated":"2022-04-15T03:41:13.036Z","comments":true,"path":"2017/11/13/理解Linux中的IO多路复用/","link":"","permalink":"https://nullcc.github.io/2017/11/13/理解Linux中的IO多路复用/","excerpt":"在Linux的I/O多路复用中，主要有这三个系统调用： select poll epoll","text":"在Linux的I/O多路复用中，主要有这三个系统调用： select poll epoll 1. selectselect的函数原型： 1int select(int nfds, fd_set *readfd, fd_set *writefd, fd_set *expectd, struct timeval *timeout); 参数解析 nfds：select监控的文件描述符的最大值+1，用于限制扫描的范围。 readfd：包含所有因状态变为可读而触发select返回的文件描述符。 writefd：包含所有因状态变为可写而触发select返回的文件描述符。 expectd：包含所有因发生异常而触发select返回的文件描述符。 timeout：select的超时时间。设为NULL表示阻塞住，直到有fd就绪，设为0表示不阻塞直接返回，设为一个大于0的值会让select阻塞一个指定的时间，这期间一旦有fd就绪就返回，否则当超过这个超时时间时，select也返回。 返回值 成功时返回一个大于0的整数。 超时返回0。 出错返回-1。 函数原理首先把需要监控的文件描述符加载到一个fd_set类型的集合中，然后调用select监控集合中的所有文件描述符。假设我们把timeout设为NULL，阻塞select调用，并且在readfd参数中传入了一个fd_set类型的集合，表示监控这个集合中所有文件描述符的读就绪事件。一旦集合中有文件描述符读就绪，select马上返回一个大于0的整数。然后调用方需要遍历这个fd_set类型的集合，对每个文件描述符使用FD_ISSET判断是否就绪，如果就绪了就处理这个文件描述符。需要注意的是，在每次调用select之前，需要使用FD_ZERO对fd_set类型的集合中每个文件描述符的就绪状态清零。 一些问题select有几个问题，首先select一次性只能监控FD_SETSIZE个文件描述符，在大多数Linux系统中这个数字是1024。我们可以修改这个宏来增加这个数字，不过由于select调用在内核中会遍历整个fd集合，集合越大效率越低，所以也不是将FD_SETSIZE设置为越大越好。第二个问题是每次调用select时，都需要把fd集合从用户态拷贝到内核态，fd集合不大时还好，一旦fd集合很大，这种拷贝的开销也会对系统产生影响。 2. pollpoll的函数原型： 1int poll(struct pollfd* fds, nfds_t nfds, int timeout); pollfd结构： 123456struct pollfd&#123; int fd; // 文件描述符 short events; // 告诉poll监听fd上的哪些事件 short revents; // 内核负责修改，用来通知应用程序fd上发生的实际事件&#125;; 参数解析 fds：一个pollfd类型的数组。 nfds：监听事件集合大小。 timeout：超时时间，为-1时会一直阻塞直到有fd就绪，为0表示立即返回，为一个大于0的整数时会让poll阻塞一个指定的时间，这期间一旦有fd就绪就返回，否则当超过这个超时时间时，poll也返回。 返回值 成功时返回一个大于0的整数，表示就绪的文件描述符的个数。 超时返回0。 出错返回-1。 函数原理poll中没有select对监听的fd个数的限制，也不再需要三个fd集合来分别存放不同事件类型的fd了，我们只需要在pollfd中指定fd的事件类型即可。而且poll也不需要像在select中那样每次调用前需要清零一次fd集合。 一些问题poll和select一样，还是需要在内核中遍历fd集合，另外在调用时也需要把fd集合从用户态拷贝到内核态，这在fd集合比较大的时候效率较低。 3. epollepoll有一组函数，如下： 创建事件表的函数原型1int epoll_create(int size); 参数： size表示要创建多大的事件表 返回值： 返回事件表的文件描述符 操作事件表的函数原型1int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); 参数： epfd：事件表文件描述符 op：操作类型，有：(1). EPOLL_CTL_ADD(2). EPOLL_CTL_MOD(3). EPOLL_CTL_DEL fd：要操作的文件描述符 事件 其中epoll_event是一个结构体： 12345struct epoll_event&#123; int events; epoll_data_t data; // 一个union，里面是事件的数据&#125;; 返回值： 成功返回0。 失败返回-1。 事件监听的函数原型1int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); 参数： epfd：事件表文件描述符 events：当有事件发生时，内核会将和事件有关的信息放入events中 maxevents：一次epoll中内核返回的的最大事件数 timeout：监听超时时间 返回值： 成功时返回大于0的整数，表示就绪的fd个数 失败返回-1 超时返回0 函数原理epoll相比于select和poll的差别比较大，epoll不需要在每次调用时都将数据从用户空间拷贝到内核空间，取而代之的是把事件表以共享内存的方式供用户空间和内核空间使用。其次，调用epoll_wait时只会把就绪的事件从事件表拷贝到events参数中（通过注册的回调函数），应用程序只要遍历这个已就绪的fd集合即可，这就避免了select和poll每次都要遍历整个fd集合的问题，提高了效率。另外epoll的时间复杂度是O(1)。 epoll有两种模式，LT和ET。LT是水平触发(Level Trigger)，ET是边缘触发(Edge Trigger)。 水平触发：当epoll监测到某个fd上有事件发生时，应用程序若没有立即处理，在下次epoll时，还会再次触发此fd上的事件。边缘触发：当epoll监测到某个fd上有事件发生时，内核只通知应用程序一次，应用程序必须立即处理，否则这个事件相当于被忽略了。 总结epoll的时间复杂度虽然比select和poll低，但也未必总是效率比它们高。在一个有非常多活跃fd的集合中，epoll由于每次都要触发回调函数，效率会降低，此时遍历fd集合来处理反而效率更高。epoll适合fd集合很大但大部分fd不活跃的场景。","categories":[{"name":"系统编程","slug":"系统编程","permalink":"https://nullcc.github.io/categories/系统编程/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://nullcc.github.io/tags/Linux/"},{"name":"I/O多路复用","slug":"I-O多路复用","permalink":"https://nullcc.github.io/tags/I-O多路复用/"}]},{"title":"垃圾回收(GC)算法介绍(1)——GC标记-清除算法","slug":"垃圾回收(GC)算法介绍(1)——GC标记-清除算法","date":"2017-11-10T16:00:00.000Z","updated":"2022-04-15T03:41:13.031Z","comments":true,"path":"2017/11/11/垃圾回收(GC)算法介绍(1)——GC标记-清除算法/","link":"","permalink":"https://nullcc.github.io/2017/11/11/垃圾回收(GC)算法介绍(1)——GC标记-清除算法/","excerpt":"在编程领域，垃圾回收(Garbage Collection)，简称GC(简便起见，以下都把垃圾回收用GC代替)。GC负责把堆上不用的内存进行回收，以便之后可以再次进行分配。回忆一下C中对内存申请，一般来说malloc和free需要成对地出现，malloc为对象申请内存空间，free负责释放申请的内存空间。但如果只有malloc但却不free，就会造成堆上的内存无法回收，造成内存泄漏。","text":"在编程领域，垃圾回收(Garbage Collection)，简称GC(简便起见，以下都把垃圾回收用GC代替)。GC负责把堆上不用的内存进行回收，以便之后可以再次进行分配。回忆一下C中对内存申请，一般来说malloc和free需要成对地出现，malloc为对象申请内存空间，free负责释放申请的内存空间。但如果只有malloc但却不free，就会造成堆上的内存无法回收，造成内存泄漏。 一些高级的语言中都内置了GC，比如Python、Java、Ruby等。编程语言内置GC对程序员来说是一个不小的福利，我们再也不用惦记申请内存后要释放了，没有了内存泄漏，应用程序更安全了。 目前流行的GC算法主要有以下三种： 1.标记-清除算法 2.引用计数法 3.GC复制算法 还有一些其他的GC算法，但基本上是上面三种算法的衍生算法。 GC标记-清除算法(Mark-Sweep)概述标记-清除算法可以说是最古老的一种GC算法了，于1960年由Lisp之父John McCarthy在其论文中发布。顾名思义，标记-清除算法分为两个阶段： 1.标记 2.清除 标记阶段，将堆上所有的活动对象打上标记。清除阶段，将堆上没有活动标记的非活动对象所占用的内存放入一个空闲链表(free_list)中。当需要在堆上申请新内存时，遍历空闲链表，找到一个合适大小的内存块进行分配。 上面的描述是非常抽象的，只说明了一个大概过程，有很多细节被隐藏了。一个大致的标记-清除过程用伪代码来表示就是： 1234mark_sweep()&#123; mark_phase() sweep_phase()&#125; 在说明标记-清除算法的详细过程之前，先来看看堆上的活动对象和非活动对象。 一个堆就是一块连续的内存空间，一个堆有一个根，从这个根出发，可以直接访问到所有的顶层活动对象，这些顶层活动对象可能还会引用活动的子对象，这样一层一层引用下去，就能找到堆上所有的活动对象，就可以对它们实施标记。那些没有被标记的对象自然就是非活动对象了，它们已经无法被访问到，对程序来说已无用处，可以说是“垃圾”了。 来看一下mark_phase，假设堆的根为$root： 1234567891011121314mark_phase()&#123; for(obj in $root)&#123; mark(obj) &#125;&#125;mark(obj)&#123; if(!obj.mark)&#123; obj.mark = True for(child in obj.children)&#123; mark(child) &#125; &#125;&#125; 在mark_phase中，遍历了堆的根上的所有顶层对象，mark函数对传入的对象递归地进行标记。 再看看’sweep_phase’，假设空闲链表为$free_list，堆的首地址为$heap_start，堆的尾地址为$heap_end： 123456789101112sweep_phase()&#123; sweeping = $heap_start while(sweeping &lt; $heap_end)&#123; if(sweeping.mark)&#123; sweeping.mark = False &#125; else &#123; sweeping.next = $free_list $free_list = sweeping &#125; sweeping += sweeping.size &#125;&#125; 在sweep_phase中，使用sweeping遍历了整个堆，对所有已被标记的活动对象，把它们的mark置为False，在下一次mark_phase的时候会再次对这些活动对象设置标记。对于非活动对象，就将这些对象链接到空闲链表$free_list的头部。 整个标记-清除过程之后，堆上所有被回收的内存块就都被空闲链表收集了，它们以一个链表的形式存在。当需要在堆上申请内存时，就遍历空闲链表，找到合适的内存块就返回它，如果遍历后没有找到可以分配的块，就返回NULL。 空闲块分配策略在之前的描述中，当需要在堆上申请内存时，遍历空闲链表，找到合适的内存块就返回它。这里的“合适”一词说的很模糊，一般来说有三种情况： 1.First-Fit: 在空闲链表中第一个找到的大小够用的内存块2.Best-Fit: 在空闲链表中最适合所申请块大小的内存块（大小够用的最小内存块）3.Worst-Fit: 在空闲链表中的最大的那个内存块 Worst-Fit由于要找到空闲链表中的最大内存块进行分配，势必要进行分割，有可能会造成很多内存碎片。在申请内存时，考虑到应该尽可能快地返回，一般还是会选择First-Fit。 空闲空间合并在清除阶段，如果发现两个紧邻的空闲内存块时，会将它们合并形成一个更大的内存块，这样做有助于减少空闲内存的碎片化。 优点和缺点标记-清除算法的优点是实现简单。缺点是容易形成内存碎片，在极端情况下会发生堆上空闲内存的总空间比所申请的空间size大，但没有哪个单个空闲块的大小够用的情况。还有一个问题就是分配速度，由于采用空闲链表，标记-清除算法的空闲内存时不连续的，每次分配都需要遍历空闲链表，因此它的分配速度是不稳定的。在后面会介绍几种缓解这些缺点的方法。 优化方案对于标记-清除算法的缺点，有一些优化措施，这些优化措施在一定程度上可以缓解这些缺点带来的问题，但无法彻底根除。 1. 使用多个空闲链表使用First-Fit时，随着时间的推移，堆上的空闲内存必然会出现很多碎片，造成申请内存时效率降低。由于应用程序很少会一下申请很大的内存空间，所以我们可以维护多个空闲链表，比如第一个空闲链表里都是2字节，第二个都是2字节，以此类推。但我们不可能枚举所有情况，所以可以类似这样处理：对100以下的各个数字，都建立相应的空闲链表，第一个空闲链表里都保存1字节的内存块，第二个都保存2字节的内存块，一直到第100个都保存100字节的内存块。大于100字节的内存块都保存在第101个空闲链表中。在申请内存时，查看相应大小的空闲链表中是否有可用内存块，有就分配，没有就从第101个内存块按照First-Fit去找。 2. 位图标记法之前我们都是在对象头部进行标记，这样就会改变对象的状态。对于Linux等使用写时复制的系统来说，这会导致很多不必要的复制开销。我们知道一个堆对应一块连续的内存空间，于是很容易想到可以使用位图的方式来记录对象的标记状态。对于堆上每个字节，使用1位来表示，直观上来说，我们可以想象有一个位数组，数组长度等于堆上内存的字节数。在标记阶段遍历堆寻找活动对象时，在相应的位图位置上进行标记，在清除阶段，我们遍历堆上的对象，每遍历一个对象，就在位图上查询该对象是否是活动对象，如果不是活动对象，就把这块内存加入空闲链表。遍历完堆后，直接把整个位图的所有位设为0，此时空闲链表也收集了所有空闲内存。使用位图标记法有个好处，它不会导致写时复制的问题，另外需要注意的是，一个应用程序可能有多个堆，有几个堆就要有几个相对应的位图。 3. 延迟清除法普通情况下，标记-清除算法在清除阶段的开销和堆的大小成正比，堆越大，应用程序因的最大暂停时间就越长。延迟清除算法把标记和清除操作延后到申请内存时。具体来说就是，在申请内存时，首先进行一次lazy_sweep，lazy_sweep会遍历堆进行标记和清除操作，遇到活动对象就标记，遇到非活动对象就清除，如果某次清除出来的内存空间大于或等于申请的size，直接返回这块内存。如果这次lazy_sweep没有找到合适的内存块，就会进行一次标记过程，然后再次调用lazy_sweep，如果还是找不到合适的内存块，说明在堆上申请内存失败了。这里需要注意的是，延迟清除法每次进行lazy_sweep时，都是从上一次lazy_sweep结束的地方开始，这和之前我们看到的每次都从堆的起始处开始有所不同。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"垃圾回收","slug":"垃圾回收","permalink":"https://nullcc.github.io/tags/垃圾回收/"}]},{"title":"垃圾回收(GC)算法介绍(2)——GC引用计数算法","slug":"垃圾回收(GC)算法介绍(2)——GC引用计数算法","date":"2017-11-10T16:00:00.000Z","updated":"2022-04-15T03:41:13.031Z","comments":true,"path":"2017/11/11/垃圾回收(GC)算法介绍(2)——GC引用计数算法/","link":"","permalink":"https://nullcc.github.io/2017/11/11/垃圾回收(GC)算法介绍(2)——GC引用计数算法/","excerpt":"GC引用计数法概述由于GC会清除那些再也无法被引用到的对象，很自然地可以想到我们可以在对象上设置一个计数器来记录它被引用的次数，示意图如下：","text":"GC引用计数法概述由于GC会清除那些再也无法被引用到的对象，很自然地可以想到我们可以在对象上设置一个计数器来记录它被引用的次数，示意图如下： 引用技术法的思想很简单，在创建对象和将对象赋值给某个变量时，将对象的引用计数加1，在移除对象和某个变量的引用关系时，将对象的运营计数减1，当对象的引用计数变为0时，递归地将该对象引用的子对象的引用计数器减1，并把该对象的内存块加入空闲链表中（没错，这里又出现了空闲链表）。在之前的标记-清除算法中，应用程序有一个明确且独立的GC过程来回收非活跃对象，引用计数法没有这样的独立的过程，它在通过增减对象的引用计数器来判别活跃对象和非活跃对象，然后在计数器值为0的时候回收对象，这种做法可以在对象不活跃的时候立即回收它。 先看一下在引用计数法下创建新对象的过程： 12345678new_obj(size)&#123; obj = get_free_space($free_list, size) if(obj == NULL) return allocation_failed() else obj.ref_cnt = 1 return obj&#125; 在引用计数法中创建新对象时，直接从空闲链表中查找可用内存块，如果找不到可用内存块，新对象创建直接就失败了，如果找到了，就将新对象的引用计数加1并返回这个对象。注意到get_free_space一旦失败，就说明空闲链表中没有合适的空间供分配了，因为在引用计数法中，除了空闲链表中的对象以外，堆上其他的对象都是活跃的。 然后是更新对象指针操作的过程： 12345678910111213141516171819update_obj_ptr(obj)&#123; incr_ref_cnt(obj) decr_ref_cnt(*ptr) *ptr = obj&#125;incr_ref_cnt(obj)&#123; obj.ref_cnt++&#125;decr_ref_cnt(obj)&#123; obj.ref_cnt-- if(obj.ref_cnt == 0)&#123; for(child in obj.children)&#123; decr_ref_cnt(child) &#125; add_to_free_list(obj) &#125;&#125; 在将一个指针指向某个对象时，首先要将对象的引用计数加1，然后将原指针指向的对象的引用计数减1，再将这个指针指向这个对象。在update_obj_ptr中之所以先incr_ref_cnt(obj)再decr_ref_cnt(*ptr)是为了防止obj和ptr是同一个对象，如果obj和ptr是同一个对象，我们先decr_ref_cnt(*ptr)，这个对象的引用计数如果为0了，就被回收了，之后incr_ref_cnt(obj)就没用了。另外注意到decr_ref_cnt中当对象的ref_cnt减为0时，要先对它引用的所有对象递归执行decr_ref_cnt后再将其加入空闲链表中。 优点和缺点引用计数法可以在对象不活跃时（引用计数为0）立刻回收其内存。因此可以保证堆上时时刻刻都没有垃圾对象的存在（先不考虑循环引用导致无法回收的情况）。 引用计数法的最大暂停时间短。由于没有了独立的GC过程，而且不需要遍历整个堆来标记和清除对象，取而代之的是在对象引用计数为0时立即回收对象，这相当于将GC过程“分摊”到了每个对象上，不会有最大暂停时间特别长的情况发生。 引用计数法也有一些问题，引用计数的增减开销在一些情况下会比较大，比如一些根引用的指针更新非常频繁，此时这种开销是不能忽视的。另外对象引用计数器本身是需要空间的，而计数器要占用多少位也是一个问题，理论上系统内存可寻址的范围越大，对象计数器占用的空间就要越大，这样在一些小对象上就会出现计数器空间比对象本身的域还要大的情况，内存空间利用率就会降低。还有一个问题是循环引用的问题，假设两个对象A和B，A引用B，B也引用A，除此之外它们都没有其他引用关系了，这个时候A和B就形成了循环引用，变成一个“孤岛”，且它们的引用计数都是1，按照引用计数法的要求，它们将无法被回收，造成内存泄漏。 优化方案1. 延迟引用计数法针对跟引用指针会有非常频繁的更新导致增减对象计数器的任务繁重这一问题，我们直接可以想到的一种方案是对根引用对象不维护计数器。非根引用对象更新指针时调用update_ptr，根引用对象直接使用*$ptr = obj，就绕过了这个问题。但这么做还是不行，因为根引用对象没有计数器值了，可能会被当成是垃圾回收掉。对此，可以使用一个ZCT（Zero Count Table），这个表专门用来记录那些计数器值经过decr_ref_cnt后变为0的对象，延迟引用计数法中，引用计数器值为0的对象不一定就是垃圾。如下图： 我们还需要修改decr_ref_cnt函数以适应这种方法： 12345678decr_ref_cnt(obj)&#123; obj.ref_cnt--; if(obj.ref_cnt == 0)&#123; if(is_full($zct) == TRUE) scan_zct() push($zct, obj) &#125;&#125; 在decr_ref_cnt函数中，减少对象的引用计数后，如果其引用计数为0，需要把该对象放到$zct中，如果此时$zct满了，执行scan_zct函数来清理对象。 还需要修改new_obj函数： 123456789101112new_obj(size)&#123; obj = get_free_space($free_list, size) if(obj == NULL)&#123; scan_zct() obj = get_free_space($free_list, size) if(obj == NULL)&#123; allocation_failed() &#125; &#125; obj.ref_cnt = 1 return obj&#125; 在new_obj函数中，当第一次内存分配失败时，调用scan_zct函数来清理对象，之后再次申请分配，如果还是失败，说明当前堆上没有可用内存块了，直接失败。 再来看看scan_zct函数： 1234567891011121314scan_zct()&#123; for(root_obj in $root) root_obj.ref_cnt++ for(obj in $zct)&#123; if(obj.ref_cnt == 0)&#123; remove_from_zct($zct, obj) delete(obj) &#125; &#125; for(root_obj in $root) root_obj.ref_cnt--&#125; scan_zct函数首先对所有根引用对象的引用计数加1，然后遍历$zct，将引用计数为0的对象（这些对象肯定不是根对象了）清除出$zct，然后调用delete(obj)删除它们，最后，很重要的一点，需要把所有根引用对象的引用计数减1。 最后看一下delete函数： 123456789delete(obj)&#123; for (child in obj.children)&#123; child.ref_cnt--; if(child.ref_cnt == 0)&#123; delete(child) &#125; &#125; add_to_free_list(obj)&#125; delete函数负责递归地将对象所引用的子对象的引用计数减1，并将对象加入到空闲链表中。延迟引用计数法的优点就是可以大大减轻那些根引用对象指针被频繁更新导致计数器更新操作繁重的问题。它的缺点也很明显，由于在scan_zct中统一做垃圾回收，无法立即收回内存，而且scan_zct的开销和$zct的大小成正比，这会导致GC的最大暂停时间增加。当想要减少最大暂停时间时，势必要减小$zct的大小，但这样一来就需要更频繁地调用scan_zct了，导致吞吐量下降。 2. sticky引用计数法之前提到的引用计数法有一个问题，需要确定给引用计数器分配多少空间。我们假设给它分配5 bits，计数范围0~31，当对象引用计数超过31时，计数器就会溢出。针对计数器溢出的情况，有两个办法，一是完全不理会，不再去增减计数器溢出对象的计数器的值。二是使用标记-清除算法。 针对第一种，已经有研究表明，绝大多数对象的引用计数只会在0和1之间变化，这些对象创建出来没多久就“死”了，很少有对象的引用计数会有非常大的值，如果有引用计数很大的对象则说明这些对象很重要，短期内不可能被销毁，因此再去操作它的引用计数意义不大。 第二种方法，使用标记-清除算法来处理： 12345mark_sweep_for_counter_overflow()&#123; reset_all_obj_ref_cnt() mark_phase() sweep_phase()&#125; 首先要将堆上的所有对象引用计数设置为0。然后进入标记阶段和清除阶段。 1234567891011121314mark_phase()&#123; for(root_obj in $root) push(root_obj, $mark_stack) while(is_empty($mark_stack) == FALSE)&#123; obj = pop($mark_stack) obj.ref_cnt++ if(obj.ref_cnt == 1)&#123; for(child in obj)&#123; push(child, $mark_stack) &#125; &#125; &#125;&#125; 标记阶段，首先把所有根引用对象都放入$mark_stack，然后依次从$mark_stack中取出对象，将其引用计数加1。这里要注意只能对各个对象和它们的子对象进栈一次，以免造成死循环，其中if(obj.ref_cnt == 1)用来判断这种情况。当$mark_stack为空时结束标记阶段。 接下来是清除阶段： 123456789sweep_phase()&#123; sweeping = $heap_start while(sweeping &lt; $heap_end)&#123; if(sweeping.ref_cnt == 0)&#123; add_to_free_list(sweeping) sweeping += sweeping.size &#125; &#125;&#125; 清除阶段遍历整个堆，将引用计数为0的对象加入空闲链表。 sticky引用计数法使用了一种标记-清除算法的变体，它可以清除循环引用。这是因为每次处理都会在最开始把所有堆上的对象的引用计数设为0，然后从根引用对象开始递归地将所有可以引用到的对象的计数器加1，很显然循环引用的对象在计数器设置为0后并不能在标记阶段被找到并设置引用计数，那么它们将在清除阶段被清除掉。 3. 1位引用计数法1位引用计数法的计数器只有1位大小，只有0和1两个取值，分分钟会造成溢出。不过据调查显示，很少有对象的引用计数大于或等于2，大部分对象创建不就后就被回收了。我们可以用计数器的0值表示引用计数为1，计数器的1值表示引用计数大于或等于2。 由于计数器取值只有0和1两种，之前提到的引用计数法都是让对象持有计数器，在1位引用计数法中，我们让指针持有计数器。将计数器值0的指针称为UNIQUE指针，计数器值1的指针称为MULTIPLE指针。 在更新对象指针的时候，之前的做法是先增加对象的引用计数，然后减少原指针指向对象的引用计数，最后将这个指针指向对象。1位引用计数法由于将计数器保存在指针中而非对象中，因此使用的是“复制指针”操作而不是“更新指针”。 下面是1位引用计数法复制指针的示意图： 最开始A中的指针指向C，现在要将A中的指针指向D，实际上可以将B中指向C的指针复制到A中完成这个操作，我们使用copy_ptr函数来执行这个过程，伪代码如下： 123456789101112copy_ptr(dest_ptr, src_ptr)&#123; delete_ptr(dest_ptr) *dest_ptr = *src_ptr set_multiple_tag(dest_ptr) if(tag(src_ptr) == UNIQUE) set_multiple_tag(src_ptr)&#125;delete_ptr(ptr)&#123; if(tag(ptr) == UNIQUE) add_to_free_list(*ptr)&#125; 1位引用计数法的优点是对象本身不需要保存计数器，只在指针中使用1位来保存计数器，计数器只有0和1两种取值，不会占用太多空间且有效利用指针的空间。另外一方面，由于只是复制指针，并不需要解引用指针获取对象，避免了内存寻址开销。1位引用计数法的缺点是引用计数器溢出的问题，当计数器的tag变成MULTIPLE后，是无法判断能否回收的，这里只有从UNIQUE指针到MULTIPLE指针的单向变化，无法从MULTIPLE指针变回UNIQUE指针，有些MULTIPLE指针引用的对象可能是需要回收的，但在这种场景下肯定是无法回收了。 4. 部分标记-清除算法引用计数法的一大问题就是没办法清除循环引用的对象，我们知道在标记-清除算法不存在循环引用对象群无法被清除的问题。因此很容易想到正常情况下用引用计数法，在某个时刻使用标记-清除算法来处理。不过存在循环引用关系的对象是极少数，为了回收这么点垃圾就每次固定在某个时间点执行标记-清除算法成本有点高，会有很多无用功。改进的方式是只对“可能存在循环引用”的对象群进行标记-清除，这种方式被称为部分标记-清除算法。 那么我们就需要识别存在循环引用关系的对象，先来看看循环引用是怎么产生的： 循环引用产生的条件： 1.一组对象互相引用构成闭环2.删除所有外部到这组对象的引用 根据上图和循环引用产生的条件，我们可以做出一点假设： 当移除一个对象的某个外部引用时，如果这个对象的引用计数递减后不为0，则它和它所引用的对象可能构成循环引用关系。 由于一个对象可能存在多个外部引用，移除一个外部引用后引用计数不为0是完全可能的，因此在上述假设的基础上我们还需要做进一步判断。于是就有了部分标记-清除算法。 部分标记-清除算法寻找的是非活跃对象，这和之前讨论的标记-清除算法寻找活跃对象是不同的。部分标记-清除算法将对象分为四种颜色来管理： 1.黑(BLACK)：绝对不是垃圾的对象2.白(WHITE)：绝对是垃圾的对象3.灰(GRAY)：被搜索完毕的对象4.阴影(HATCH)：可能是循环引用的垃圾对象 我们在对象头部分配2位的空间来保存这个信息，命名为obj.color，00~11可以表示这四种颜色。其中有一个$hatch_queue用来存放所有被标记为HATCH的对象。 我们构造一个堆，初始状态如下： 现在将root到A的引用删除，这会在内部调用一次decr_ref_cnt，首先来看decr_ref_cnt函数： 123456789decr_ref_cnt(obj)&#123; obj.ref_cnt-- if(obj.ref_cnt == 0) delete(obj) else if(obj.color != HATCH)&#123; obj.color = HATCH enqueue($hatch_queue, obj) &#125;&#125; 这个decr_ref_cnt和以前有所不同，在递减对象的引用计数后，如果为0就回收对象，否则说明对象还被其他对象引用，有可能是循环引用对象群的一个成员，我们在标记它为HATCH后（如果已经是HATCH就说明对象已经在$hatch_queue中，此时什么也不做），将它入队$hatch_queue，之后要遍历$hatch_queue找出循环引用的对象，经过： 还需要修改new_obj函数： 123456789101112new_obj(size)&#123; obj = get_free_space($free_list, size) if(obj != NULL)&#123; obj.ref_cnt = 1 obj.color = BLACK return obj &#125; else if (is_empty($hatch_queue) == FALSE)&#123; scan_hatch_queue() return new_obj(size) &#125; else allocation_failed()&#125; 创建新对象时，ref_cnt初始为1，并且要把color设置为BLACK。对象创建失败时，需要扫描$hatch_queue尝试释放循环引用对象，然后递归调用new_obj，否则创建对象失败。 scan_hatch_queue函数搜索整个$hatch_queue，目的是找出循环引用的对象然后释放。 12345678910scan_hatch_queue()&#123; obj = dequeue($hatch_queue) if(obj.color == HATCH)&#123; paint_gray(obj) scan_gray(obj) collect_white(obj) &#125; else if(is_empty($hatch_queue) == FLASE)&#123; scan_hatch_queue() &#125;&#125; scan_hatch_queue函数每次从$hatch_queue中出队一个对象，如果对象颜色为HATCH，就执行paint_gray、scan_gray和collect_white，否则如果$hatch_queue不为空，递归调用scan_hatch_queue。 paint_gray函数递归地标识一个对象和它的所有子对象，将它们的color设置为GRAY，并： 123456789paint_gray(obj)&#123; if(obj.color == BLACK || obj.color == HATCH)&#123; obj.color = GRAY for(child in obj.children)&#123; child.ref_cnt-- paint_gray(child) &#125; &#125;&#125; 在理解paint_gray函数之前，我们需要理解一个规则，假设有A-&gt;B-&gt;C-&gt;A这样的引用关系： A、B、C三个对象互相引用，引用计数都是1。如果从A出发，标记它，然后递归地将它的子对象也标记且引用计数递减，直到能到达的所有对象都被标记完。最后的结果就是形成循环引用的那几个对象都被标记且引用计数都变成0。至于那些被在循环引用群里的对象引用的其他对象，有两种情况： 1.这个对象只被循环引用群里的对象所引用，在经过上面的过程后，这个对象的引用计数也会变成0而被回收。2.这个对象除了被循环引用群里的对象所引用，还被其他对象引用到（假设有1个对象引用它），在经过上面的过程后，这个对象的引用计数大于0。 执行paint_gray时候的示例堆： 因此我们可以得到，在经过paint_gray的过程后，引用计数为0的对象都是需要回收的，我们将它的color变为WHITE，引用计数大于0的对象将其color设置为BLACK，于是就有了scan_gray函数： 1234567891011scan_gray(obj)&#123; if(obj.color == WHITE)&#123; if(obj.ref_cnt &gt; 0) paint_black(obj) else &#123; obj.color = WHITE for(child in obj.children) scan_gray(child) &#125; &#125;&#125; 执行scan_gray时候的示例堆： 最后是collect_white函数： 12345678collect_white(obj)&#123; if(obj.color == WHITE)&#123; for(child in obj.children)&#123; collect_white(child) &#125; add_to_free_list(obj) &#125;&#125; 调用collect_white之后，就回收了循环引用对象。 局部标记-清除算法的优点是可以发现并清除循环引用对象群。缺点也很明显，开销比较大，对于一个可能是循环引用的对象，需要执行paint_gray、scan_gray和collect_white，相当于遍历了三次这个对象的和它的子对象，如果需要检查的对象比较多，代价就太大了，并且也失去了引用计数法立即回收对象的这个优势。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"垃圾回收","slug":"垃圾回收","permalink":"https://nullcc.github.io/tags/垃圾回收/"}]},{"title":"Redis中的底层数据结构(1)——双端链表","slug":"Redis中的底层数据结构(1)——双端链表","date":"2017-11-09T16:00:00.000Z","updated":"2022-04-15T03:41:13.018Z","comments":true,"path":"2017/11/10/Redis中的底层数据结构(1)——双端链表/","link":"","permalink":"https://nullcc.github.io/2017/11/10/Redis中的底层数据结构(1)——双端链表/","excerpt":"本文将详细说明Redis中双端链表的实现。 在Redis源码（这里使用3.2.11版本）中，双端链表的实现在adlist.h和adlist.c中。","text":"本文将详细说明Redis中双端链表的实现。 在Redis源码（这里使用3.2.11版本）中，双端链表的实现在adlist.h和adlist.c中。 双端链表中的数据结构双端链表是Redis中列表键的内部实现之一。 先来看一下双端链表的数据结构： 12345678910111213141516171819202122/* 双端链表节点数据结构 */typedef struct listNode &#123; struct listNode *prev; // 前一个节点指针 struct listNode *next; // 后一个节点指针 void *value; // 节点的数据域&#125; listNode;/* 双端链表迭代器数据结构 */typedef struct listIter &#123; listNode *next; // 下一个节点指针 int direction; // 迭代方向&#125; listIter;/* 双端链表数据结构 */typedef struct list &#123; listNode *head; // 链表头节点指针 listNode *tail; // 链表尾节点指针 void *(*dup)(void *ptr); // 链表节点复制函数 void (*free)(void *ptr); // 链表节点释放函数 int (*match)(void *ptr, void *key); // 链表节点比较函数 unsigned long len; // 链表长度&#125; list; 双端链表节点结构的prev指向前向节点，next指向后继节点，value是节点的数据域指针。双端链表结构的header和tail分别指向链表的头节点和尾节点，方便从链表头部和尾部开始遍历链表。len保存了链表长度，即节点数量。双端链表迭代器数据结构保存了next表示下一个节点的指针，direction表示迭代方向（头-&gt;尾/尾-&gt;头）。注意三个和节点有关的函数： void *(*dup)(void *ptr);声明了一个节点复制的函数指针，被指向的函数返回值类型为void *。 void (*free)(void *ptr);声明了一个节点释放的函数指针，被指向的函数返回值类型为void。 int (*match)(void *ptr, void *key);声明了一个节点比较的函数指针，被指向的函数返回值类型为int。 双端链表的宏和函数原型先看双端链表的一组宏： 123456789101112#define listLength(l) ((l)-&gt;len) // 获取链表长度#define listFirst(l) ((l)-&gt;head) // 获取链表头节点#define listLast(l) ((l)-&gt;tail) // 获取链表尾节点#define listPrevNode(n) ((n)-&gt;prev) // 获取当前节点的前一个节点#define listNextNode(n) ((n)-&gt;next) // 获取当前节点的后一个节点#define listNodeValue(n) ((n)-&gt;value) // 获取当前节点的数据#define listSetDupMethod(l,m) ((l)-&gt;dup = (m)) // 设置链表的节点复制函数#define listSetFreeMethod(l,m) ((l)-&gt;free = (m)) // 设置链表的节点数据域释放函数#define listSetMatchMethod(l,m) ((l)-&gt;match = (m)) // 设置链表的节点比较函数#define listGetDupMethod(l) ((l)-&gt;dup) // 获取链表的节点复制函数#define listGetFree(l) ((l)-&gt;free) // 获取链表的节点释放函数#define listGetMatchMethod(l) ((l)-&gt;match) // 获取链表的节点比较函数 这些宏实际上就是一些结构体指针的间接引用赋值和取值，用来代替函数实现一些简单的操作，这样做相对于定义函数可以提高效率，毕竟上面这些宏如果用函数实现开销要大一些。 下面是双端链表相关的函数原型： 123456789101112131415list *listCreate(void); // 创建一个空的链表void listRelease(list *list); // 释放一个链表list *listAddNodeHead(list *list, void *value); // 为链表添加头节点list *listAddNodeTail(list *list, void *value); // 为链表添加尾节点list *listInsertNode(list *list, listNode *old_node, void *value, int after); // 为链表插入节点void listDelNode(list *list, listNode *node); // 删除指定节点listIter *listGetIterator(list *list, int direction); // 获取指定迭代方向的链表迭代器listNode *listNext(listIter *iter); // 使用迭代器获取下一个节点void listReleaseIterator(listIter *iter); // 释放链表迭代器list *listDup(list *orig); // 复制一个链表listNode *listSearchKey(list *list, void *key); // 在链表中查找数据域等于key的节点listNode *listIndex(list *list, long index); // 在链表中查找指定索引的节点void listRewind(list *list, listIter *li); // 使迭代器的当前位置回到链表头，正向迭代void listRewindTail(list *list, listIter *li); // 使迭代器的当前位置回到链表尾，反向迭代void listRotate(list *list); // 移除链表当前的尾节点，并把它设置为头节点 双端链表的函数实现下面的代码几乎包含了Redis双端链表的所有函数定义。双端链表作为一种通用数据结构，在现实中非常常用，其中的插入、删除、迭代、查找等操作也是数据结构课程中链表相关的基础知识。这部分的内容比较简单，Redis的源码实现也非常简练高效，且代码质量很好。 listCreate函数创建一个新链表。被创建的链表可以使用AlFreeList()函数释放，但是每个节点的数据域需要在调用AlFreeList()函数之前调用用户自定义的节点释放函数来释放。 12345678910111213list *listCreate(void)&#123; struct list *list; if ((list = zmalloc(sizeof(*list))) == NULL) // 为链表结构开辟内存空间 return NULL; list-&gt;head = list-&gt;tail = NULL; // 初始化头尾节点为NULL list-&gt;len = 0; // 初始化链表长度为0 list-&gt;dup = NULL; // 初始化链表复制函数为NULL list-&gt;free = NULL; // 初始化节点数据域释放函数为NULL list-&gt;match = NULL; // 初始化节点比较函数为NULL return list;&#125; listRelease函数释放整个链表，此函数不能失败。 123456789101112131415void listRelease(list *list)&#123; unsigned long len; listNode *current, *next; current = list-&gt;head; // 当前节点从头节点开始 len = list-&gt;len; while(len--) &#123; // 从头至尾遍历整个链表 next = current-&gt;next; // 先保存当前节点的下一个节点指针 if (list-&gt;free) list-&gt;free(current-&gt;value); // 使用节点释放函数释放当前节点的数据域 zfree(current); // 释放当前节点 current = next; // 更新当前节点指针 &#125; zfree(list); // 释放整个链表&#125; listAddNodeHead函数在链表头添加一个数据域包含指向’value’指针的新节点。出错时，会返回NULL且不会执行任何操作(链表不会有任何改变)。成功时，会返回你传入的’list’指针。 12345678910111213141516171819list *listAddNodeHead(list *list, void *value)&#123; listNode *node; if ((node = zmalloc(sizeof(*node))) == NULL) // 初始化一个新节点 return NULL; node-&gt;value = value; // 设置新节点的数据域为指定值 if (list-&gt;len == 0) &#123; // 如果当前链表长度为0，头尾节点同时指向新节点 list-&gt;head = list-&gt;tail = node; node-&gt;prev = node-&gt;next = NULL; &#125; else &#123; // 如果当前链表长度大于0，设置新节点为头节点 node-&gt;prev = NULL; node-&gt;next = list-&gt;head; list-&gt;head-&gt;prev = node; list-&gt;head = node; &#125; list-&gt;len++; // 更新链表长度 return list;&#125; listAddNodeTail函数在链表尾添加一个数据域包含指向’value’指针的新节点。出错时，会返回NULL且不会执行任何操作(链表不会有任何改变)。成功时，会返回你传入的’list’指针。 12345678910111213141516171819list *listAddNodeTail(list *list, void *value)&#123; listNode *node; if ((node = zmalloc(sizeof(*node))) == NULL) // 初始化一个新节点 return NULL; node-&gt;value = value; // 设置新节点的数据域为指定值 if (list-&gt;len == 0) &#123; // 如果当前链表长度为0，头尾节点同时指向新节点 list-&gt;head = list-&gt;tail = node; node-&gt;prev = node-&gt;next = NULL; &#125; else &#123; // 如果当前链表长度大于0，设置新节点为尾节点 node-&gt;prev = list-&gt;tail; node-&gt;next = NULL; list-&gt;tail-&gt;next = node; list-&gt;tail = node; &#125; list-&gt;len++; // 更新链表长度 return list;&#125; listInsertNode函数插入新节点到链表中某个节点的指定位置(前/后)。 12345678910111213141516171819202122232425262728list *listInsertNode(list *list, listNode *old_node, void *value, int after) &#123; listNode *node; if ((node = zmalloc(sizeof(*node))) == NULL) // 初始化一个新节点 return NULL; node-&gt;value = value; // 设置新节点的数据域为指定值 if (after) &#123; // 插入到老节点的后面 node-&gt;prev = old_node; // 设置新节点的上一个节点为老节点 node-&gt;next = old_node-&gt;next; // 设置新节点的下一个节点为老节点的下一个节点 if (list-&gt;tail == old_node) &#123; // 如果链表尾节点为老节点，更新尾节点为新节点 list-&gt;tail = node; &#125; &#125; else &#123; // 插入到老节点的前面 node-&gt;next = old_node; // 设置新节点的下一个节点为老节点 node-&gt;prev = old_node-&gt;prev; // 设置新节点的上一个节点为老节点的上一个节点 if (list-&gt;head == old_node) &#123; // 如果链表头节点为老节点，更新头节点为新节点 list-&gt;head = node; &#125; &#125; if (node-&gt;prev != NULL) &#123; // 更新新节点和它上一个节点的关系 node-&gt;prev-&gt;next = node; &#125; if (node-&gt;next != NULL) &#123; // 更新新节点和它下一个节点的关系 node-&gt;next-&gt;prev = node; &#125; list-&gt;len++; // 更新链表长度 return list;&#125; listDelNode函数从指定链表中移除指定节点。此函数不能失败。 1234567891011121314void listDelNode(list *list, listNode *node)&#123; if (node-&gt;prev) // 更新指定节点和它上一个节点的关系 node-&gt;prev-&gt;next = node-&gt;next; else list-&gt;head = node-&gt;next; // 指定节点是头结点时，设置指定节点的下一个节点为头结点 if (node-&gt;next) // 更新指定节点和它下一个节点的关系 node-&gt;next-&gt;prev = node-&gt;prev; else list-&gt;tail = node-&gt;prev; // 指定节点是尾结点时，设置指定节点的上一个节点为尾结点 if (list-&gt;free) list-&gt;free(node-&gt;value); // 释放指定节点的数据域 zfree(node); // 释放指定节点 list-&gt;len--; // 更新链表长度&#125; listGetIterator函数返回一个链表的迭代器’iter’。初始化之后每次调用listNext()函数都会返回链表的下一个元素。此函数不能失败。 123456789101112listIter *listGetIterator(list *list, int direction)&#123; listIter *iter; if ((iter = zmalloc(sizeof(*iter))) == NULL) return NULL; // 初始化链表迭代器 if (direction == AL_START_HEAD) iter-&gt;next = list-&gt;head; else iter-&gt;next = list-&gt;tail; iter-&gt;direction = direction; // 设置迭代方向 return iter;&#125; listReleaseIterator函数释放迭代器内存。 123void listReleaseIterator(listIter *iter) &#123; zfree(iter);&#125; listRewind函数使迭代器的当前位置回到链表头，正向迭代。 1234void listRewind(list *list, listIter *li) &#123; li-&gt;next = list-&gt;head; li-&gt;direction = AL_START_HEAD;&#125; listRewindTail函数使迭代器的当前位置回到链表尾，反向迭代。1234void listRewindTail(list *list, listIter *li) &#123; li-&gt;next = list-&gt;tail; li-&gt;direction = AL_START_TAIL;&#125; listNext函数返回迭代器的下一个元素。如果没有下一个元素，此函数返回NULL，否则返回指定列表下一个元素的指针。 123456789101112listNode *listNext(listIter *iter)&#123; listNode *current = iter-&gt;next; // 获取下一个节点指针 if (current != NULL) &#123; if (iter-&gt;direction == AL_START_HEAD) // 正向迭代时更新迭代器下一个节点指针为当前节点的后一个节点 iter-&gt;next = current-&gt;next; else // 反向迭代时更新迭代器下一个节点指针为当前节点的前一个节点 iter-&gt;next = current-&gt;prev; &#125; return current;&#125; listDup函数复制整个链表。内存不足时返回NULL。成功则返回原始链表的拷贝。节点数据域的’Dup’方法由listSetDupMethod()函数设置，用来拷贝节点数据域。如果没有设置改函数，拷贝节点的数据域会使用原始节点数据域的指针，这相当于浅拷贝。原始链表不管在改函数成功还是失败的情况下都不会被修改。 123456789101112131415161718192021222324252627282930list *listDup(list *orig)&#123; list *copy; listIter iter; listNode *node; if ((copy = listCreate()) == NULL) // 初始化拷贝链表 return NULL; copy-&gt;dup = orig-&gt;dup; // 拷贝链表和原始链表的节点复制函数相同 copy-&gt;free = orig-&gt;free; // 拷贝链表和原始链表的节点数据域释放函数相同 copy-&gt;match = orig-&gt;match; // 拷贝链表和原始链表的节点比较函数相同 listRewind(orig, &amp;iter); // 使迭代器的当前位置回到链表头，正向迭代 while((node = listNext(&amp;iter)) != NULL) &#123; // 遍历原始链表 void *value; if (copy-&gt;dup) &#123; // 设置了节点数据域复制函数 value = copy-&gt;dup(node-&gt;value); // 复制节点数据域 if (value == NULL) &#123; // 数据域为NULL直接释放拷贝链表并返回NULL listRelease(copy); return NULL; &#125; &#125; else // 未设置节点数据域复制函数 value = node-&gt;value; // 直接取原始链表节点的数据域指针赋值 if (listAddNodeTail(copy, value) == NULL) &#123; // 把复制得到的value添加到拷贝链表的尾部 listRelease(copy); return NULL; &#125; &#125; return copy;&#125; listSearchKey函数在链表中查找包含指定key的节点。使用由listSetMatchMethod()函数设置的’match’方法来判断是否匹配。如果没有设置’match’方法，就使用每个节点的’value’指针直接和’key’指针进行比较。匹配成功时，返回第一个匹配的节点指针(搜索从链表头开始)。没有找到匹配的节点就返回NULL。 12345678910111213141516171819listNode *listSearchKey(list *list, void *key)&#123; listIter iter; listNode *node; listRewind(list, &amp;iter); while((node = listNext(&amp;iter)) != NULL) &#123; // 遍历整个链表 if (list-&gt;match) &#123; // 如果设置了节点数据域比较函数，就调用它进行比较 if (list-&gt;match(node-&gt;value, key)) &#123; return node; &#125; &#125; else &#123; // 否则直接比较key和node-&gt;value if (key == node-&gt;value) &#123; return node; &#125; &#125; &#125; return NULL;&#125; listIndex函数把链表当成一个数组，返回指定索引的节点。负索引值用来从尾巴开始计算，-1表示最后一个元素，-2表示倒数第二个元素，以此类推。当索引值超出返回返回NULL。 12345678910111213listNode *listIndex(list *list, long index) &#123; listNode *n; if (index &lt; 0) &#123; // index小于0，从尾部开始遍历 index = (-index)-1; n = list-&gt;tail; while(index-- &amp;&amp; n) n = n-&gt;prev; &#125; else &#123; // index大于0，从头部开始遍历 n = list-&gt;head; while(index-- &amp;&amp; n) n = n-&gt;next; &#125; return n;&#125; listRotate函数移除链表当前的尾节点，并把它设置为头节点。 1234567891011121314void listRotate(list *list) &#123; listNode *tail = list-&gt;tail; if (listLength(list) &lt;= 1) return; /* Detach current tail */ list-&gt;tail = tail-&gt;prev; // 设置尾节点为当前尾节点的前一个节点 list-&gt;tail-&gt;next = NULL; // 设置新尾节点后向关系 /* Move it as head */ list-&gt;head-&gt;prev = tail; // 设置当前头节点的前一个节点为原来的尾节点 tail-&gt;prev = NULL; // 设置新头节点前向关系 tail-&gt;next = list-&gt;head; // 设置新头节点后向关系 list-&gt;head = tail; // 更新新头节指针&#125;","categories":[{"name":"源码分析","slug":"源码分析","permalink":"https://nullcc.github.io/categories/源码分析/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://nullcc.github.io/tags/Redis/"},{"name":"数据结构","slug":"数据结构","permalink":"https://nullcc.github.io/tags/数据结构/"}]},{"title":"Redis中的底层数据结构(2)——简单动态字符串(sds)","slug":"Redis中的底层数据结构(2)——简单动态字符串(sds)","date":"2017-11-09T16:00:00.000Z","updated":"2022-04-15T03:41:13.018Z","comments":true,"path":"2017/11/10/Redis中的底层数据结构(2)——简单动态字符串(sds)/","link":"","permalink":"https://nullcc.github.io/2017/11/10/Redis中的底层数据结构(2)——简单动态字符串(sds)/","excerpt":"本文将详细说明Redis中简单动态字符串(sds)的实现。 在Redis源码（这里使用3.2.11版本）中，sds的实现在sds.h和sds.c中。","text":"本文将详细说明Redis中简单动态字符串(sds)的实现。 在Redis源码（这里使用3.2.11版本）中，sds的实现在sds.h和sds.c中。 sds字符串和C原生字符串的对比Redis并没有直接使用C语言的原生字符串，而是有一个专用的字符串实现：sds。sds相比于C语言原生字符串有很多优势： sds获取字符串长度的效率高。要想获取C语言原生字符串的长度，需要遍历整个字符串对字符个数计数，直到遇到一个\\0为止，时间复杂度为O(n)。sds在头部保存了len用来表示字符串的实际长度，获取sds字符串长度的时间复杂度为O(1)。 sds可以避免缓冲区溢出。一个简单的例子，字符串拼接，对C语言原生字符串str1和str2来说，把str2拼接到str1后，如果没有为str1申请好足够的内存，直接拼接可能造成str1后的内存区域被覆盖从而导致不可预知的后果。sds字符串在拼接时，会自动检查空间是否足够，如果不够会自动按照一定的规则进行分配，因此无需担心溢出问题。 sds的内存分配策略可以有效降低修改字符串时内存重分配的开销。在sdsMakeRoomFor函数中有这么一段代码（只截取一小部分）： 1234567// ...other code...// 扩充后的长度小于sds最大预分配长度时，把newlen加倍以防止短期内再扩充if (newlen &lt; SDS_MAX_PREALLOC) newlen *= 2;else // 否则直接加上sds最大预分配长度 newlen += SDS_MAX_PREALLOC;// ...other code... 上面这段代码表示，在对一个sds字符串进行扩充时，Redis会认为这个字符串还有进一步被扩充的可能性，因此会根据一定规则来预先分配一部分空间来避免短期内再次申请内存分配。另外sds字符串在缩短内容时，也不会立即释放多出来的空间，sds字符串在alloc属性中标识了占用的总空间大小，在需要的时候，Redis会进行释放。 4.sds是二进制安全的。C原生字符串的结尾是\\0，也就是说它在字符串内容中不能包含\\0，如果包含了会导致字符串被截断，因此C原生字符串只能用来保存文本数据，无法保存图片等包含\\0的数据。sds字符串使用len属性来标识字符串长度而不是\\0，所以其内容可以是任意的二进制数据。 sds头文件详解sds的定义表明sds实际上是一个char *： 1typedef char *sds; 但这还不足以说明sds是什么，源码中还定义了五种sds header的类型： 12345678910111213141516171819202122232425262728293031323334353637383940/* 注意: sdshdr5这种类型从未被使用, 我们仅仅直接访问它的flags。 * 这里记录的是type为5的sds的布局。 * __attribute__ ((__packed__))表示结构体字节对齐，这是GNU C特有的语法 */struct __attribute__ ((__packed__)) sdshdr5 &#123; unsigned char flags; char buf[];&#125;;struct __attribute__ ((__packed__)) sdshdr8 &#123; uint8_t len; // 已使用的字符串长度 uint8_t alloc; // 分配的内存空间大小，不包括头部和空终止符 unsigned char flags; // 3个最低有效位表示类型，5个最高有效位未使用 char buf[]; // 字符数组&#125;;struct __attribute__ ((__packed__)) sdshdr16 &#123; uint16_t len; uint16_t alloc; unsigned char flags; char buf[];&#125;;struct __attribute__ ((__packed__)) sdshdr32 &#123; uint32_t len; uint32_t alloc; unsigned char flags; char buf[];&#125;;struct __attribute__ ((__packed__)) sdshdr64 &#123; uint64_t len; uint64_t alloc; unsigned char flags; char buf[];&#125;;/* SDS类型值，一共5种类型 */#define SDS_TYPE_5 0#define SDS_TYPE_8 1#define SDS_TYPE_16 2#define SDS_TYPE_32 3#define SDS_TYPE_64 4#define SDS_TYPE_MASK 7 // sds类型掩码 0b00000111，因为flags中只有3个最低有效位表示类型#define SDS_TYPE_BITS 3 // 表示sds类型的比特位数，前面有提到：3个最低有效位表示类型 除了sdshdr5不被使用以外，可以观察到其他四种类型的头部中len和alloc域的类型都不同，不同类型的头部支持的字符串长度不同，这是为了空间效率才这么做的，后面会有详细分析。 根据sds header的定义，来看看一个头部类型为sdshdr8的sds字符串的内存布局： sdshdr8中的len表示sds字符串的实际长度，也就是buf字符数组的长度，alloc表示分配给字符串的空间大小，注意这个大小不包含头部和结尾的终止符。也就是说alloc是大于或等于len的，当alloc等于len时，内存布局就如上图所示，如果当alloc大于len，在字符串和和结尾终止符(\\0)之间，会用\\0填充，下面是一个len等于12，alloc等于15的sds字符串的内存布局示意图： 先看几个在sds实现中很常用的宏： 12345678/* 从sds获取其header起始位置的指针，并声明一个sh变量赋值给它，获得方式是sds的地址减去头部大小 */#define SDS_HDR_VAR(T,s) struct sdshdr##T *sh = (void*)((s)-(sizeof(struct sdshdr##T)));/* 从sds获取其header起始位置的指针，作用和上面一个定义差不多，只不过不赋值给sh变量 */#define SDS_HDR(T,s) ((struct sdshdr##T *)((s)-(sizeof(struct sdshdr##T))))// 获取type为5的sds的长度，由于其flags的5个最高有效位表示字符串长度，所以直接把flags右移3位即是其字符串长度#define SDS_TYPE_5_LEN(f) ((f)&gt;&gt;SDS_TYPE_BITS) sdslen函数获取一个sds的长度。 123456789101112131415161718static inline size_t sdslen(const sds s) &#123; /* 通过sds字符指针获得header类型的方法是，先向低地址方向偏移1个字节的位置，得到flags字段， 然后取flags的最低3个bit得到header的类型。 */ unsigned char flags = s[-1]; switch(flags&amp;SDS_TYPE_MASK) &#123; // 操作：0b00000??? &amp; 0b00000111，根据sds类型获取其字符串长度 case SDS_TYPE_5: return SDS_TYPE_5_LEN(flags); case SDS_TYPE_8: return SDS_HDR(8,s)-&gt;len; case SDS_TYPE_16: return SDS_HDR(16,s)-&gt;len; case SDS_TYPE_32: return SDS_HDR(32,s)-&gt;len; case SDS_TYPE_64: return SDS_HDR(64,s)-&gt;len; &#125; return 0;&#125; sdsavail函数获取一个sds的空闲空间，计算方式是：已分配的空间 - 字符串长度大小。 12345678910111213141516171819202122232425static inline size_t sdsavail(const sds s) &#123; unsigned char flags = s[-1]; switch(flags&amp;SDS_TYPE_MASK) &#123; // 同上，获取sds类型 case SDS_TYPE_5: &#123; // SDS_TYPE_5未被使用，直接返回0 return 0; &#125; case SDS_TYPE_8: &#123; SDS_HDR_VAR(8,s); // 从sds获取其header起始位置的指针 return sh-&gt;alloc - sh-&gt;len; &#125; case SDS_TYPE_16: &#123; SDS_HDR_VAR(16,s); return sh-&gt;alloc - sh-&gt;len; &#125; case SDS_TYPE_32: &#123; SDS_HDR_VAR(32,s); return sh-&gt;alloc - sh-&gt;len; &#125; case SDS_TYPE_64: &#123; SDS_HDR_VAR(64,s); return sh-&gt;alloc - sh-&gt;len; &#125; &#125; return 0;&#125; sdssetlen函数设置sds的字符串长度 1234567891011121314151617181920212223static inline void sdssetlen(sds s, size_t newlen) &#123; unsigned char flags = s[-1]; switch(flags&amp;SDS_TYPE_MASK) &#123; // 同上，获取sds类型 case SDS_TYPE_5: // SDS_TYPE_5的sds &#123; unsigned char *fp = ((unsigned char*)s)-1; // fp是sdshdr5的flags的指针 *fp = SDS_TYPE_5 | (newlen &lt;&lt; SDS_TYPE_BITS); // 把newlen右移SDS_TYPE_BITS位再和SDS_TYPE_5合成即可 &#125; break; case SDS_TYPE_8: SDS_HDR(8,s)-&gt;len = newlen; // 直接改写header中的len break; case SDS_TYPE_16: SDS_HDR(16,s)-&gt;len = newlen; break; case SDS_TYPE_32: SDS_HDR(32,s)-&gt;len = newlen; break; case SDS_TYPE_64: SDS_HDR(64,s)-&gt;len = newlen; break; &#125;&#125; sdsinclen函数增加sds的长度。 123456789101112131415161718192021222324static inline void sdsinclen(sds s, size_t inc) &#123; unsigned char flags = s[-1]; switch(flags&amp;SDS_TYPE_MASK) &#123; // 同上，获取sds类型 case SDS_TYPE_5: &#123; unsigned char *fp = ((unsigned char*)s)-1; // fp是sdshdr5的flags的指针 unsigned char newlen = SDS_TYPE_5_LEN(flags)+inc; // 计算出newlen *fp = SDS_TYPE_5 | (newlen &lt;&lt; SDS_TYPE_BITS); // 同sdssetlen &#125; break; case SDS_TYPE_8: SDS_HDR(8,s)-&gt;len += inc; // 直接增加header中的len break; case SDS_TYPE_16: SDS_HDR(16,s)-&gt;len += inc; break; case SDS_TYPE_32: SDS_HDR(32,s)-&gt;len += inc; break; case SDS_TYPE_64: SDS_HDR(64,s)-&gt;len += inc; break; &#125;&#125; sdsalloc函数获取sds容量，sdsalloc() = sdsavail() + sdslen()。 12345678910111213141516static inline size_t sdsalloc(const sds s) &#123; unsigned char flags = s[-1]; switch(flags&amp;SDS_TYPE_MASK) &#123; // 同上，获取sds类型 case SDS_TYPE_5: return SDS_TYPE_5_LEN(flags); case SDS_TYPE_8: // 其他type直接返回header的alloc属性 return SDS_HDR(8,s)-&gt;alloc; case SDS_TYPE_16: return SDS_HDR(16,s)-&gt;alloc; case SDS_TYPE_32: return SDS_HDR(32,s)-&gt;alloc; case SDS_TYPE_64: return SDS_HDR(64,s)-&gt;alloc; &#125; return 0;&#125; sdssetalloc函数设置sds容量。 1234567891011121314151617181920static inline void sdssetalloc(sds s, size_t newlen) &#123; unsigned char flags = s[-1]; switch(flags&amp;SDS_TYPE_MASK) &#123; // 同上，获取sds类型 case SDS_TYPE_5: /* Nothing to do, this type has no total allocation info. */ break; case SDS_TYPE_8: // 其他type直接设置header的alloc属性 SDS_HDR(8,s)-&gt;alloc = newlen; break; case SDS_TYPE_16: SDS_HDR(16,s)-&gt;alloc = newlen; break; case SDS_TYPE_32: SDS_HDR(32,s)-&gt;alloc = newlen; break; case SDS_TYPE_64: SDS_HDR(64,s)-&gt;alloc = newlen; break; &#125;&#125; 下面是sds.h中声明的函数原型： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849sds sdsnewlen(const void *init, size_t initlen); // 创建一个长度为initlen的sds，使用init指向的字符数组来初始化数据sds sdsnew(const char *init); // 内部调用sdsnewlen，创建一个sdssds sdsempty(void); // 返回一个空的sdssds sdsdup(const sds s); // 拷贝一个sds并返回这个拷贝void sdsfree(sds s); // 释放一个sdssds sdsgrowzero(sds s, size_t len); // 使一个sds的长度增长到一个指定的值，末尾未使用的空间用0填充sds sdscatlen(sds s, const void *t, size_t len); // 连接一个sds和一个二进制安全的数据t，t的长度为lensds sdscat(sds s, const char *t); // 连接一个sds和一个二进制安全的数据t，内部调用sdscatlensds sdscatsds(sds s, const sds t); // 连接两个sdssds sdscpylen(sds s, const char *t, size_t len); // 把二进制安全的数据t复制到一个sds的内存中，覆盖原来的字符串，t的长度为lensds sdscpy(sds s, const char *t); // 把二进制安全的数据t复制到一个sds的内存中，覆盖原来的字符串，内部调用sdscpylen/* 通过fmt指定个格式来格式化字符串 */sds sdscatvprintf(sds s, const char *fmt, va_list ap);#ifdef __GNUC__sds sdscatprintf(sds s, const char *fmt, ...) __attribute__((format(printf, 2, 3)));#elsesds sdscatprintf(sds s, const char *fmt, ...);#endifsds sdscatfmt(sds s, char const *fmt, ...); // 将格式化后的任意数量个字符串追加到s的末尾sds sdstrim(sds s, const char *cset); // 删除sds两端由cset指定的字符void sdsrange(sds s, int start, int end); // 通过区间[start, end]截取字符串void sdsupdatelen(sds s); // 根据字符串占用的空间来更新lenvoid sdsclear(sds s); // 把字符串的第一个字符设置为'\\0'，把字符串设置为空字符串，但是并不释放内存int sdscmp(const sds s1, const sds s2); // 比较两个sds的相等性sds *sdssplitlen(const char *s, int len, const char *sep, int seplen, int *count); // 使用分隔符sep对s进行分割，返回一个sds数组void sdsfreesplitres(sds *tokens, int count); // 释放sds数组tokens中的count个sdsvoid sdstolower(sds s); // 将sds所有字符转换为小写void sdstoupper(sds s); // 将sds所有字符转换为大写sds sdsfromlonglong(long long value); // 将长整型转换为字符串sds sdscatrepr(sds s, const char *p, size_t len); // 将长度为len的字符串p以带引号的格式追加到s的末尾sds *sdssplitargs(const char *line, int *argc); // 将一行文本分割成多个参数，参数的个数存在argcsds sdsmapchars(sds s, const char *from, const char *to, size_t setlen); // 将字符串s中，出现存在from中指定的字符，都转换成to中的字符，from与to有位置关系sds sdsjoin(char **argv, int argc, char *sep); // 使用分隔符sep将字符数组argv拼接成一个字符串sds sdsjoinsds(sds *argv, int argc, const char *sep, size_t seplen); // 和sdsjoin类似，不过拼接的是一个sds数组/* 暴露出来作为用户API的低级函数 */sds sdsMakeRoomFor(sds s, size_t addlen); // 为指定的sds扩充大小，扩充的大小为addlenvoid sdsIncrLen(sds s, int incr); // 根据incr增加或减少sds的字符串长度sds sdsRemoveFreeSpace(sds s); // 移除一个sds的空闲空间size_t sdsAllocSize(sds s); // 获取一个sds的总大小（包括header、字符串、末尾的空闲空间和隐式项目）void *sdsAllocPtr(sds s); // 获取一个sds确切的内存空间的指针（一般的sds引用都是一个指向其字符串的指针）/* 导出供外部程序调用的sds的分配/释放函数 */void *sds_malloc(size_t size); // sds分配器的包装函数，内部调用s_mallocvoid *sds_realloc(void *ptr, size_t size); // sds分配器的包装函数，内部调用s_reallocvoid sds_free(void *ptr); // sds释放器的包装函数，内部调用s_free sds实现详解下面列举了一部分是sds.c中的函数定义，由于sds.c代码量较多（超过1500行），其中有一些函数是帮助函数，或测试代码，这里只列举比较重要的函数详细解释。 sdsHdrSize函数获取sds header的大小。 123456789101112131415static inline int sdsHdrSize(char type) &#123; switch(type&amp;SDS_TYPE_MASK) &#123; // 获取sds类型 case SDS_TYPE_5: return sizeof(struct sdshdr5); case SDS_TYPE_8: return sizeof(struct sdshdr8); case SDS_TYPE_16: return sizeof(struct sdshdr16); case SDS_TYPE_32: return sizeof(struct sdshdr32); case SDS_TYPE_64: return sizeof(struct sdshdr64); &#125; return 0;&#125; sdsReqType函数根据字符串大小判断sds类型。 1234567891011static inline char sdsReqType(size_t string_size) &#123; if (string_size &lt; 1&lt;&lt;5) return SDS_TYPE_5; if (string_size &lt; 1&lt;&lt;8) return SDS_TYPE_8; if (string_size &lt; 1&lt;&lt;16) return SDS_TYPE_16; if (string_size &lt; 1ll&lt;&lt;32) return SDS_TYPE_32; return SDS_TYPE_64;&#125; sdsnewlen函数使用init指针指向的数据和initlen的长度创建一个新的sds字符串。如果init指针是NULL，字符串会被初始化为长度为initlen，内容全为0字节。 sds字符串总是以’\\0’字符结尾的，所以即使你创建了如下的sds字符串： mystring = sdsnewlen(“abc”,3);由于这个字符串在结尾隐式包含了一个’\\0’，所以你可以使用printf()函数打印它。然而，sds字符串是二进制安全的，并且可以在中间包含’\\0’字符，因为在sds字符串header中保存了字符串长度。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556sds sdsnewlen(const void *init, size_t initlen) &#123; void *sh; sds s; char type = sdsReqType(initlen); // 使用初始长度判断该创建哪种类型的sds字符串 /* Empty strings are usually created in order to append. Use type 8 * since type 5 is not good at this. */ /* 空字符串一般在创建后都会追加数据进去（完全可能大于32个字节），使用type 8的字符串类型要优于type 5 */ if (type == SDS_TYPE_5 &amp;&amp; initlen == 0) type = SDS_TYPE_8; int hdrlen = sdsHdrSize(type); // 获取header长度 unsigned char *fp; /* flags pointer. */ sh = s_malloc(hdrlen+initlen+1); // 为sds字符串header申请内存空间，大小为：头部大小+初始化长度大小+1（其中1是为'\\0'留的） if (!init) // 初始数据指针为NULL memset(sh, 0, hdrlen+initlen+1); // 把整个sds的内容都设置为0 if (sh == NULL) return NULL; // 申请内存失败返回NULL s = (char*)sh+hdrlen; // 字符串指针 fp = ((unsigned char*)s)-1; // flags指针 switch(type) &#123; // 根据sds类型设置header中的数据 case SDS_TYPE_5: &#123; *fp = type | (initlen &lt;&lt; SDS_TYPE_BITS); break; &#125; case SDS_TYPE_8: &#123; SDS_HDR_VAR(8,s); sh-&gt;len = initlen; sh-&gt;alloc = initlen; *fp = type; break; &#125; case SDS_TYPE_16: &#123; SDS_HDR_VAR(16,s); sh-&gt;len = initlen; sh-&gt;alloc = initlen; *fp = type; break; &#125; case SDS_TYPE_32: &#123; SDS_HDR_VAR(32,s); sh-&gt;len = initlen; sh-&gt;alloc = initlen; *fp = type; break; &#125; case SDS_TYPE_64: &#123; SDS_HDR_VAR(64,s); sh-&gt;len = initlen; sh-&gt;alloc = initlen; *fp = type; break; &#125; &#125; if (initlen &amp;&amp; init) memcpy(s, init, initlen); // 将初始化数据指针init指向的数据拷贝到字符串中 s[initlen] = '\\0'; // 设置最后一个字节为'\\0' return s;&#125; sdsempty函数创建一个空sds(字符串长度为0)字符串。即使在这种情况下，字符串也总是有一个隐式的’\\0’结束符。 123sds sdsempty(void) &#123; return sdsnewlen(\"\",0);&#125; sdsnew函数使用一个以’\\0’为结束符的C字符串创建一个新的sds字符串。 1234sds sdsnew(const char *init) &#123; size_t initlen = (init == NULL) ? 0 : strlen(init); // 初始化数据指针为NULL时，字符串长度为0 return sdsnewlen(init, initlen);&#125; sdsdup函数复制一个sds字符串 123sds sdsdup(const sds s) &#123; return sdsnewlen(s, sdslen(s));&#125; sdsfree函数释放一个sds字符串，如果该字符串是NULL则什么都不做。 1234void sdsfree(sds s) &#123; if (s == NULL) return; s_free((char*)s-sdsHdrSize(s[-1]));&#125; sdsupdatelen函数使用通过strlen()获取的sds字符串长度来设置sds字符串的长度，所以只考虑到第一个空字符前的字符串长度。当sds字符串被手动修改的时候这个函数很有用，比如下面的例子： s = sdsnew(“foobar”); s[2] = ‘\\0’; sdsupdatelen(s); printf(“%d\\n”, sdslen(s));上面的代码输出是”2”，但是如果我们注释掉调用sdsupdatelen()的那行代码，输出则是’6’，因为字符串被强行修改了，但字符串的逻辑长度还是6个字节。 1234void sdsupdatelen(sds s) &#123; int reallen = strlen(s); // 获取字符串的真实长度（会取第一个终止符'\\0'之前的字符串长度） sdssetlen(s, reallen); // 重新设置sds的字符串长度&#125; sdsclear函数就地修改一个sds字符串为空（长度为0）。然而，所有当前的缓冲区都不会被释放，而是设置成空闲空间，所以下一次追加操作可以使用原来的空闲空间而不需要分配空间。 1234void sdsclear(sds s) &#123; sdssetlen(s, 0); // 设置sds字符串的长度为0 s[0] = '\\0'; // 设置字符串首地址为终止符'\\0'&#125; sdsMakeRoomFor函数扩充sds字符串的空闲空间，调用此函数后，可以保证在原sds字符串后面扩充了addlen个字节的空间，外加1个字节的终止符。注意：这个函数不会改变调用sdslen()返回的字符串长度，仅仅改变了空闲空间的大小。 123456789101112131415161718192021222324252627282930313233343536373839404142sds sdsMakeRoomFor(sds s, size_t addlen) &#123; void *sh, *newsh; size_t avail = sdsavail(s); // 获取sds字符串的空闲空间大小 size_t len, newlen; char type, oldtype = s[-1] &amp; SDS_TYPE_MASK; // 获取sds字符串类型 int hdrlen; /* 如果当前空闲空间大于addlen，就不做扩充操作，直接返回 */ if (avail &gt;= addlen) return s; len = sdslen(s); // sds字符串当前长度 sh = (char*)s-sdsHdrSize(oldtype); // sds字符串header指针 newlen = (len+addlen); // 扩充后的新长度 if (newlen &lt; SDS_MAX_PREALLOC) // 扩充后的长度小于sds最大预分配长度时，把newlen加倍以防止短期内再扩充 newlen *= 2; else // 否则直接加上sds最大预分配长度 newlen += SDS_MAX_PREALLOC; type = sdsReqType(newlen); // 获取新长度下的sds字符串类型 /* 不要使用type 5：由于用户向字符串追加数据时，type 5的字符串无法保存空闲空间，所以 * 每次追加数据时都要调用sdsMakeRoomFor() */ if (type == SDS_TYPE_5) type = SDS_TYPE_8; // 比较短的字符串一律用type 8 hdrlen = sdsHdrSize(type); // 计算sds字符串header长度 if (oldtype==type) &#123; // 字符串类型不变的情况下 newsh = s_realloc(sh, hdrlen+newlen+1); // 在原header指针上重新分配新的大小 if (newsh == NULL) return NULL; s = (char*)newsh+hdrlen; // 更新字符串指针 &#125; else &#123; /* 一旦header大小变化，需要把字符串前移，并且不能使用realloc */ newsh = s_malloc(hdrlen+newlen+1); // 新开辟一块内存 if (newsh == NULL) return NULL; memcpy((char*)newsh+hdrlen, s, len+1); // 把原始sds字符串的内容复制到新的内存区域 s_free(sh); // 释放原始sds字符串的头指针指向的内存 s = (char*)newsh+hdrlen; // 更新sds字符串指针 s[-1] = type; // 更新flags字节信息 sdssetlen(s, len); // 更新sds字符串header中的len &#125; sdssetalloc(s, newlen); // 更新sds字符串header中的alloc return s;&#125; sdsRemoveFreeSpace函数重新分配sds字符串的空间，保证结尾没有空闲空间。其中包含的字符串不变，但下一次进行字符串连接操作时需要一次空间重新分配。调用此函数后，原来作为参数传入的sds字符串的指针不再是有效的，所有引用必须被替换为函数返回的新指针。 12345678910111213141516171819202122232425sds sdsRemoveFreeSpace(sds s) &#123; void *sh, *newsh; char type, oldtype = s[-1] &amp; SDS_TYPE_MASK; int hdrlen; size_t len = sdslen(s); // 字符串真正的长度 sh = (char*)s-sdsHdrSize(oldtype); // 获取sds字符串header指针 type = sdsReqType(len); // 计算字符串的新type hdrlen = sdsHdrSize(type); // 计算字符串的新header大小 if (oldtype==type) &#123; // 字符串类型不变 newsh = s_realloc(sh, hdrlen+len+1); // realloc，大小更新为：header大小+真实字符串大小+1 if (newsh == NULL) return NULL; s = (char*)newsh+hdrlen; // 更新sds字符串指针 &#125; else &#123; // 字符串类型改变 newsh = s_malloc(hdrlen+len+1); // 新开辟一块内存 if (newsh == NULL) return NULL; memcpy((char*)newsh+hdrlen, s, len+1); // 复制数据到新内存中 s_free(sh); // 释放原始的sds字符串内存 s = (char*)newsh+hdrlen; // 更新sds字符串指针 s[-1] = type; // 更新flags sdssetlen(s, len); // 更新sds字符串header中的len &#125; sdssetalloc(s, len); // 更新sds字符串header中的alloc return s;&#125; sdsAllocSize函数返回指定sds字符串的分配空间大小，包括: 1) sds header大小。 2) 字符串本身的大小。 3) 末尾的空闲空间大小（如果有的话）。 4) 隐式包含的终止符。 1234size_t sdsAllocSize(sds s) &#123; size_t alloc = sdsalloc(s); // 获取sds header的alloc return sdsHdrSize(s[-1])+alloc+1; // header大小+alloc（字符串大小+空闲空间大小）+1&#125; sdsAllocPtr函数返回sds分配空间的首地址（一般来说sds字符串的指针是其字符串缓冲区的首地址） 123void *sdsAllocPtr(sds s) &#123; return (void*) (s-sdsHdrSize(s[-1])); // 字符串缓冲区的首地址减去header大小即可&#125; sdsIncrLen函数取决于’incr’参数，此函数增加sds字符串的长度或减少剩余空闲空间的大小。同时也将在新字符串的末尾设置终止符。此函数用来修正调用sdsMakeRoomFor()函数之后字符串的长度，在当前字符串后追加数据这些需要设置字符串新长度的操作之后。注意：可以使用一个负的增量值来右对齐字符串。使用sdsIncrLen()和sdsMakeRoomFor()函数可以用来满足如下模式，从内核中直接复制一部分字节到一个sds字符串的末尾，且无须把数据先复制到一个中间缓冲区中： oldlen = sdslen(s); s = sdsMakeRoomFor(s, BUFFER_SIZE); nread = read(fd, s+oldlen, BUFFER_SIZE); … check for nread &lt;= 0 and handle it … sdsIncrLen(s, nread); 12345678910111213141516171819202122232425262728293031323334353637383940void sdsIncrLen(sds s, int incr) &#123; unsigned char flags = s[-1]; size_t len; switch(flags&amp;SDS_TYPE_MASK) &#123; // 判断sds字符串类型 case SDS_TYPE_5: &#123; unsigned char *fp = ((unsigned char*)s)-1; // flags指针 unsigned char oldlen = SDS_TYPE_5_LEN(flags); // 原始字符串大小 assert((incr &gt; 0 &amp;&amp; oldlen+incr &lt; 32) || (incr &lt; 0 &amp;&amp; oldlen &gt;= (unsigned int)(-incr))); *fp = SDS_TYPE_5 | ((oldlen+incr) &lt;&lt; SDS_TYPE_BITS); // 更新flags中字符串大小的比特位 len = oldlen+incr; // 更新header的len break; &#125; case SDS_TYPE_8: &#123; SDS_HDR_VAR(8,s); // 获取sds字符串的header指针 assert((incr &gt;= 0 &amp;&amp; sh-&gt;alloc-sh-&gt;len &gt;= incr) || (incr &lt; 0 &amp;&amp; sh-&gt;len &gt;= (unsigned int)(-incr))); len = (sh-&gt;len += incr); // 更新header的len break; &#125; case SDS_TYPE_16: &#123; SDS_HDR_VAR(16,s); assert((incr &gt;= 0 &amp;&amp; sh-&gt;alloc-sh-&gt;len &gt;= incr) || (incr &lt; 0 &amp;&amp; sh-&gt;len &gt;= (unsigned int)(-incr))); len = (sh-&gt;len += incr); break; &#125; case SDS_TYPE_32: &#123; SDS_HDR_VAR(32,s); assert((incr &gt;= 0 &amp;&amp; sh-&gt;alloc-sh-&gt;len &gt;= (unsigned int)incr) || (incr &lt; 0 &amp;&amp; sh-&gt;len &gt;= (unsigned int)(-incr))); len = (sh-&gt;len += incr); break; &#125; case SDS_TYPE_64: &#123; SDS_HDR_VAR(64,s); assert((incr &gt;= 0 &amp;&amp; sh-&gt;alloc-sh-&gt;len &gt;= (uint64_t)incr) || (incr &lt; 0 &amp;&amp; sh-&gt;len &gt;= (uint64_t)(-incr))); len = (sh-&gt;len += incr); break; &#125; default: len = 0; /* Just to avoid compilation warnings. */ &#125; s[len] = '\\0'; // 设置终止符&#125; sdsgrowzero函数增长一个sds字符串到一个指定长度。扩充出来的不是原来字符串的空间会被设置为0。如果指定的长度比当前长度小，不做任何操作。 12345678910111213sds sdsgrowzero(sds s, size_t len) &#123; size_t curlen = sdslen(s); // 当前字符串长度 if (len &lt;= curlen) return s; // 设置的长度小于当前长度，直接返回原始sds字符串指针 s = sdsMakeRoomFor(s,len-curlen); // 扩充sds if (s == NULL) return NULL; /* Make sure added region doesn't contain garbage */ /* 确保新增的区域不包含垃圾数据 */ memset(s+curlen,0,(len-curlen+1)); /* also set trailing \\0 byte */ sdssetlen(s, len); // 更新sds字符串header中的len return s;&#125; sdscatlen函数向指定的sds字符串’s’尾部追加由’t’指向的二进制安全的字符串，长度’len’字节。调用此函数后，原来作为参数传入的sds字符串的指针不再是有效的，所有引用必须被替换为函数返回的新指针。 12345678910sds sdscatlen(sds s, const void *t, size_t len) &#123; size_t curlen = sdslen(s); // 当前字符串长度 s = sdsMakeRoomFor(s,len); // 扩充len字节 if (s == NULL) return NULL; memcpy(s+curlen, t, len); // 追加数据到原字符串末尾 sdssetlen(s, curlen+len); // 更新sds字符串header中的len s[curlen+len] = '\\0'; // 设置终止符 return s;&#125; sdscat函数追加指定的C字符串到sds字符串’s’的尾部。调用此函数后，原来作为参数传入的sds字符串的指针不再是有效的，所有引用必须被替换为函数返回的新指针。 123sds sdscat(sds s, const char *t) &#123; return sdscatlen(s, t, strlen(t));&#125; sdscatsds函数追加指定的sds字符串’t’到已经存在的sds字符串’s’末尾。调用此函数后，原来作为参数传入的sds字符串的指针不再是有效的，所有引用必须被替换为函数返回的新指针。 123sds sdscatsds(sds s, const sds t) &#123; return sdscatlen(s, t, sdslen(t));&#125; sdscpylen函数把由’t’指向的二进制安全的字符串复制到sds字符串’s’的内存空间中，长度为’len’，覆盖原来的数据。 12345678910sds sdscpylen(sds s, const char *t, size_t len) &#123; if (sdsalloc(s) &lt; len) &#123; s = sdsMakeRoomFor(s,len-sdslen(s)); // 原sds总空间不足就扩充 if (s == NULL) return NULL; &#125; memcpy(s, t, len); // 将t指向的数据直接覆盖s s[len] = '\\0'; // 设置终止符 sdssetlen(s, len); // 更新sds字符串header中的len return s;&#125; sdscpy函数和sdscpylen()函数类似，但是’t’指向的必须是一个以’\\0’结尾的字符串，所以可以用strlen()获取该字符串长度。 123sds sdscpy(sds s, const char *t) &#123; return sdscpylen(s, t, strlen(t));&#125;","categories":[{"name":"源码分析","slug":"源码分析","permalink":"https://nullcc.github.io/categories/源码分析/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://nullcc.github.io/tags/Redis/"},{"name":"数据结构","slug":"数据结构","permalink":"https://nullcc.github.io/tags/数据结构/"}]},{"title":"谈谈I/O中的同步、异步、阻塞、非阻塞和I/O多路复用","slug":"谈谈IO中的同步、异步、阻塞、非阻塞和IO多路复用","date":"2017-11-09T16:00:00.000Z","updated":"2022-04-15T03:41:13.042Z","comments":true,"path":"2017/11/10/谈谈IO中的同步、异步、阻塞、非阻塞和IO多路复用/","link":"","permalink":"https://nullcc.github.io/2017/11/10/谈谈IO中的同步、异步、阻塞、非阻塞和IO多路复用/","excerpt":"谈到I/O模型，我们马上能联想到的是同步、异步，阻塞和非阻塞之类的模型。但是这些模型之间到底有什么区别，很多人只能说对一部分，但说不全，或者理解上有偏差。本文将详细剖析这其中的区别。","text":"谈到I/O模型，我们马上能联想到的是同步、异步，阻塞和非阻塞之类的模型。但是这些模型之间到底有什么区别，很多人只能说对一部分，但说不全，或者理解上有偏差。本文将详细剖析这其中的区别。 1.阻塞I/O (Blocking I/O) 阻塞I/O的典型执行过程如下： 用户进程发起recvfrom系统调用，内核执行该系统调用。 内核等待数据就绪。 数据就绪。 内核将数据从内核缓冲区拷贝到用户进程缓冲区，返回成功。 用户进程从用户进程缓冲区读取数据，处理数据，接着往下执行。 从中这个过程中我们可以观察到，用户进程在发起recvfrom系统调用后，直到数据被内核从内核缓冲区拷贝到用户进程缓冲区这整个过程中，都无法继续执行，处于被阻塞的状态，所以这是一个阻塞(Blocking)I/O。而且我们发现上面的I/O过程在内核部分分为两个步骤： 内核等待数据就绪（阻塞） 内核将数据从内核缓冲区拷贝到用户进程缓冲区（阻塞） 阻塞I/O在这两个步骤上都被阻塞了。 2.非阻塞I/O (Blocking I/O) 非阻塞I/O的执行过程如下： 用户进程发起recvfrom系统调用，内核执行该系统调用，并直接返回一个数据未就绪状态给用户进程，然后内核等待数据就绪。 在内核等待数据就绪过程中，用户进程不断发起recvfrom系统调用（轮询），向内核查询数据是否就绪了，如果数据还未就绪，内核还是返回一个数据未就绪状态给用户进程。 内核的数据就绪了，内核将数据从内核缓冲区拷贝到用户进程缓冲区，并返回成功。 用户进程从用户进程缓冲区读取数据，处理数据，接着往下执行。 这里和阻塞I/O的不同点在于，用户进程发起recvfrom系统调用后，内核会直接返回，如果数据未就绪就返回未就绪的状态，如果数据就绪就从内核缓冲区拷贝数据到用户进程缓冲区中。对于I/O过程中内核执行阶段的两个部分： 内核等待数据就绪（非阻塞） 内核将数据从内核缓冲区拷贝到用户进程缓冲区（阻塞） 我们发现，在第一阶段，用户进程不会被阻塞，但在第二阶段用户进程被阻塞了。 3. I/O多路复用 (I/O multiplexing) I/O多路复用的执行过程如下： 用户进程发起select系统调用，内核执行该系统调用。 内核等待数据就绪。 内核数据就绪，返回数据就绪。 用户进程发起recvfrom系统调用，内核将数据从内核缓冲区拷贝到用户进程缓冲区，返回成功。 用户进程从用户进程缓冲区读取数据，处理数据，接着往下执行。 当用户进程发起select系统调用时，内核会负责监控该select系统调用维护的所有socket，当有任何一个socket数据就绪时，直接返回socket就绪。之后的过程和阻塞I/O和非阻塞I/O是相同的。I/O多路复用的关键在于“多路复用”，select系统调用可以同时监控多个（Linux中一般不超过1024，这个数字定义在FD_SETSIZE中）socket的状态，当有任何一个socket就绪就处理之。 可以发现使用select系统调用时，用户进程会被阻塞在select上（也可以设置一个超时时间），当有socket就绪，用户进程调用recvfrom，用户进程又会被阻塞在数据拷贝上。因此在内核部分，I/O多路复用的执行情况如下： select系统调用（阻塞） 内核将数据从内核缓冲区拷贝到用户进程缓冲区（阻塞） 这两个阶段用户进程都被阻塞了。 4. 异步I/O (Asynchronous I/O) 异步I/O的执行过程如下： 用户进程发起异步I/O系统调用，内核直接返回并执行系统调用，用户进程此时可以继续往下执行而不会被阻塞。 内核等待数据就绪。 内核的数据就绪了，内核将数据从内核缓冲区拷贝到用户进程缓冲区，并发送信号通知用户进程数据可用。 用户进程的信号处理程序处理数据。 异步I/O过程中，用户进程从头到尾都完全没有被阻塞。 总结严格来说，上面展示的I/O模型的1，2，3都属于同步I/O，因为它们多多少少在某些地方被阻塞了，真正的异步I/O是用户进程完全不能被阻塞的。人们经常谈论到的阻塞和非阻塞实际上只考虑了数据就绪这一部分而没有考虑数据复制，我们要看到I/O调用在内核中还分成了不同部分，而这些不同就是决定其到底是同步I/O还是异步I/O的关键。按照这种标准，Linux下实际上没有真正的异步I/O，因为所有I/O方案在复制阶段都有阻塞，而Windows实现了IOCP，可以说是真正的异步I/O。","categories":[{"name":"系统编程","slug":"系统编程","permalink":"https://nullcc.github.io/categories/系统编程/"}],"tags":[{"name":"I/O","slug":"I-O","permalink":"https://nullcc.github.io/tags/I-O/"}]},{"title":"Python中的generator和yield","slug":"Python中的generator和yield","date":"2017-10-15T16:00:00.000Z","updated":"2022-04-15T03:41:13.016Z","comments":true,"path":"2017/10/16/Python中的generator和yield/","link":"","permalink":"https://nullcc.github.io/2017/10/16/Python中的generator和yield/","excerpt":"Python中的generator(生成器)是一个很强大的概念，generator function(生成器函数)被调用后会返回一个生成器。需要注意的是，generator function在被调用时，并不会真正地执行，而是返回一个generator。要想执行这个generator，有两种方式：for循环或手动调用generator的next方法。","text":"Python中的generator(生成器)是一个很强大的概念，generator function(生成器函数)被调用后会返回一个生成器。需要注意的是，generator function在被调用时，并不会真正地执行，而是返回一个generator。要想执行这个generator，有两种方式：for循环或手动调用generator的next方法。 下面的代码创建了一个generator function，这是一个计数器，参数n表示要计数器的最大值： 123456789101112def create_counter(n): print(\"in create_counter\") i = 1 while True: yield i i = i+1counter = create_counter(100)print(counter) # &lt;generator object create_counter at 0x10e55a728&gt;print(counter.next()) # 1print(counter.next()) # 2print(counter.next()) # 3 调用create_counter并不会真正执行函数，因为调用create_counter时并没有立即运行print(&quot;in create_counter&quot;)这行代码。调用后，counter变成一个generator。接着不断调用counter.next()来获取计数器的结构。 在Python中，一个function内部如果有yield关键字，它就是一个generator function。那么generator function有什么用呢？ 举个简单的例子，如果要生成一个1-1000000的数字列表，我们当然可以选择使用简单粗暴的方式，直接用： 1mylist = [i for i in range(1000000)] 来获得这个数字列表。不过有个问题，如果你想要迭代这个列表，一次性生成这个列表的代价有点大，内存占用会很高，如果要生成1亿个数呢？此时使用生成器就比较有优势了： 12for i in create_counter(1000000): print(i) 这么做会每次在迭代的时候生成一个数字，而不是一次性生成所有数字。 总结一下，对于generator function来说，在调用它的时候并不会实际执行函数内的代码，而是返回一个generator。我们通过调用它的next()方法来执行函数内部的代码。当遇到yield的时候，generator会返回yield后面表达式的值，然后就generator会“挂起”，直到下次调用它的next()方法，才继续在上次中断处往下执行。这是一种惰性求值的方式：在真正需要的时候才产生值，而不是一开始就产生所有值。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://nullcc.github.io/tags/Python/"}]},{"title":"使用supervisor管理进程","slug":"使用supervisor管理进程","date":"2017-10-12T16:00:00.000Z","updated":"2022-04-15T03:41:13.028Z","comments":true,"path":"2017/10/13/使用supervisor管理进程/","link":"","permalink":"https://nullcc.github.io/2017/10/13/使用supervisor管理进程/","excerpt":"生产环境中经常使用supervisor来管理和监控服务器进程，它可以把一个应用进程变成一个daemon，常驻后台运行，还能监控进程状态，重启、停止所管理的进程。supervisor是一个用Python写成的工具，但它不支持Python 3.x。因此真正在用它的时候，一般用Python 2.7（也是很多Linux发行版默认安装的Python版本）来启动supervisor。","text":"生产环境中经常使用supervisor来管理和监控服务器进程，它可以把一个应用进程变成一个daemon，常驻后台运行，还能监控进程状态，重启、停止所管理的进程。supervisor是一个用Python写成的工具，但它不支持Python 3.x。因此真正在用它的时候，一般用Python 2.7（也是很多Linux发行版默认安装的Python版本）来启动supervisor。 Ubuntu下直接执行下面的命令安装supervisor： 1apt-get install supervisor 先使用echo_supervisord_conf命令生成配置文件： 1echo_supervisord_conf &gt; /etc/supervisord.conf 然后在[supervisord]配置节点下，把 1logfile=/tmp/supervisord.log 改成1logfile=/var/log/supervisord.log 把1pidfile=/var/run/supervisord.pid 改成1pidfile=/var/run/supervisord.pid 在[supervisorctl]配置节点下，把 1serverurl=unix:///tmp/supervisor.sock 改成1serverurl=unix:///var/run/supervisor.sock 并把该节点下的注释符号;都删除。 最后在文件末尾追加下面的配置： 123456789[program:flask_demo]command=/bin/bash -c 'source /home/nullcc/flask_demo/venv/bin/activate &amp;&amp; gunicorn -w 4 -b :8082 index:app' ; supervisor启动命令directory=/home/nullcc/flask_demo/ ; 项目的文件夹路径startsecs=0 ; 启动时间stopwaitsecs=0 ; 终止等待时间autostart=false ; 是否自动启动autorestart=false ; 是否自动重启stdout_logfile=/home/nullcc/flask_demo/log/gunicorn.log ; log 日志stderr_logfile=/home/nullcc/flask_demo/log/gunicorn.err ; 错误日志 可以用下面的命令来启动supervisor，-c参数表示让supervisord使用指定的配置文件。 1supervisord -c /etc/supervisord.conf 启动supervisor后，可以在本地的9001端口看到supervisor的web监控界面： 开机启动Supervisord默认情况下，Supervisord没有被安装成一个服务，我们需要进行一些配置，以Ubuntu为例： 123456789# 下载脚本sudo su - root -c \"sudo curl https://gist.githubusercontent.com/howthebodyworks/176149/raw/d60b505a585dda836fadecca8f6b03884153196b/supervisord.sh &gt; /etc/init.d/supervisord\"# 设置该脚本为可以执行sudo chmod +x /etc/init.d/supervisord# 设置为开机自动运行sudo update-rc.d supervisord defaults# 试一下，是否工作正常service supervisord stopservice supervisord start 这个脚本下载下来以后，需要检查里面的一些配置，比如配置文件的地址，pid文件地址等，把它们配置成我们需要的样子以后就可以了。","categories":[{"name":"自动化部署","slug":"自动化部署","permalink":"https://nullcc.github.io/categories/自动化部署/"}],"tags":[{"name":"supervisor","slug":"supervisor","permalink":"https://nullcc.github.io/tags/supervisor/"}]},{"title":"使用nginx+gunicorn部署flask","slug":"使用nginx+gunicorn部署flask","date":"2017-10-11T16:00:00.000Z","updated":"2022-04-15T03:41:13.028Z","comments":true,"path":"2017/10/12/使用nginx+gunicorn部署flask/","link":"","permalink":"https://nullcc.github.io/2017/10/12/使用nginx+gunicorn部署flask/","excerpt":"本文会就使用nginx+gunicorn部署flask给出一个步骤说明。","text":"本文会就使用nginx+gunicorn部署flask给出一个步骤说明。 由于Ubuntu默认是Python 2.7，我们安装一个Python 3.5: 1sudo apt-get install python3 然后安装pip： 12sudo apt-get install python-pipsudo pip install --upgrade pip 使用pip安装virtualenv： 1sudo pip install virtualenv 假设源码部署在/home/nullcc/demo目录下， 执行： 1cd /home/nullcc/demo 在项目根目录下运行以下命令激活一个虚拟环境： 12virtualenv -p python3.5 --no-site-packages venv # 这个命令可能需要多试几次才能成功，天朝的网络你懂的. venv/bin/activate 激活虚拟环境后，在命令行等待符号前面会有一个(venv)标志，比如这样： 1(venv) nullcc@ubuntu:~/demo$ 如果需要取消激活当前venv，运行： 1deactivate 创建一个文件index.py，输入下列代码，这是一个最简单的flask应用： 123456789from flask import Flaskapp = Flask(__name__)@app.route('/')def hello_world(): return 'Hello World!'if __name__ == '__main__': app.run() 然后再创建一个requirements.txt： 123--index https://pypi.doubanio.com/simpleFlask==0.10.1 运行下面命令安装依赖项： 1pip install -r requirements.txt 再安装gunicorn：1pip install gunicorn 执行：1gunicorn -w 4 -b :8082 index:app --log-level=debug -w表示启动多少个worker，-b表示访问地址。 执行：1curl 127.0.0.1:8082 会返回Hello World!&#39;字符串。 至于nginx，可以使用： 1sudo apt-get install nginx 来安装，具体配置可以参考nginx的说明。","categories":[{"name":"自动化部署","slug":"自动化部署","permalink":"https://nullcc.github.io/categories/自动化部署/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://nullcc.github.io/tags/Python/"}]},{"title":"用Jenkins实现自动化部署和测试","slug":"用Jenkins实现自动化部署和测试","date":"2017-10-11T16:00:00.000Z","updated":"2022-04-15T03:41:13.037Z","comments":true,"path":"2017/10/12/用Jenkins实现自动化部署和测试/","link":"","permalink":"https://nullcc.github.io/2017/10/12/用Jenkins实现自动化部署和测试/","excerpt":"在日常开发和产品在测试环境和生产环境部署时，遇到的一个痛点就是如何高效地完成部署+测试这个环节。很多团队一直采用手工部署的方式，这在系统规模不大的情况下没问题，但是当系统发展到一定规模以后，手工对多服务和多机进行部署无疑是一个巨大的工作量，过程机械繁琐还容易出错。这时候就很有必要升级为自动化部署和测试。","text":"在日常开发和产品在测试环境和生产环境部署时，遇到的一个痛点就是如何高效地完成部署+测试这个环节。很多团队一直采用手工部署的方式，这在系统规模不大的情况下没问题，但是当系统发展到一定规模以后，手工对多服务和多机进行部署无疑是一个巨大的工作量，过程机械繁琐还容易出错。这时候就很有必要升级为自动化部署和测试。 自动化部署和测试的方案有很多，本文主要介绍如何使用Jenkins来实现自动化部署和测试。 Jenkins是一个Java开发的开源的持续集成工具(CI)，它提供了一个软件平台，主要用于实现软件的自动化构建、测试和部署。下面是系统配置： 操作系统：Ubuntu 16.04 LTSJenkins版本：Jenkins 2.73.2。 运行以下命令安装Jenkins： 1234wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add -sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ &gt; /etc/apt/sources.list.d/jenkins.list'sudo apt-get updatesudo apt-get install jenkins 安装完毕后，Jenkins会以守护进程的模式运行在后台，我们可以直接在浏览其中访问这台机器的8080端口，在我的机器上是http://172.16.130.130:8080/： 为了解锁Jenkins，我们需要访问/var/lib/jenkins/secrets/initialAdminPassword这个文件获取一个超级管理员的密码，这在每台机器上部署的密码是不一样的，找到以后填入即可。继续下一步后，Jenkins会进行一些系统初始化的工作，可能会耗费一点时间。然后进入下面的界面： 我们选择Install suggested plugins，安装系统推荐的插件，作为演示案例已经足够了。 之后系统会自动安装一些插件，安装完毕后会跳转到下面的界面要求我们创建一个管理员账号： 创建后就可以进入Jenkins的管理界面了： 点击创建一个新任务： 点击OK进入配置项目界面，有一些基础选项需要填写，我们把一个叫做flask_demo的示例程序放在Github上，可以利用Jenkins的Github Plugin来为我们自动拉取代码自动部署，配置如下： 通用配置： 源码管理配置： 构建触发器和构建环境配置（由于是示例，暂时不填）： 构建和构建后操作配置： 其中构建中的Execute Shell是一组shell命令，如下： 1234567891011pwdps -ef|grep supervisord|grep -v grep|awk '&#123;print $2&#125;'|xargs kill -9cd ..sudo rm -r /home/nullcc/flask_democp -r ./flask_demo /home/nullcccd /home/nullcc/flask_demovirtualenv -p python3.5 --no-site-packages venv. venv/bin/activatepip install -r requirements.txtdeactivatesudo supervisord -c /etc/supervisord.conf 主要的作用是把拉取的源码复制到部署目录下，并配置virtualenv，然后启动supervisor管理相应Python进程。上述shell代码只是一个很简单且不正式的例子，但是作为示例已经可以工作了。生产环境中的这部分shell脚本可能会非常复杂。也可以是用一些现成的自动化部署工具，比如Ruby的Capistrano、node的shipit等都是不错的自动化web部署工具，这些工具在多机部署时非常有用。 配置完后可以在Jenkins首页看到一个项目构建列表： 这个列表展示了所有项目的构建情况，类似最近一次构建是失败还是成功，构建持续时间等。 构建项目时，在首页还可以看到一个构建进度指示： Jenkins支持通过web hook触发自动构建，在Github等一些仓库的项目设置中可以进行配置。比如Github中的项目，可以在项目的Settings下的Webhooks选项下进行配置，具体的配置方法需要参考不同源码管理平台下的文档。 Jenkins还有很多强大的功能，比如按照日程表构建、构建后通过Email通知、和Github等平台的互操作等，有待进一步探索。","categories":[{"name":"自动化部署","slug":"自动化部署","permalink":"https://nullcc.github.io/categories/自动化部署/"}],"tags":[{"name":"自动化部署","slug":"自动化部署","permalink":"https://nullcc.github.io/tags/自动化部署/"},{"name":"Jenkins","slug":"Jenkins","permalink":"https://nullcc.github.io/tags/Jenkins/"}]},{"title":"Python多线程详解","slug":"Python多线程详解","date":"2017-10-10T16:00:00.000Z","updated":"2022-04-15T03:41:13.017Z","comments":true,"path":"2017/10/11/Python多线程详解/","link":"","permalink":"https://nullcc.github.io/2017/10/11/Python多线程详解/","excerpt":"在Python中我们常用threading来处理多线程，我们先看一个例子：","text":"在Python中我们常用threading来处理多线程，我们先看一个例子： 123456789101112131415161718# 代码段1import threadingfrom time import sleepdef counter(n): print(\"The current thread [%s] is running.\" % threading.current_thread().getName()) for i in range(n): print(i) sleep(1) print(\"The current thread [%s] ended.\" % threading.current_thread().getName())print(\"The current thread [%s] is running.\" % threading.current_thread().getName())t = threading.Thread(target=counter, args=(5,))t.start()t.join()print(\"The current thread [%s] ended.\" % threading.current_thread().getName()) 输出如下： 123456789The current thread [MainThread] is running.The current thread [Thread-1] is running.01234The current thread [Thread-1] ended.The current thread [MainThread] ended. 分析一下上面代码的一些关键点： t = threading.Thread(target=counter, args=(5,))创建了一个线程，传入的target参数是线程要执行的对象，这里是一个函数，args是给函数传递的参数。 t.start()启动线程。 t.join()会阻塞调用线程的那个线程直到被调用线程结束，所以我们看到MainThread在Thread-1退出后才退出。 再看下面一段代码： 1234567891011121314151617# 代码段2import threadingfrom time import sleepdef counter(n): print(\"The current thread [%s] is running.\" % threading.current_thread().getName()) for i in range(n): print(i) sleep(1) print(\"The current thread [%s] ended.\" % threading.current_thread().getName())print(\"The current thread [%s] is running.\" % threading.current_thread().getName())t = threading.Thread(target=counter, args=(5,))t.start()print(\"The current thread [%s] ended.\" % threading.current_thread().getName()) 上面这段代码唯一的区别是去掉了t.join()，输出如下： 123456789The current thread [MainThread] is running.The current thread [Thread-1] is running.0The current thread [MainThread] ended.1234The current thread [Thread-1] ended. 我们发现MainThread没有等Thread-1执行就结束了，这之后Thread-1继续执行直到结束。这印证了刚才提到的一个事实：t.join()会阻塞调用线程的那个线程直到被调用线程结束。 接下来看另外一段代码： 12345678910111213141516171819202122232425262728# 代码段3import threadingfrom time import sleepdef daemon(): print(\"The current thread [%s] is running.\" % threading.current_thread().getName()) while True: print('in daemon') sleep(1) print(\"The current thread [%s] ended.\" % threading.current_thread().getName())def counter(n): print(\"The current thread [%s] is running.\" % threading.current_thread().getName()) for i in range(n): print(i) sleep(1) print(\"The current thread [%s] ended.\" % threading.current_thread().getName())print(\"The current thread [%s] is running.\" % threading.current_thread().getName())t1 = threading.Thread(target=daemon)t2 = threading.Thread(target=counter, args=(5,))t1.setDaemon(True)t2.setDaemon(False)t1.start()t2.start()print(\"The current thread [%s] ended.\" % threading.current_thread().getName()) 输出如下： 12345678910111213141516The current thread [MainThread] is running.The current thread [Thread-1] is running.in daemonThe current thread [Thread-2] is running.0The current thread [MainThread] ended.in daemon1in daemon2in daemon3in daemon4in daemonThe current thread [Thread-2] ended. 在代码段3中，线程t1被我们设置成守护线程，里面是一个无限循环，线程t2是用户线程，从0计数到4，每秒计数一次。可以观察到，当t2结束时，t1也跟着结束。在threading.py中，有这么一段注释：The entire Python program exits when no alive non-daemon threads are left.也就是说，当一个Python进程中没有任何非守护线程时，这个进程就会退出。在一个进程中，守护线程主要是用来在后台做一些幕后工作，是为用户线程服务的，用户线程都退出了，守护线程也失去了守护对象，所以也退出了。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://nullcc.github.io/tags/Python/"}]},{"title":"MySQL中InnoDB引擎的事务隔离级别","slug":"MySQL中InnoDB引擎的事务隔离级别","date":"2017-09-28T16:00:00.000Z","updated":"2022-04-15T03:41:13.015Z","comments":true,"path":"2017/09/29/MySQL中InnoDB引擎的事务隔离级别/","link":"","permalink":"https://nullcc.github.io/2017/09/29/MySQL中InnoDB引擎的事务隔离级别/","excerpt":"如果想要对MySQL中的InnoDB引擎的事务特性有深入的了解，就必须掌握它的四种事务隔离级别。如下：","text":"如果想要对MySQL中的InnoDB引擎的事务特性有深入的了解，就必须掌握它的四种事务隔离级别。如下： READ UNCOMMITTED（未提交读） READ COMMITTED（提交读） REPEATABLE READ（可重复读） SERIALIZABLE（可串行化） READ UNCOMMITTED（未提交读） 在READ UNCOMMITTED的隔离级别下，一个事务中的修改，即使还没有被提交，对其他事务也是可见的。这有可能导致“脏读”的问题。设想一种情况，事务A开始，做了一些处理，紧接着事务B也开始，修改了一些数据，但还未提交，此时事务A读取了事务B修改的这部分数据，并用于自己的处理，但之后事务B的其他操作失败导致回滚。这时候事务B的这些修改实际上最终并未应用于数据库，但事务A却依赖了这些修改。一般情况下如果不是特别需要，不会使用READ UNCOMMITTED。 READ COMMITTED（提交读） 在READ COMMITTED的隔离级别下，一个事务只能看见已经被提交的事务做的修改。也就是说，一个事务的修改在被提交之前对其他事务都是不可见的。READ COMMITTED又被称为UNREPEATABLE READ（不可重复读），因为在一个事务中，多次读取同一个数据有可能得到不同的结果（如果有其他事务在此期间修改了这个数据并成功提交）。这是大部分数据库的默认隔离级别。 REPEATABLE READ（可重复读） 在REPEATABLE READ的隔离级别下，在同一个事务中多次读取相同记录的结果是相同的，因此它解决了“脏读”的问题。这是MySQL InnoDB的默认隔离级别。 SERIALIZABLE（可串行化） 在SERIALIZABLE是最高的隔离级别。读加共享锁，写加排他锁，读写互斥。这可能导致大量的锁争用问题，因此并发能力很低，而且经常会导致死锁的问题，一般很少使用。在某些必须保证请求串行执行的高一致性场合有可能被使用。 一些名词解释解释一下几个重要的名词：脏读、可重复读、不可重复读、幻读、快照读、当前读和加锁读。 脏读 对于事务A和事务B来说，如果事务B对数据做了修改，但还未提交，此时如果事务A能够读取到事务B的这个数据更新，就是脏读。 可重复读 可重复读指的是，对于一个事务来说，在开始后别的事务对数据做的更新都不会被它读取到，这个事务只能读取到事务开始前数据的状态。由于在同一个事务内，多次读取同一行一数据都都会返回同一个结果（不管此时这行数据有没有被其他事务所修改），所以被称为“可重复读”。 不可重复读 假设有这样一种场景，事务A开始，读取了一行数据，然后事务B开始，修改了这行数据，然后事务B提交，之后事务A又读取了这行数据，此时事务A读取到的数据是事务B修改后的样子。由于在同一个事务内，多次读取同一行一数据可能返回不同的结果（因为其他事务修改了这行数据），所以被称为“不可重复读”。 幻读 幻读的场景是，事务A开始，用某个条件查询出了一个结果集，然后事务B开始，插入了一行数据，这行数据满足之前的查询条件，然后事务B提交，接着事务A再次用相同的条件查询结果，得到的结果比原来多了这一行事务B新提交的数据。 快照读 在REPEATABLE READ级别，由于是可重复读，实际上我们读到的是历史数据，又叫“快照读”。 当前读 插入、更新和删除操作都属于一种特殊的读操作，叫当前读。因为它们都需要读取出最新的数据而不是历史的“快照”数据。 加锁读 加锁读只出现在SERIALIZABLE级别中，该级别对读操作加共享锁，对写操作加排他锁，读写互斥。 四种隔离级别的一些特性总结： 隔离级别 脏读可能性 不可重复读可能性 幻读可能性 加锁读 READ UNCOMMITTED Yes Yes Yes No READ COMMITTED No Yes Yes No REPEATABLE READ No No Yes No SERIALIZABLE No No No Yes 我们知道，行锁可以用来防止不同事务之间对数据的修改操作造成的冲突问题，但是对于插入操作却无能为力。为了满足事务的ACID特性，我们防止插入操作造成的幻读问题，因此就有了Next-Key锁。Next-Key锁是行锁和间隙锁的结合，它可以让InnoDB在REPEATABLE READ级别下不会发生别的事务插入导致的幻读。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://nullcc.github.io/categories/数据库/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://nullcc.github.io/tags/MySQL/"},{"name":"InnoDB","slug":"InnoDB","permalink":"https://nullcc.github.io/tags/InnoDB/"}]},{"title":"Python中try-except-finally的问题","slug":"Python中try-except-finally的问题","date":"2017-08-28T16:00:00.000Z","updated":"2022-04-15T03:41:13.016Z","comments":true,"path":"2017/08/29/Python中try-except-finally的问题/","link":"","permalink":"https://nullcc.github.io/2017/08/29/Python中try-except-finally的问题/","excerpt":"代码片段1： 1234567891011def func(): try: print(\"In try\") return True except: return False finally: print(\"In finally\")res = func()print(res)","text":"代码片段1： 1234567891011def func(): try: print(\"In try\") return True except: return False finally: print(\"In finally\")res = func()print(res) 运行结果： In try In finally True 在try块中如果没有错误发生，运行顺序是这样的： try -&gt; finally -&gt; try return 代码片段2： 12345678910111213def func(): try: print(\"In try\") raise Error() return True except: print(\"In except\") return False finally: print(\"In finally\")res = func()print(res) 运行结果： In try In except In finally False 在try块中如果有错误发生，运行顺序是这样的： try -&gt; except -&gt; finally- &gt; except return 代码片段3： 12345678910111213def func(): try: print(\"In try\") return True except: print(\"In except\") return False finally: print(\"In finally\") return \"finally\"res = func()print(res) 运行结果： In try In finally finally 在try块中如果没有错误发生，且finally块中有return，运行顺序是这样的： try -&gt; finally -&gt; finally return 代码片段4： 1234567891011121314def func(): try: print(\"In try\") raise Error() return True except: print(\"In except\") return False finally: print(\"In finally\") return \"finally\"res = func()print(res) 运行结果： In try In except In finally finally 在try块中如果有错误发生，且finally块中有return，运行顺序是这样的： try -&gt; except -&gt; finally -&gt; finally return 从上面四个代码片段可以总结出try-except-finally的规律： 在finally块中没有return语句的情况下，在try或except块返回之前，都会先运行finally块，然后再返回到try或except块结束整个执行流。 在finally块中有return语句的情况下，在try或except块返回之前，都会先运行finally块，然后直接返回，此时不会再返回到try或except块。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://nullcc.github.io/tags/Python/"}]},{"title":"MySQL锁表遇到的问题","slug":"MySQL锁表遇到的问题","date":"2017-08-24T16:00:00.000Z","updated":"2022-04-15T03:41:13.016Z","comments":true,"path":"2017/08/25/MySQL锁表遇到的问题/","link":"","permalink":"https://nullcc.github.io/2017/08/25/MySQL锁表遇到的问题/","excerpt":"今天在测试环境要更改一个表的结构，一个操作是把某个字段改成unique的，但是这个表中原来的数据这个字段有重复，于是在提交更改的时候就失败了。","text":"今天在测试环境要更改一个表的结构，一个操作是把某个字段改成unique的，但是这个表中原来的数据这个字段有重复，于是在提交更改的时候就失败了。 之后发现对这个表的查询全部超时，马上想到难道是表被锁定了。于是查了下查看表当前进程的命令： show processlist 发现了之前提交的更改表结构的语句，执行失败却把表锁定了，此时需要解除这种锁定，找到相应的id后执行： kill [锁表SQL的Id] 之后表锁解除，查询正常。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://nullcc.github.io/categories/数据库/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://nullcc.github.io/tags/MySQL/"}]},{"title":"Ubuntu下使用shipit构建自动化部署","slug":"Ubuntu下使用shipit构建自动化部署","date":"2017-08-23T16:00:00.000Z","updated":"2022-04-15T03:41:13.021Z","comments":true,"path":"2017/08/24/Ubuntu下使用shipit构建自动化部署/","link":"","permalink":"https://nullcc.github.io/2017/08/24/Ubuntu下使用shipit构建自动化部署/","excerpt":"安装先安装node，执行： sudo apt-get update sudo apt-get install nodejs 然后安装node版本管理工具nvm： wget -qO- https://raw.githubusercontent.com/creationix/nvm/v0.33.0/install.sh | bash 然后运行： nvm install node 可以安装最新版本的node。","text":"安装先安装node，执行： sudo apt-get update sudo apt-get install nodejs 然后安装node版本管理工具nvm： wget -qO- https://raw.githubusercontent.com/creationix/nvm/v0.33.0/install.sh | bash 然后运行： nvm install node 可以安装最新版本的node。 安装shipit-deploy接下来安装shipit-deploy，执行： sudo npm install shipit-cli -g npm install shipit-deploy 配置新建shipitfile.js文件： module.exports = function (shipit) { require(&apos;shipit-deploy&apos;)(shipit); shipit.initConfig({ default: { workspace: &apos;/home/nullcc/workspace&apos;, deployTo: &apos;/home/nullcc/deploy/koa2_demo&apos;, repositoryUrl: &apos;https://github.com/nullcc/koa2_demo.git&apos;, ignores: [&apos;.git&apos;, &apos;node_modules&apos;], keepReleases: 2, deleteOnRollback: false, // key: &apos;/path/to/key&apos;, shallowClone: true }, staging: { servers: &apos;nullcc@172.16.130.130&apos; } }); }; 运行： shipit staging deploy 备注需要注意，为了使整个部署过程完全自动化不需要输入密码，需要把加入ssh-key，具体步骤如下（在你的home目录下）： ssh-keygen # 新建密钥对 cd .ssh cat id_rsa.pub &gt;&gt; authorized_keys chmod 600 authorized_keys chmod 700 ~/.ssh 然后编辑/etc/ssh/sshd_config文件： RSAAuthentication yes PubkeyAuthentication yes PermitRootLogin yes 保存后重启SSH服务： service sshd restart 之后再运行shipit staging deploy就不用输入密码了。","categories":[{"name":"自动化部署","slug":"自动化部署","permalink":"https://nullcc.github.io/categories/自动化部署/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://nullcc.github.io/tags/Linux/"},{"name":"自动化部署","slug":"自动化部署","permalink":"https://nullcc.github.io/tags/自动化部署/"}]},{"title":"ubuntu启用SSH","slug":"ubuntu启用SSH","date":"2017-08-19T16:00:00.000Z","updated":"2022-04-15T03:41:13.026Z","comments":true,"path":"2017/08/20/ubuntu启用SSH/","link":"","permalink":"https://nullcc.github.io/2017/08/20/ubuntu启用SSH/","excerpt":"先安装ssh服务，执行命令： sudo apt-get install openssh-server 安装后可用一以下命令查看ssh服务的启用情况： sudo pe -e | grep ssh 如果ssh服务没有启用，可以执行： sudo service ssh start 来启动ssh服务，如果要关闭ssh服务，相应地输入： sudo service ssh stop 即可。","text":"先安装ssh服务，执行命令： sudo apt-get install openssh-server 安装后可用一以下命令查看ssh服务的启用情况： sudo pe -e | grep ssh 如果ssh服务没有启用，可以执行： sudo service ssh start 来启动ssh服务，如果要关闭ssh服务，相应地输入： sudo service ssh stop 即可。 然后用ifconfig命令查看本机IP，假设IP为172.16.130.130，在其他机器上执行： ssh nullcc@172.16.130.130 -p 22 再输入用户密码即可SSH到这台机器上进行操作。","categories":[{"name":"其他","slug":"其他","permalink":"https://nullcc.github.io/categories/其他/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://nullcc.github.io/tags/Linux/"}]},{"title":"MySQL主从复制详解","slug":"MySQL主从复制详解","date":"2017-08-16T16:00:00.000Z","updated":"2022-04-15T03:41:13.015Z","comments":true,"path":"2017/08/17/MySQL主从复制详解/","link":"","permalink":"https://nullcc.github.io/2017/08/17/MySQL主从复制详解/","excerpt":"数据库主从复制的作用 在保证系统高可用时，我们会想把数据库做成主-从的架构，这可以通过binlog，把主库的内容同步到从库，以备份数据。更进一步，如果主库挂了，从库可以顶上，保证系统的高可用性。","text":"数据库主从复制的作用 在保证系统高可用时，我们会想把数据库做成主-从的架构，这可以通过binlog，把主库的内容同步到从库，以备份数据。更进一步，如果主库挂了，从库可以顶上，保证系统的高可用性。 目标系统 Ubuntu Server LTS 16.04.3 x64（两台，主从） master: 172.16.130.130（主）MySQL版本5.7.19 backup: 172.16.130.131（从）MySQL版本5.7.19 安装 安装MySQL： sudo apt-get install mysql-server 配置 修改主服务的/etc/mysql/my.cnf文件，添加： [mysqld] server-id=1 log_bin=master-bin binlog_format=mixed binlog_do_db=blog binlog_ignore_db=mysql 然后重启主服务器mysql： /etc/init.d/mysql restart 登录主服务器mysql，执行： grant replication slave on *.* to &apos;slave&apos; @&apos;172.16.130.131&apos; identified by &apos;123456&apos;; 然后刷新一下权限表： FLUSH PRIVILEGES; 然后登录主服务器mysql，执行： show master status; 可以看到如下结果： 接着修改从服务器的/etc/mysql/my.cnf文件，添加： [mysqld] server-id=2 binlog_format=mixed binlog_do_db=blog binlog_ignore_db=mysql 然后重启从服务器mysql，执行： /etc/init.d/mysql restart 登录从服务器mysql，连接主服务器，执行： change master to master_host=&apos;172.16.130.130&apos;, master_port=3306, master_user=&apos;slave&apos;, master_password=&apos;123456&apos;, master_log_file=&apos;master-bin.000007&apos;, master_log_pos=1510 注意这里master_host为master的地址，master_user和master_password分别为之前master分配给slave的账号和密码，master_log_file是之前执行show master status;时File的值， master_log_pos是之前执行show master status;时Position的值。 然后启动slave数据同步： start slave; 查看slave状态： show slave status\\G; 可以看到： 说明从服务器连接主服务器成功。 这时候主服务器上blog库的变化会同步给从服务器上的blog库。 遇到的问题主从同步失败的可能原因如下： 主从的/etc/mysql/my.cnf中server-id配置成一样的。 主的MySQL不允许远程连接。 从在执行change master时参数有问题。 相应的解决方案： 主从/etc/mysql/my.cnf中server-id一定要不同。 检查主的iptables，允许3306端口对外开放。 从在执行change master时的参数一定要和主执行show master status;时的对应参数一致。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://nullcc.github.io/categories/数据库/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://nullcc.github.io/tags/MySQL/"}]},{"title":"理解和使用keepalived（部署配置篇）","slug":"理解和使用keepalived（部署配置篇）","date":"2017-08-14T16:00:00.000Z","updated":"2022-04-15T03:41:13.037Z","comments":true,"path":"2017/08/15/理解和使用keepalived（部署配置篇）/","link":"","permalink":"https://nullcc.github.io/2017/08/15/理解和使用keepalived（部署配置篇）/","excerpt":"keepalived的用处 在分布式系统架构中，为了保证高可用，我们通常的做法是使用双机热备的方式来提供服务。举个例子，我们有一个nginx作为反向代理对外提供服务，如果只有一个nginx实例，一旦该实例不可用，请求就无法转发到下游服务，造成整体服务出现问题。因此可以准备一个备服务器，一旦主服务器服务挂了，立刻自动切换至备服务器，整个切换过程对外透明，不会给用户造成影响。keepalived的作用就是负责检测主服务器的服务状态，一旦发现不可用，就切换到备服务器上，如果主服务器恢复正常，会再次切换到主服务器上。","text":"keepalived的用处 在分布式系统架构中，为了保证高可用，我们通常的做法是使用双机热备的方式来提供服务。举个例子，我们有一个nginx作为反向代理对外提供服务，如果只有一个nginx实例，一旦该实例不可用，请求就无法转发到下游服务，造成整体服务出现问题。因此可以准备一个备服务器，一旦主服务器服务挂了，立刻自动切换至备服务器，整个切换过程对外透明，不会给用户造成影响。keepalived的作用就是负责检测主服务器的服务状态，一旦发现不可用，就切换到备服务器上，如果主服务器恢复正常，会再次切换到主服务器上。 目标系统 Ubuntu Server LTS 16.04.3 x64（两台，主备） 安装 安装keepalived: sudo apt-get install keepalived 安装nginx: sudo apt-get install nginx 开启SSH服务 sudo apt-get install openssh-server 安装后可以通过： ssh username@host -p 22 ssh到虚拟机中操作 网络配置准备两台Ubuntu虚拟主机，配置内网IP地址： 主： sudo ifconfig ens33 172.16.130.130 netmask 255.255.255.0 备： sudo ifconfig ens33 172.16.130.131 netmask 255.255.255.0 配置后： master: 172.16.130.130（主） backup: 172.16.130.131（备） 需要注意的是主备机器的IP必须在相同的网段。 然后需要配置主备机器的虚拟IP地址，设置为172.16.130.150，分别对主备两台机器运行： sudo ifconfig ens33:0 172.16.130.150 netmask 255.255.255.0 配置后： vip: 172.16.130.150（虚拟IP） 如果要删除虚拟IP，可以运行： ifconfig ens33:0 down 为了让虚拟机重启后虚拟IP的配置依然有效，需要在/etc/network/interfaces文件中添加： auto ens33:0 iface ens33:0 inet static name Ethernet alias LAN card address 172.16.130.150 netmask 255.255.255.0 broadcast 172.16.130.255 network 172.16.130.0 然后重启网络服务： /etc/init.d/networking restart 然后在/etc/keepalived/keepalived.conf中输入配置，注意主从服务器的配置有一点点差别： 主服务器： global_defs { notification_email { } } vrrp_script chk_nginx { script &quot;/etc/keepalived/check_nginx.sh&quot; interval 2 # 每2s检查一次 weight -5 # 检测失败（脚本返回非0）则优先级减少5个值 fall 3 # 如果连续失败次数达到此值，则认为服务器已down rise 2 # 如果连续成功次数达到此值，则认为服务器已up，但不修改优先级 } vrrp_instance VI_1 { # 实例名称 state MASTER # 可以是MASTER或BACKUP，不过当其他节点keepalived启动时会自动将priority比较大的节点选举为MASTER interface ens33 # 节点固有IP（非VIP）的网卡，用来发VRRP包做心跳检测 virtual_router_id 51 # 虚拟路由ID,取值在0-255之间,用来区分多个instance的VRRP组播,同一网段内ID不能重复;主备必须为一样 priority 100 # 权重，主服务器要比备服务器高 advert_int 1 # 检查间隔默认为1秒,即1秒进行一次master选举(可以认为是健康查检时间间隔) authentication { # 认证区域,认证类型有PASS和HA（IPSEC）,推荐使用PASS(密码只识别前8位) auth_type PASS # 默认是PASS认证 auth_pass 1111 # PASS认证密码 } virtual_ipaddress { 172.16.130.150 # 虚拟VIP地址,允许多个,一行一个 } track_script { # 引用VRRP脚本，即在 vrrp_script 部分指定的名字。定期运行它们来改变优先级，并最终引发主备切换。 chk_nginx } } 备服务器： global_defs { notification_email { } } vrrp_script chk_nginx { script &quot;/etc/keepalived/check_nginx.sh&quot; interval 2 # 每2s检查一次 weight -5 # 检测失败（脚本返回非0）则优先级减少5个值 fall 3 # 如果连续失败次数达到此值，则认为服务器已down rise 2 # 如果连续成功次数达到此值，则认为服务器已up，但不修改优先级 } vrrp_instance VI_1 { # 实例名称 state BACKUP # 可以是MASTER或BACKUP，不过当其他节点keepalived启动时会自动将priority比较大的节点选举为MASTER interface ens33 # 节点固有IP（非VIP）的网卡，用来发VRRP包做心跳检测 virtual_router_id 51 # 虚拟路由ID,取值在0-255之间,用来区分多个instance的VRRP组播,同一网段内ID不能重复;主备必须为一样 priority 50 # 权重，主服务器要比备服务器高 advert_int 1 # 检查间隔默认为1秒,即1秒进行一次master选举(可以认为是健康查检时间间隔) authentication { # 认证区域,认证类型有PASS和HA（IPSEC）,推荐使用PASS(密码只识别前8位) auth_type PASS # 默认是PASS认证 auth_pass 1111 # PASS认证密码 } virtual_ipaddress { 172.16.130.150 # 虚拟VIP地址,允许多个,一行一个 } track_script { # 引用VRRP脚本，即在 vrrp_script 部分指定的名字。定期运行它们来改变优先级，并最终引发主备切换。 chk_nginx } } check_nginx.sh: #more /etc/keepalived/check_http.sh #!/bin/bash #代码一定注意空格，逻辑就是：如果nginx进程不存在则启动nginx,如果nginx无法启动则kill掉keepalived所有进程 A=`ps -C nginx --no-header | wc -l` if [ $A -eq 0 ];then /etc/init.d/nginx start sleep 3 if [ `ps -C nginx --no-header | wc -l` -eq 0 ];then killall keepalived fi fi 需要注意的一个问题是，check_nginx.sh的执行权限一定要配置正确，否则keepalived运行可能会有问题。 保存后执行： sudo /etc/init.d/keepalived restart 配置完毕。 测试我们在172.16.130.130（主）这台机器上执行： ps -ef | grep nginx 结果如下： root 5230 1 0 16:27 ? 00:00:00 nginx: master process /usr/sbin/nginx -g daemon on; master_process on; www-data 5231 5230 0 16:27 ? 00:00:00 nginx: worker process nullcc 5233 5046 0 16:27 pts/0 00:00:00 grep --color=auto nginx 说明这台机器nginx确实是在后台运行的，172.16.130.131（备）也是类似。测试结果如下： http://172.16.130.130（主）正常 http://172.16.130.131（备）正常 http://172.16.130.150（虚拟IP）正常 此时可以在172.16.130.130（主）上执行： ip a 当前虚拟IP指向的是主服务器： 如上图所示，此时访问虚拟IP http://172.16.130.150 实际上访问的是172.16.130.130这台主服务器的nginx服务。现在我们直接关闭172.16.130.130（主）的服务，再依次访问主备服务器和虚拟IP： http://172.16.130.130（主）失败 http://172.16.130.131（备）正常 http://172.16.130.150（虚拟IP）正常 说明此时keepalived已经监测到主服务器的nginx不可用了，把虚拟IP漂移到备服务器上，保证了nginx的高可用。 此时可以在172.16.130.131（备）上执行： ip a 当前虚拟IP指向的是备服务器： 如果我们再把172.16.130.130（主）启动起来，keepalived会感知到主服务器恢复正常，会再次把虚拟IP指向主服务器。","categories":[{"name":"web后端","slug":"web后端","permalink":"https://nullcc.github.io/categories/web后端/"}],"tags":[{"name":"keepalived","slug":"keepalived","permalink":"https://nullcc.github.io/tags/keepalived/"}]},{"title":"后端架构设计的一些想法","slug":"后端架构设计的一些想法","date":"2017-08-08T16:00:00.000Z","updated":"2022-04-15T03:41:13.030Z","comments":true,"path":"2017/08/09/后端架构设计的一些想法/","link":"","permalink":"https://nullcc.github.io/2017/08/09/后端架构设计的一些想法/","excerpt":"存在问题一般的项目，在初始阶段由于团队人力物力有限，不大可能把模块拆分得很细。为了节省资源和快速开发，会选择把所有功能做在一个项目中，一次部署就完成了整个系统的部署。","text":"存在问题一般的项目，在初始阶段由于团队人力物力有限，不大可能把模块拆分得很细。为了节省资源和快速开发，会选择把所有功能做在一个项目中，一次部署就完成了整个系统的部署。 随着需求的变化、用户数量的增加，并发量提高，系统复杂度也会不断提高，原来的架构和开发模式会带来很多问题。 系统各个模块之间具有强耦合性。 很多业务逻辑关联度不大的代码会对实际的业务逻辑代码造成比较强的侵入，比如各种写日志的操作代码会遍布系统的各个角落。 一些功能可能会重复被开发，且开发人员的水平参差不齐，容易造成代码冗余，严重情况下可能造成性能问题。 原来的项目协作方式会成为项目推进的瓶颈，所有人都工作在同一份代码上，沟通不顺畅，团队内部一团糟。 解决方法此时是项目架构调整的好时机。需要考虑以下一些问题： 如何对系统功能进行划分，解耦系统的各个部分。 各个子系统之间如何沟通？ 如何做到高可用？ 如何把系统设计成可以水平扩展的架构（只要增加机器就能线性地提高性能，理论上可以拥有无限的性能）？ 新架构下的团队协作如何展开？ 一个比较通用的后端架构如下图： 对于一些可以独立出来开发的功能，比如用户模块、支付模块、日志模块、消息模块、后台模块，都可以考虑单独封装成服务来开发和运维。这些独立的服务对外提供一致的接口，供API端、后台等用户界面调用。这样各个模块之间的耦合度就非常低了，对于一个特定的服务调用，可以由专门的人进行集中维护和优化，调用方也不会直接接触到数据库，提高了整体的安全性和可用性，再也不用担心别的系统用一个效率很差的查询来把数据库拖垮了。对于调用方来说，事情变得简单了，只要调用后端服务提供的接口就好，所有调用方都可以从服务层获得稳定、一致、高效的服务。 服务拆分粒度我们也需要慎重考虑服务拆分的粒度。拆分粒度太粗，还是容易造成原来的那些问题；拆分粒度太细，维护成本又高。服务拆分粒度需要具体问题具体分析，如何判断要不要拆分一个服务呢？下面列举几种可能的情况供参考： 当系统中的某个功能模块对整体性能造成影响时，应该考虑拆分出来，并且在服务级别做负载均衡。 一个模块很明显地和很多模块有直接联系，应该考虑拆分出来，独立对外提供服务。 调用方和服务的交互一般来说，存在两种类型的远程调用： 需要立刻得到响应的调用。 不需要立刻得到响应的调用。 需要立刻得到响应的调用的例子有查询用户订单记录、查询用户基本信息等。不需要立刻得到响应的调用有写一个行为日志到日志服务中、请求某个服务执行某个异步操作等。这两种类型的远程调用应该区别对待，对于需要立刻得到响应的，我们可以使用各种RPC方案，比如JSONRPC、thrift、protobuf等，具体RPC方案的选择在此不讨论。对于不需要立刻得到响应的，可以利用消息队列来处理，上游调用方只管往MQ的某些频道中中扔消息，下游相关服务会订阅这些频道，负责消费消息。比如刚才提到的写行为日志的操作，调用方根本不关心如何写日志，这些调用方（生产者）仅仅是把一条日志消息给MQ，由MQ路由这些消息到下游服务（消费者），这样就完成了相关服务和调用方的解耦。而且可以对同一个服务启动多个实例进行消费，MQ会对这些实例做轮询，保证服务的负载均衡。","categories":[{"name":"web后端","slug":"web后端","permalink":"https://nullcc.github.io/categories/web后端/"}],"tags":[{"name":"后端架构","slug":"后端架构","permalink":"https://nullcc.github.io/tags/后端架构/"}]},{"title":"node中的module.exports和exports的区别","slug":"node中的module.exports和exports的区别","date":"2017-07-18T16:00:00.000Z","updated":"2022-04-15T03:41:13.025Z","comments":true,"path":"2017/07/19/node中的module.exports和exports的区别/","link":"","permalink":"https://nullcc.github.io/2017/07/19/node中的module.exports和exports的区别/","excerpt":"对于做node.js开发的同学来说，肯定经常要与module.exports和exports这两个东西打交道，似乎在它们都可以用于导出模块。","text":"对于做node.js开发的同学来说，肯定经常要与module.exports和exports这两个东西打交道，似乎在它们都可以用于导出模块。 不过为何要设置两种方式来导出模块呢？我们就来研究一下module.exports和exports的关联和区别。 我们先来看看在一个模块中，module.exports和exports的值分别是什么。 1234// a.jsconsole.log(module.exports); // &#123;&#125;console.log(exports); // &#123;&#125;console.log(module.exports === exports); // true 运行上述代码，我们发现module.exports和exports的初始值都是{}，更重要的一点是初始状态下，module.exports和exports指向的是同一个对象。也就是说，初始状态下，module.exports就是exports。这个信息对我们很重要，接着往下看。 分别运行下面两段代码： 123456789101112// a.jsmodule.exports = function(name, age) &#123; this.name = name; this.age = age; this.say = function() &#123; console.log(\"My name is \" + name + \", I\\'m \" + age + \" yeas old.\") &#125;&#125;console.log(module.exports); // [Function]console.log(exports); // &#123;&#125;console.log(module.exports === exports); // false 1234567// b.jsvar Person = require(\"./a.js\");console.log(Person); // [Function]jack = new Person(\"Jack\", 30); // My name is Jack, I'm 30 yeas old.jack.say(); a.js中对module.exports进行赋值，在b.js中require了a.js，a.js通过module.exports导出了一个function。 我们再运行下面两段代码： 123456789101112// a.jsexports = function(name, age) &#123; this.name = name; this.age = age; this.say = function() &#123; console.log(\"My name is \" + name + \", I\\'m \" + age + \" yeas old.\") &#125;&#125;console.log(module.exports); // &#123;&#125;console.log(exports); // [Function]console.log(module.exports === exports); // false 1234567// b.jsvar Person = require(\"./a.js\");console.log(Person); // &#123;&#125;jack = new Person(\"Jack\", 30); // TypeError: Person is not a functionjack.say(); 运行结果和之前完全不同，首先在a.js中，module.exports的值是{}，exports则被赋值了一个[Function]，此时module.exports和exports不再是同一个东西，它们各有所指。在b.js中，从a.js中导出的对象是{}，因此调用jack = new Person(&quot;Jack&quot;, 30);会报错。这说明了一个很重要的事实：在node中从模块中导出都是通过module.exports，它是模块和外界交互的一个接口。 再来： 123456789101112// a.jsexports.Person = function(name, age) &#123; this.name = name; this.age = age; this.say = function() &#123; console.log(\"My name is \" + name + \", I\\'m \" + age + \" yeas old.\") &#125;&#125;console.log(module.exports); // &#123; Person: [Function] &#125;console.log(exports); // &#123; Person: [Function] &#125;console.log(module.exports === exports); // true 1234567// b.jsvar Person = require(\"./a.js\").Person;console.log(Person); // [Function]jack = new Person(\"Jack\", 30); // My name is Jack, I'm 30 yeas old.jack.say(); 这里并没有直接覆盖exports，而是对它的一个属性进行赋值，此时module.exports和exports还是指向同一个对象。 上述几个代码片段的结果引出了以下三个结论： module.exports的初始值是{}。 初始状态下，exports是module.exports的引用，如果对exports赋值（而不是对它的属性赋值），exports就不再指向module.exports。 node使用module.exports导出模块。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"node","slug":"node","permalink":"https://nullcc.github.io/tags/node/"}]},{"title":"谈谈Python中的super","slug":"谈谈Python中的super","date":"2017-07-13T16:00:00.000Z","updated":"2022-04-15T03:41:13.043Z","comments":true,"path":"2017/07/14/谈谈Python中的super/","link":"","permalink":"https://nullcc.github.io/2017/07/14/谈谈Python中的super/","excerpt":"先来看一段Python代码：","text":"先来看一段Python代码： 123456789101112131415161718192021222324252627282930313233class Base(object): def __init__(self): print(\"enter Base\") print(\"leave Base\")class A(Base): def __init__(self): print(\"enter A\") super(A, self).__init__() print(\"leave A\")class B(Base): def __init__(self): print(\"enter B\") super(B, self).__init__() print(\"leave B\")class C(A, B): def __init__(self): print(\"enter C\") super(C, self).__init__() print(\"leave C\")if __name__ == \"__main__\": c = C() 运行这段代码的结果是： 12345678enter Center Aenter Benter Baseleave Baseleave Bleave Aleave C 继承关系为： 1234567 Base / \\ / \\A B \\ / \\ / C __init__的调用顺序是C-&gt;A-&gt;B-&gt;Base。 我们可以用C.mro()看一下类的继承顺序： 1[&lt;class '__main__.C'&gt;, &lt;class '__main__.A'&gt;, &lt;class '__main__.B'&gt;, &lt;class '__main__.Base'&gt;, &lt;class 'object'&gt;] 所以这里super(ClassName, self).__init__()并不是仅仅表示对父类的调用，而是调用在继承链上位于ClassName后一个的类的__init__方法。因此super(C, self).__init__()调用的是位于继承链上C的下一个类，也就是A，其他的调用类似。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://nullcc.github.io/tags/Python/"}]},{"title":"Python中_x,__x和__x__的区别","slug":"Python中_x,__x和__x__的区别","date":"2017-06-11T16:00:00.000Z","updated":"2022-04-15T03:41:13.016Z","comments":true,"path":"2017/06/12/Python中_x,__x和__x__的区别/","link":"","permalink":"https://nullcc.github.io/2017/06/12/Python中_x,__x和__x__的区别/","excerpt":"Python中x,\\_x和__x__的区别很多人不甚了解，本文将做一个全面介绍。","text":"Python中x,\\_x和__x__的区别很多人不甚了解，本文将做一个全面介绍。 假设有一个类： 123456789101112131415class EncryptedFile(): \"\"\" 加密文件类 \"\"\" def __init__(self, name, content): self.name = name self.content = content def get_encrypted_content(self): return self._encrypt() def _encrypt(self): # encrypt the file content here 上述代码中出现了__init__和_encrypt两个方法，__init__是一个Python的魔术方法，它是内建的方法，这个方法负责初始化Python类的实例，还有很多魔术方法，比如__len__、__new__等等。_encrypt是一个私有的方法，实际上Python并没有Java那种真正私有的方法，Python在规范中说明了私有方法或私有变量以单个_开头。 再看一个类继承的情况： 1234567891011121314151617class A(): def __init__(self, name): self.__name = \"a_name\"class B(): def __init__(self, name): self.__name = \"b_name\"class C(A, B): def __init__(self, name): A.__init__(self, name) B.__init__(self, name) self.__name = namec = C(\"c_name\")print(c.__dict__) # &#123;'_A__name': 'a_name', '_B__name': 'b_name', '_C__name': 'c_name'&#125;print(c.__name) # AttributeError: 'C' object has no attribute '__name' 上面代码中定义了两个类A和B，且类C多继承于A和B，A、B和C三个类都有一个同名的实例变量__name。由于在继承体系中可能存在同名的变量，因此需要加以区分：我们在代码中引用c.__name的时候会报错。注意观察可以发现，在类继承中，以__开头，至多一个_结尾的变量在子类中会被改写为_{class_name}__{variable_name}。在上例中，类A的__name在子类C中被改写为_A__name，类B的__name在子类C中被改写为_B__name，类C的__name在子类C中被改写为_C__name。这样做可以有效避免类继承的情况下同名变量无法被区分的情况。 总结一下： __x__ _x __x或者__x_ 含义 Python内建魔术方法或魔术变量 约定的私有变量命名规范 为了避免在继承中命名冲突而起的变量名，将被改写为_{class_name}__{variable_name}","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://nullcc.github.io/tags/Python/"}]},{"title":"Python中的魔术方法","slug":"Python中的魔术方法","date":"2017-06-11T16:00:00.000Z","updated":"2022-04-15T03:41:13.017Z","comments":true,"path":"2017/06/12/Python中的魔术方法/","link":"","permalink":"https://nullcc.github.io/2017/06/12/Python中的魔术方法/","excerpt":"本文将详细描述Python中的魔术方法，这些方法是Python中很有意思也很重要的一部分内容，掌握它们可以让你对Python的面向对象特性有更深的理解，也会让你的Python技能更上一层楼。","text":"本文将详细描述Python中的魔术方法，这些方法是Python中很有意思也很重要的一部分内容，掌握它们可以让你对Python的面向对象特性有更深的理解，也会让你的Python技能更上一层楼。 Python的魔术方法分类我们会从几个大类来讨论Python的魔术方法： 对象的构造和初始化 用于比较和运算符的魔术方法 打印对象的魔术方法 控制对象属性访问的魔术方法 可迭代对象和容器的魔术方法 反射的魔术方法 可调用的对象的魔术方法 会话管理的魔术方法 创建对象描述器的魔术方法 拷贝的魔术方法 对象的序列化和反序列化的魔术方法 1.对象的构造和初始化 方法名 含义 __new__(cls, [,…]) 创建新的类实例 __init__(self, [,…]) 初始化类实例 __del__(self) 在对象被垃圾回收时调用 来看下面这段代码： 1234567891011121314151617181920212223242526272829303132class ProgramLauguage(object): def __init__(self, name): print(\"in __init__ &#123;&#125;\".format(self)) self.name = name def __new__(cls, name, *args, **kwargs): print(\"in __new__ &#123;&#125;\".format(cls)) return object.__new__(cls, *args, **kwargs) def __del__(self): print(\"in __del__ &#123;&#125;\".format(self.name)) del selfd1 = dict()d2 = dict()d3 = dict()d1[\"ruby\"] = ProgramLauguage(\"Ruby\")d2[\"ruby\"] = d1[\"ruby\"]d3[\"ruby\"] = d1[\"ruby\"]print('del d1[\"ruby\"]')del d1[\"ruby\"]print('del d2[\"ruby\"]')del d2[\"ruby\"]print('del d3[\"ruby\"]')del d3[\"ruby\"]python = ProgramLauguage(\"Python\") 运行上面的代码，会打印： 123456789in __new__ &lt;class '__main__.ProgramLauguage'&gt;in __init__ &lt;__main__.ProgramLauguage object at 0x10daf3080&gt;del d1[\"ruby\"]del d2[\"ruby\"]del d3[\"ruby\"]in __del__ Rubyin __new__ &lt;class '__main__.ProgramLauguage'&gt;in __init__ &lt;__main__.ProgramLauguage object at 0x10daf3080&gt;in __del__ Python 从上面的输出可以看出，在实例化一个类时，将首先调用该类的__new__方法，在__new__方法中创建一个新对象，然后把实例化类时传入的参数原封不动地传递给__init__方法。在__init__方法内部，初始化这个实例。需要注意的是，__new__需要返回这个新创建的对象，而__init__不需要返回任何东西。因为__new__负责生成新实例，__init__负责初始化这个新实例。需要注意的是，如果__new__没有正确返回当前类cls的实例，那__init__是不会被调用的，即使是父类的实例也不行。 比较会让人迷惑的是__del__方法，乍一看会以为在调用del obj时调用对象的__del__，其实不是这样的，注意观察d1、d2和d3，这三个字典引用了同一个ProgramLauguage实例，在依次对这三个字典用引用的同一个实例调用del时，只有在del d3[&quot;ruby&quot;]之后才打印了in __del__ Ruby。这就说明并不是del obj这个操作触发了__del__，准确地说，__del__会在对象被垃圾回收的时候被调用。因此我们可以把一些对象内部的清理操作放在__del__中。 2.用于比较和运算符的魔术方法用于比较的魔术方法 方法名 含义 __cmp__(self, other) 定义了大于、小于和等于的方法 __eq__(self, other) 定义了等号的行为, == __ne__(self, other) 定义了不等号的行为, != __lt__(self, other) 定义了小于号的行为， &lt; __gt__(self, other) 定义了大于号的行为， &gt; __le__(self, other) 定义了小于等于号的行为， &lt;= __ge__(self, other) 定义了大于等于号的行为， &gt;= 来看下面这段代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Cat(object): def __init__(self, name, age, weight): self.name = name self.age = age self.weight = weight def __cmp__(self, other): if self.age &lt; s.age: return -1 elif self.age &gt; s.age: return 1 else: return 0 def __eq__(self, other): return self.name == other.name and self.age == other.age and self.weight == other.weight def __ne__(self, other): return self.name != other.name or self.age != other.age or self.weight != other.weight def __lt__(self, other): return self.age &lt; other.age def __gt__(self, other): return self.age &gt; other.age def __le__(self, other): return self.age &lt;= other.age def __ge__(self, other): return self.age &gt;= other.agecat1 = Cat('Mio', 1, 4)cat2 = Cat('Mio', 1, 4)cat3 = Cat('Lam', 2, 6)print(cat1 == cat2) # Trueprint(cat2 == cat3) # Falseprint(cat2 != cat3) # Trueprint(cat1 &lt; cat3) # Trueprint(cat1 &lt;= cat3) # Trueprint(cat3 &gt; cat1) # Trueprint(cat3 &gt;= cat1) # True 需要说明的是，__cmp__定义了大于、小于和等于的方法，当前对象大于、小于和等于另一个对象时，它的返回值分别大于0、小于0和等于0。其他几个用于比较的魔术方法的含义就如开头所述，很好理解。 用于运算符的魔术方法一元操作符和函数魔术方法 方法名 含义 __pos__(self) 实现+号的特性 __neg__(self) 实现-号的特性 __abs__(self) 实现内置 abs() 函数的特性 __invert__(self) 实现~符号的特性(取反) 看下面这段代码： 1234567891011121314151617181920212223242526class Person(object): def __init__(self, name, age): self.name = name self.age = age def __pos__(self): self.age += 1 def __neg__(self): self.age -= 1 def __abs__(self): return abs(self.age) def __invert__(self): return ~self.agep = Person('Jack', 20)print(p.age) # 20+pprint(p.age) # 21-pprint(p.age) # 20print(abs(p)) # 20print(~p) # -21 这段代码很简单，就不过多解释了。 普通算数操作符魔术方法 方法名 含义 __add__(self, other) 实现加法 __sub__(self, other) 实现减法 __mul__(self, other) 实现乘法 __floordiv__(self, other) 实现地板除法(//)，即整数除法 __div__(self, other) 实现/符号的除法，只在py2生效 __truediv__(self, other) 实现真除法，用于py3 __mod__(self, other) 实现取模运算 __divmod___(self, other) 实现内置的divmod()函数 __pow__(self, other) 实现**指数运算 __lshift__(self, other) 实现使用 &lt;&lt; 的按位左移位 __rshift__(self, other) 实现使用 &gt;&gt; 的按位右移位 __and__(self, other) 实现使用 &amp; 的按位与 __or__(self, other) 实现使用 \\ 的按位或 __xor__(self, other) 实现使用 ^ 的按位异或 我们来实现一个MyNumber类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111class MyNumber(object): def __init__(self, num): self.num = num def __add__(self, other): \"\"\" MyNumber(x) + MyNumber(y) \"\"\" return self.__class__(self.num + other.num) def __sub__(self, other): \"\"\" MyNumber(x) - MyNumber(y) \"\"\" return self.__class__(self.num - other.num) def __mul__(self, other): \"\"\" MyNumber(x) * MyNumber(y) \"\"\" return self.__class__(self.num * other.num) def __floordiv__(self, other): \"\"\" MyNumber(x) // MyNumber(y) \"\"\" return self.__class__(self.num // other.num) def __div__(self, other): \"\"\" [in py2] MyNumber(x) / MyNumber(y) \"\"\" return self.__class__(self.num / other.num) def __truediv__(self, other): \"\"\" [in py3] MyNumber(x) / MyNumber(y) \"\"\" return self.__class__(self.num / other.num) def __mod__(self, other): \"\"\" MyNumber(x) % MyNumber(y) \"\"\" return self.__class__(self.num % other.num) def __pow__(self, other): \"\"\" MyNumber(x) ** MyNumber(y) \"\"\" return self.__class__(self.num ** other.num) def __lshift__(self, other): \"\"\" MyNumber(x) &lt;&lt; MyNumber(y) \"\"\" return self.__class__(self.num &lt;&lt; other.num) def __rshift__(self, other): \"\"\" MyNumber(x) &gt;&gt; MyNumber(y) \"\"\" return self.__class__(self.num &gt;&gt; other.num) def __and__(self, other): \"\"\" MyNumber(x) &amp; MyNumber(y) \"\"\" return self.__class__(self.num &amp; other.num) def __or__(self, other): \"\"\" MyNumber(x) | MyNumber(y) \"\"\" return self.__class__(self.num | other.num) def __xor__(self, other): \"\"\" MyNumber(x) ^ MyNumber(y) \"\"\" return self.__class__(self.num ^ other.num)num1 = MyNumber(2)num2 = MyNumber(3)num3 = num1 + num2num4 = num2 - num1num5 = num1 * num2num7 = num2 // num1num8 = num2 / num1num9 = num2 % num1num10 = num2 ** num1num11 = num2 &lt;&lt; num1num12 = num2 &gt;&gt; num1num13 = num2 &amp; num1num14 = num2 | num1num15 = num2 ^ num1print(num3.num) # 5 (2+3)print(num4.num) # 1 (3-2)print(num5.num) # 6 (2*3)print(num7.num) # 1 (3//2)print(num8.num) # 1.5 (3/2)print(num9.num) # 1 (3%2)print(num10.num) # 9 (3**2)print(num11.num) # 12 (3&lt;&lt;2)print(num12.num) # 0 (3&gt;&gt;2)print(num13.num) # 2 (3&amp;2)print(num14.num) # 3 (3|2)print(num15.num) # 1 (3^2) 这部分代码也相对简单，就不解释了。 另外，普通算数操作符魔术方法均有相对应的反运算符魔术方法，即把两个操作数的位置对调，它们对应的反运算符魔术方法就是在方法名前__后加上r，比如__add__的反运算符魔术方法就是__radd__，其他的以此类推。 增量赋值魔术方法 方法名 含义 __iadd__(self, other) 实现赋值加法 += __isub__(self, other) 实现赋值减法 -= __mul__(self, other) 实现赋值乘法 *= __ifloordiv__(self, other) 实现赋值地板除法(//=)，即整数除法 __idiv__(self, other) 实现/符号的赋值除法 /= __itruediv__(self, other) 实现赋值真除法，需要from future import division __imod__(self, other) 实现赋值取模运算 %= __pow__(self, other) 实现指数赋值运算 **= __ilshift__(self, other) 实现使用 &lt;&lt;= 的赋值按位左移位 __irshift__(self, other) 实现使用 &gt;&gt;= 的赋值按位右移位 __iand__(self, other) 实现使用 &amp;= 的赋值按位与赋值 __ior__(self, other) 实现使用 \\ = 的赋值按位或 __ixor__(self, other) 实现使用 ^= 的赋值按位异或 这部分魔术方法的示例代码和上面的差不多，只是把相应的运算改成赋值运算而已，代码略。 类型转换魔术方法 方法名 含义 __int__(self) 实现整形的强制转换 __long__self) 实现长整形的强制转换，long在py3中和int整合了 __float__(self) 实现浮点型的强制转换 __complex__(self) 实现复数的强制转换 __bin__(self) 实现二进制数的强制转换 __oct__(self) 实现八进制的强制转换 __hex__(self) 实现十六进制的强制转换 __index__(self) 当对象是被应用在切片表达式中时，实现整形强制转换 __trunc__(self) 当使用 math.trunc(self) 的时候被调用，整数截断 __coerce__(self, other) 实现混合模式算数，只在py2有效 3.打印对象的魔术方法 方法名 含义 __str__(self) 定义当 str() 调用的时候的返回值(人类可读) __repr__(self) 定义当 repr() 被调用的时候的返回值(机器可读) __unicode__self) 定义当 unicode() 调用的时候的返回值，只在py2中有效 __hash__(self) 定义当 hash() 调用的时候的返回值，它返回一个整形 __nonzero__(self) 定义当 bool() 调用的时候的返回值 示例代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class MyNumber(object): def __init__(self, num): self.num = num def __str__(self): \"\"\" str(MyNumber(x)) \"\"\" return str(self.num) def __repr__(self): \"\"\" repr(MyNumber(x)) \"\"\" return \"&lt;&#123;&#125; &#123;&#125;&gt;\".format(__class__, str(self.num)) def __unicode__(self): \"\"\" [only in py2] unicode(MyNumber(x)) \"\"\" return str(self.num) def __hash__(self): \"\"\" hash(MyNumber(x)) \"\"\" return hash(self.num) def __nonzero__(self): \"\"\" [only in py2] nonzero(MyNumber(x)) \"\"\" return bool(self.num) def __bool__(self): \"\"\" [only in py3] bool(MyNumber(x)) \"\"\" return bool(self.num)num1 = MyNumber(123)num2 = MyNumber(0)print(str(num1)) # 123print(repr(num1)) # &lt;&lt;class '__main__.MyNumber'&gt; 123&gt;print(hash(num1)) # 123print(bool(num1)) # Trueprint(bool(num2)) # False 4.控制对象属性访问的魔术方法 方法名 含义 __getattr__(self, name) 当用户试图访问一个根本不存在（或者暂时不存在）的属性时，你可以通过这个魔法方法来定义类的行为 __setattr__(self, name, value) 定义当试图对一个对象的属性赋值时的行为 __delattr__(self, name) 定义当试图删除一个对象的属性时的行为 __getattribute__(self, name) getattribute 允许你自定义属性被访问时的行为，只能用于新式类，而且很容易引起无限递归调用，可以用过使用父类的getattribute避免，建议不要使用 示例代码如下： 123456789101112131415161718192021222324class Person(object): def __init__(self, name): self.name = name def __getattr__(self, name): print('in __getattr__') return None def __setattr__(self, name, value): print('in __setattr__') self.__dict__[name] = value def __delattr__(self, name): print('in __delattr__') self.__dict__[name] = Nonep = Person(\"Jack\") # in __setattr__print(p.name) # Jackprint(p.no_exit_attr) # in __getattr__, Nonep.name = \"Smith\" # in __setattr__print(p.name) # Smithdel p.name # in __delattr__print(p.name) # None 5.可迭代对象和容器的魔术方法 方法名 含义 __len__(self) 定义调用len()函数时的行为 __getitem__(self, key) 定义获取容器内容时的行为 __setitem__(self, key, value) 定义设置容器内容时的行为 __delitem__(self, key) 定义删除容器内容时的行为 __iter__(self) 定义迭代容器内容时的行为 __contains__(self, item) 定义对容器使用in时的行为 __reversed__(self) 定义对容器使用reversed()时的行为 示例代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class MyDictIterator: def __init__(self, n): self.i = 0 self.n = n def __iter__(self): return self def next(self): if self.i &lt; self.n: i = self.i self.i += 1 return i else: raise StopIteration()class MyList(object): def __init__(self, list=[]): self.list = list def __len__(self): return len(self.list) def __getitem__(self, key): print('in __getitem__') return self.list[key] def __setitem__(self, key, value): print('in __setitem__') self.list[key] = value def __delitem__(self, key): print('in __delitem__') del self.list[key] def __iter__(self): print('in __iter__') return iter(self.list) def __contains__(self, item): print('in __contains__') return item in self.list def __reversed__(self): print('in __reversed__') return reversed(self.list)list1 = MyList([\"foo\", \"bar\", \"baz\"])print(len(list1)) # 3print(list1[0]) # in __getitem__ foolist1[0] = 'FOO' # in __setitem__print(list1[0]) # in __getitem__ FOOdel list1[0] # in __delitem__print(list1[0]) # in __getitem__ barfor w in list1: print(w) # in __iter__ bar bazprint(\"bar\" in list1) # in __contains__ Trueprint(\"BAR\" in list1) # in __contains__ Falseprint(reversed(list1)) # in __reversed__ &lt;list_reverseiterator object at 0x110005128&gt; 6.反射 方法名 含义 __instancecheck__(self, instance) 检查一个实例是否是你定义的类的一个实例（例如 isinstance(instance, class) ） __subclasscheck__(self, subclass) 检查一个类是否是你定义的类的子类（例如 issubclass(subclass, class) ） 7.可调用的对象的魔术方法 方法名 含义 __call__(self, [args…] 使对象可以像函数一样被调用 1234567891011121314class Point(object): def __init__(self, x, y): self.x = x self.y = y def __call__(self, x, y): self.x = x self.y = yp = Point(1, 0)print(\"(&#123;&#125;, &#123;&#125;)\".format(p.x, p.y)) # (1, 0)p(2, 1)print(\"(&#123;&#125;, &#123;&#125;)\".format(p.x, p.y)) # (2, 1) 定义了__call__方法的类的实例可以像函数一样被调用。 8.会话管理的魔术方法 方法名 含义 __enter__(self) 定义了当会话开始的时候初始化对象的行为 __exit__(self, exception_type, exception_val, trace) 定义了当会话结束时的行为 Python可以通过with来开启一个会话控制器，会话控制器通过两个魔术方法来定义：__enter__(self)和__exit__(self, exception_type, exception_val, trace)。__enter__定义了当会话开始的时候初始化对象的行为，它的返回值会被with语句的目标或as后面的名字绑定。__exit__定义了当会话结束时的行为，它一般做一些清理工作，比如关键文件等。如果with代码块执行成功，__exit__的exception_type、exception_val和trace三个参数都会是None，如果执行失败，你可以在会话管理器内处理这个异常或将异常交由用户处理。如果要在会话管理器内处理异常，__exit__最后要返回True。 来看一个例子： 1234567891011121314151617class FileObject(object): def __init__(self, file): self.file = file def __enter__(self): return self.file def __exit__(self, exception_type, exception_val, trace): try: self.file.close() except: print('File close failed!') return Truewith FileObject(open('./test.py')) as file: print(file) # &lt;_io.TextIOWrapper name='./test.py' mode='r' encoding='UTF-8'&gt; 通过使用会话管理器，我们可以包装对象的打开和关闭操作，减少忘记关闭资源这种误操作。 9.创建对象描述器的魔术方法 方法名 含义 __get__(self, instance, owner) __set__(self, instance, value) __delete__(self, instance) 拷贝的魔术方法 方法名 含义 __copy__(self) __deepcopy__(self, memodict=) 11. 对象的序列化和反序列化的魔术方法 方法名 含义 __getinitargs__(self) __getnewargs__(self) __getstate__(self) __setstate__(self, state)","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://nullcc.github.io/tags/Python/"}]},{"title":"Python包导入详解","slug":"Python包导入详解","date":"2017-06-08T16:00:00.000Z","updated":"2022-04-15T03:41:13.017Z","comments":true,"path":"2017/06/09/Python包导入详解/","link":"","permalink":"https://nullcc.github.io/2017/06/09/Python包导入详解/","excerpt":"在Python导入包有以下四种情况： 主程序导入系统内置模块或已安装的依赖模块。 主程序和模块程序在同一目录下。 主程序所在目录是模块所在目录的上级目录。 主程序导入上级目录中的模块或其他目录(与主程序所在目录平级)下的模块。","text":"在Python导入包有以下四种情况： 主程序导入系统内置模块或已安装的依赖模块。 主程序和模块程序在同一目录下。 主程序所在目录是模块所在目录的上级目录。 主程序导入上级目录中的模块或其他目录(与主程序所在目录平级)下的模块。 下面依次来看看。 主程序导入系统内置模块或已安装的依赖模块1import json 这种写法是直接导入包名，使用该模块下的函数时都必须带上json前缀，比如json.loads()` 1from datetime import datetime 这种写法是导入包中具体的某个模块，这里导入的是datetime包中的datetime模块，可以直接使用这个datetime模块，比如datetime.now() 1import os.path 这种写法是导入os包中的path模块，使用时必须以os.path为前缀调用该模块下的函数，比如os.path.exists(a_file_path)检查某个文件是否存在 主程序和模块程序在同一目录下文件目录结构： 123--src |--a.py |--main.py 在main.py中导入a.py中的模块： 1from a import A 主程序所在目录是模块所在目录的上级目录文件目录结构： 12345--src |--a |--__init__.py |--a.py |--main.py 在main.py中导入a.py中的模块： 1from a.a import A 主程序导入上级目录中的模块或其他目录(与主程序所在目录平级)下的模块文件目录结构： 1234567--src |--a.py |--b |--__init__.py |--b.py |--sub |--main.py 在sub/main.py中导入a.py和b.py中的模块： 12345import osimport syssys.path.append(os.path.abspath(os.path.dirname(__file__) + '/' + '..'))from a import Afrom b.b import B 这种情况下需要在sys.path中添加父目录，才能让Python找到具体模块的路径。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://nullcc.github.io/tags/Python/"}]},{"title":"ElasticSearch基本原理","slug":"elasticsearch基本原理","date":"2017-05-17T16:00:00.000Z","updated":"2022-04-15T03:41:13.024Z","comments":true,"path":"2017/05/18/elasticsearch基本原理/","link":"","permalink":"https://nullcc.github.io/2017/05/18/elasticsearch基本原理/","excerpt":"本部分简要介绍一下ElasticSearch的基本原理，不会太深入，仅仅是介绍最基础的一些概念。","text":"本部分简要介绍一下ElasticSearch的基本原理，不会太深入，仅仅是介绍最基础的一些概念。 1.基于Apache Lucene2.使用REST API3.输入数据分析分析(analysis)是这样一个过程： 首先，表征化一个文本块为适用于倒排索引单独的词(term)然后标准化这些词为标准形式，提高它们的“可搜索性”或“查全率” 对输入数据的分析是比较复杂的，由分析器完成。分析器(analyzer)的组成： 1.零个或多个字符过滤器(character filter) 这是分词之前的操作，使用字符过滤器可以过滤掉HTML字符，并映射一些字符(比如&apos;&amp;&apos;-&gt;&apos;and) 2.一个分词器(tokenizer) 对文本进行分词，把完整文本断成一个一个的词(英文或一些西方语言可以利用空格断词，中文或一些东方系语言需要使用词库断词) 3.零个或多个标记过滤器(token filter)，又被称为表征过滤器 标记过滤器可以把分词后的单词标准化，比如lowercase过滤器可以把所有单词都转换成小写(例如Cat-&gt;cat)，stemmer过滤器则会把单词转换为它的词根或基本形式(例如cats-&gt;cat)。 ElasticSearch内置的一些analyzer： analyzer logical name description standard analyzer standard standard tokenizer, standard filter, lower case filter, stop filter simple analyzer simple lower case tokenizer stop analyzer stop lower case tokenizer, stop filter keyword analyzer keyword 不分词，内容整体作为一个token(not_analyzed) pattern analyzer whitespace 正则表达式分词，默认匹配\\W+ language analyzers lang 各种语言 snowball analyzer snowball standard tokenizer, standard filter, lower case filter, stop filter, snowball filter custom analyzer custom 一个Tokenizer, 零个或多个Token Filter, 零个或多个Char Filter ElasticSearch内置的tokenizer列表： token filter logical name description standard filter standard ascii folding filter asciifolding length filter length 去掉太长或者太短的 lowercase filter lowercase 转成小写 ngram filter nGram edge ngram filter edgeNGram porter stem filter porterStem 波特词干算法 shingle filter shingle 定义分隔符的正则表达式 stop filter stop 移除 stop words word delimiter filter word_delimiter 将一个单词再拆成子分词 stemmer token filter stemmer stemmer override filter stemmer_override keyword marker filter keyword_marker keyword repeat filter keyword_repeat kstem filter kstem snowball filter snowball phonetic filter phonetic 插件 synonym filter synonyms 处理同义词 compound word filter dictionary_decompounder, hyphenation_decompounder 分解复合词 reverse filter reverse 反转字符串 elision filter elision 去掉缩略语 truncate filter truncate 截断字符串 unique filter unique pattern capture filter pattern_capture pattern replace filte pattern_replace 用正则表达式替换 trim filter trim 去掉空格 limit token count filter limit 限制token数量 hunspell filter hunspell 拼写检查 common grams filter common_grams normalization filter arabic_normalization, persian_normalization ES内置的character filter列表： character filter logical name description mapping char filter mapping 根据配置的映射关系替换字符 html strip char filter html_strip 去掉HTML元素 pattern replace char filter pattern_replace 用正则表达式处理字符串 对具体字段的查询 当你查询全文(full text)字段，查询将使用相同的分析器来分析查询字符串，以产生正确的词列表。 当你查询一个确切值(exact value)字段，查询将不分析查询字符串，但是你可以自己指定。 4.评分和查询相关性默认情况下，Apache Lucene使用TF/IDF(term frequency/inverse document frequency，词频/逆向文档频率)评分机制，这是一种计算文档在我们查询上下文中相关度的算法，也可以使用其他算法。 5.数据架构的主要概念(1)索引 索引(index)是ElasticSearch对逻辑数据的逻辑存储，可以把它认为是关系型数据库的表。ElasticSearch可以把索引放在一台机器上或者分散放在多台机器上，每个索引有一个或多个分片(shard)，每个分片可以有多个副本(replica)。 (2)文档 存储在ElasticSearch中的主要实体是文档(document)，它相当于关系型数据库的行。ElasticSearch和MongoDB不同的是，MongoDB中相同字段类型可以不同，但ElasticSearch中相同字段必须是类型相同的。 文档包含多个字段。从客户端的角度看，文档是一个JSON对象。每个文档存储在一个索引中并由一个ElasticSearch自动生成的唯一标识符和文档类型。 索引、文档类型和文档ID唯一确定一个文档。 (3)文档类型 在ElasticSearch中，一个索引可以存放很多不同用途的文档，可以用文档类型加以区分。 但是需要记住一点，同一索引的所有文档类型中，同一字段名只能有一种类型。 (4)映射 映射制定了ElasticSearch应该如何处理相应的字段。 ElasticSearch在映射中存储有关字段的信息，每一个文档类型都有自己的映射，即使我们没有手动指定(当然也可以手动指定)。 例如年龄字段和内容字段就需要不同的处理，前者不需要做分析，后者需要。 6.ElasticSearch的主要概念(1)节点和集群 ElasticSearch可以运行在单机上，不过为了处理大规模的数据和保证容错和高可用性，ElasticSearch被设计成分布式的，可以用多台机器组成ElasticSearch集群(cluster)，每台机器称为一个节点(node)。 (2)分片 当有大量数据需要处理时，单机的处理能力就不够了，此时可以把一个索引拆分成几个分片，分别放在不同的机器上。当搜索请求到来时，ElasticSearch会把查询发送到相关分片上，并将结果合并到一起回送客户端，然而客户端并不知道这些事情。而且分片可以加快索引速度。 (3)副本 为了提高吞吐量和保证高可用性，一个分片可以有零个或多个复制分片，称为副本。这些相同的分片中会有一个作为主分片来响应请求，其余副本保证了当主分片或主分片所在机器挂掉时晋升为主分片，保证可用性。 (4)节点间的同步 每个节点都会在本地保存信息，并会自动同步。 7.Elasticsearch 分片交互过程(1)Elasticsearch如何把数据存储到分片中 这里有一个问题：当我们存储数据时，数据应该存放在哪一个分片中(主分片还是复制分片)？当我们取数据时，应当从哪个分片去取？ 数据存储到分片中使用以下规则： shard = hash(routing) % number_of_primary_shards 这里，routing是一个字符串，一般是文档的_id值，也可以是用户自定义的值。使用hash函数计算出routing的散列值，再对主分片数取模运算，结果就是我们想要的那个分片，这个值范围是0-number_of_primary_shards - 1。 这样做也有个问题，就是在索引建立以后，主分片数不能更改。否则会有一部分数据无法被索引到。 (2)主分片和复制分片之间如何交互 这里有3个节点，2个主分片，每个主分片分别对应2个复制分片，node1为主节点： 1、索引与删除一个文档 2、更新一个文档 3、检索文档 检索的过程将分为查询阶段与获取阶段。 检索文档-查询语句： 检索文档-查询阶段： 查询阶段主要定位了所要检索数据的具体位置，但是我们还必须取回它们才能完成整个检索过程。 检索文档-获取阶段：","categories":[{"name":"web后端","slug":"web后端","permalink":"https://nullcc.github.io/categories/web后端/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://nullcc.github.io/tags/ElasticSearch/"}]},{"title":"使用ElasticSearch搭建高性能可扩展的全文搜索引擎","slug":"使用ElasticSearch搭建高性能可扩展的全文搜索引擎","date":"2017-05-17T16:00:00.000Z","updated":"2022-04-15T03:41:13.028Z","comments":true,"path":"2017/05/18/使用ElasticSearch搭建高性能可扩展的全文搜索引擎/","link":"","permalink":"https://nullcc.github.io/2017/05/18/使用ElasticSearch搭建高性能可扩展的全文搜索引擎/","excerpt":"准备环境 下载 ElasticSearch 最新版本下载: https://www.elastic.co/downloads/elasticsearch指定版本下载: https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-[版本号].zip","text":"准备环境 下载 ElasticSearch 最新版本下载: https://www.elastic.co/downloads/elasticsearch指定版本下载: https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-[版本号].zip 安装1. 安装ElasticSearch 首先下载ElasticSearch(请先确认你已经安装了JDK) cd到所在文件夹后运行以下命令 123unzip elasticsearch-1.2.2.zipmv elasticsearch-1.2.2 /usr/local/ln -s /usr/local/elasticsearch-1.2.2 /usr/local/elasticsearch 安装到自启动项我们还需要设置ElasticSearch自启动，成为daemon常驻后台，这里需要用到elasticsearch-servicewrapper，unzip后把service文件夹copy到 /usr/local/elasticsearch/bin下，运行以下命令可以安装ElasticSearch： 1sudo /usr/local/elasticsearch/bin/service/elasticsearch install 启动： 1sudo /usr/local/elasticsearch/bin/service/elasticsearch start 如果启动失败，可能是内存设置有问题，打开bin/service/elasticsearch.conf文件，设置Elasticsearch能够分配的JVM内存大小。一般情况下，设置成总内存的50%比较好： 1set.default.ES_HEAP_SIZE=512 如果要限制ES_MIN_MEM和ES_MAX_MEM，建议设置成一样大，避免出现频繁的内存分配。 修改节点名称有时候需要指定一个明确的节点名称，如果不指定，ElasticSearch会随机为我们生成一个节点名，每次启动都不同。要手动指定节点名，需要打开/usr/local/elasticsearch/config/elasticsearch.yml文件，修改(比如要指定该节点名称为meiqu)： 1node.name: \"meiqu\" 保存后，重启Elasticsearch就行了。 2. 安装插件123head: /usr/local/elasticsearch/bin/plugin -install mobz/elasticsearch-headmarvel: /usr/local/elasticsearch/bin/plugin -i elasticsearch/marvel/latestmongodb插件: /usr/local/elasticsearch/bin/plugin --install com.github.richardwilly98.elasticsearch/elasticsearch-river-mongodb/2.0.1 3. 配置分词插件ElasticSearch默认采用standard分词，默认的分词对中文来说效果不好，只是简单粗暴地把所有中文字拆分出来，并没有根据词的语意来分词(没有使用中文词库)，因此在真正查询的时候，准确率不高，我们可以使用ik分词插件或者其他插件，这里以ik分词来做示例。 1.从 https://github.com/medcl/elasticsearch-rtf/tree/master/plugins/analysis-ik 下载 elasticsearch-analysis-ik-1.2.6.jar，放到/usr/local/elasticsearch/plugins/analysis-ik文件下(没有就新建一个)。 2.从 https://github.com/medcl/elasticsearch-analysis-ik 下载ik分词插件，unzip后把config目录下的ik目录放到/usr/local/elasticsearch/config文件夹下。 3.打开/usr/local/elasticsearch/config/elasticsearch.yml，在最后加上： 1234567891011121314151617181920index: analysis: tokenizer: my_tokenizer: type: ik use_smart: false analyzer: ik: alias: [ik_analyzer] type: org.elasticsearch.index.analysis.IkAnalyzerProvider ik_max_word: type: ik use_smart: false ik_smart: type: ik use_smart: true my_analyzer: type: custom tokenizer: my_tokenizer filter: [lowercase, stemmer] 配置完需要重启ElasticSearch。 4.运行在console里运行： 1sudo /usr/local/elasticsearch/bin/elasticsearch start 在后台运行： 1sudo /usr/local/elasticsearch/bin/service/elasticsearch start 5.控制台1234marvel: http://localhost:9200/_plugin/marvelsense: http://localhost:9200/_plugin/marvel/sense/index.htmlhead: http://localhost:9200/_plugin/head/river-mongodb http://localhost:9200/_plugin/river-mongodb/ PS: 以下的示例全部使用marvel的sense来运行。 经过以上的配置，我们的ElasticSearch的默认分词算法已经变成ik了。 我们可以先来测试一下我们配置的ik分词效果如何，作为对比，会先运行默认分词的例子。 首先新建一个索引： 1curl -XPUT http://localhost:9200/index standard分词： 1234GET /meiqu/_analyze?analyzer=standard&amp;pretty=true&#123; \"text\":\"PHP是全世界最好的编程语言\"&#125; 结果有点蛋疼，默认分词是直接切分了每个汉字，结果如下： ik分词： 1234GET /meiqu/_analyze?analyzer=ik&amp;pretty=true&#123; \"text\":\"PHP是全世界最好的编程语言\"&#125; ik分词比较合理地做到了根据词语的意思来分词，效果还不错（在分词中，一些助动词经常被省略，比如’是’）： 利用成熟的分词插件，可以让我们的全文索引功能事倍功半。 自定义分词词典先制作自己的词典，然后修改文件 /usr/local/elasticsearch/config/ik/IKAnalyzer.cfg.xml 中的词典配置项就行。 6.同步mongodb到ElasticSearch 注意: 请先确保拥有至少一个mongoDB的副本集合，以下是MongoDB River Plugin、ElasticSearch和MongoDB的版本搭配列表 MongoDB River Plugin ElasticSearch MongoDB TokuMX master 1.4.2 3.0.0 1.5.1 2.0.9 1.4.2 3.0.0 1.5.1 2.0.5 1.4.2 2.6.6 1.5.1 2.0.2 1.3.5 2.6.5 1.5.1 2.0.1 1.2.2 2.4.9 -&gt; 2.6.3 1.5.0 2.0.0 1.0.0 -&gt; 1.1.1 2.4.9 1.7.4 0.90.10 2.4.8 1.7.3 0.90.7 2.4.8 1.7.2 0.90.5 2.4.8 1.7.1 0.90.5 2.4.6 1.7.0 0.90.3 2.4.5 1.6.11 0.90.2 2.4.5 1.6.9 0.90.1 2.4.4 1.6.8 0.90.0 2.4.3 1.6.7 0.90.0 2.4.3 1.6.6 0.90.0 2.4.3 为ElasticSearch创建和mongoDB对应的index和type: 12345678910111213PUT /_river/mongodb/_meta &#123; \"type\": \"mongodb\", \"mongodb\": &#123; \"db\": \"DATABASE_NAME\", \"collection\": \"COLLECTION\", \"gridfs\": true &#125;, \"index\": &#123; \"name\": \"ES_INDEX_NAME\", \"type\": \"ES_TYPE_NAME\" &#125; &#125; 同步食物表、运动表、贴士表和用户表在真正开始同步之前我们先来做一些准备工作，这些工作很重要，直接影响到我们搜索匹配的精确度和排序，就是指定mapping： 为索引创建别名有时我们需要更改索引中的映射，这就需要重建索引，为了做到无缝切换索引和零停机时间，可以使用别名机制。先为上述的索引创建别名，这个别名就叫做meiqu，真正的索引名可以为meiqu_v1、meiqu_v2之类的，然后在查询时，只需要使用别名meiqu就行了，这样客户端代码不需要修改。 我们创建了一个索引meiqu_v1，并创建了一个别名指向它： 创建一个索引和别名： 1PUT /meiqu_v1 只所以在索引名后加一个版本号是由于以后可能会重建索引，为了保证生产环境在重建索引时的平滑过渡，需要有一个别名机制。这里先新建一个索引，创建别名需要在重建索引完毕后再进行。 为foods表指定mapping： 123456789101112131415161718PUT /meiqu_v1/foods/_mapping&#123; \"foods\": &#123; \"properties\": &#123; \"name\": &#123; \"type\" : \"string\", \"analyzer\" : \"my_analyzer\" &#125;, \"nutrientInfoArr\": &#123; \"properties\": &#123; \"content\": &#123; \"type\": \"string\" &#125; &#125; &#125; &#125; &#125;&#125; 为activities表指定mapping： 1234567891011PUT /meiqu_v1/activities/_mapping&#123; \"activities\": &#123; \"properties\": &#123; \"name\": &#123; \"type\" : \"string\", \"analyzer\" : \"my_analyzer\" &#125; &#125; &#125;&#125; 为tips表指定mapping： 12345678910111213141516171819PUT /meiqu_v1/tips/_mapping&#123; \"tips\": &#123; \"properties\": &#123; \"title\": &#123; \"type\" : \"string\", \"analyzer\" : \"my_analyzer\" &#125;, \"summary\":&#123; \"type\" : \"string\", \"analyzer\" : \"my_analyzer\" &#125;, \"content\":&#123; \"type\" : \"string\", \"analyzer\" : \"my_analyzer\" &#125; &#125; &#125;&#125; 为user表指定mapping： 123456789101112131415PUT /meiqu_v1/users/_mapping&#123; \"users\": &#123; \"properties\": &#123; \"profile\": &#123; \"properties\": &#123; \"nickname\": &#123; \"type\" : \"string\", \"analyzer\" : \"my_analyzer\" &#125; &#125; &#125; &#125; &#125;&#125; 指定mapping可以选择我们需要的字段来做分词和建立倒排索引，因为分词和建立倒排索引是需要消耗很多性能的，例如_id、url之类的字段我们没必要为他们做这些，所以我们需要指定哪些重要的字段需要分词。 食物表 1234567891011121314PUT /_river/mongodb_foods/_meta&#123; \"type\": \"mongodb\", \"mongodb\": &#123; \"host\": \"192.168.1.119\", \"port\": \"27017\", \"db\": \"meiqu618_20150211\", \"collection\": \"foods\" &#125;, \"index\": &#123; \"name\": \"meiqu_v1\", \"type\": \"foods\" &#125; &#125; 运动表 1234567891011121314PUT /_river/mongodb_activities/_meta&#123; \"type\": \"mongodb\", \"mongodb\": &#123; \"host\": \"192.168.1.119\", \"port\": \"27017\", \"db\": \"meiqu618_20150211\", \"collection\": \"activities\" &#125;, \"index\": &#123; \"name\": \"meiqu_v1\", \"type\": \"activities\" &#125; &#125; 贴士表 1234567891011121314PUT /_river/mongodb_tips/_meta&#123;\"type\": \"mongodb\",\"mongodb\": &#123; \"host\": \"192.168.1.119\", \"port\": \"27017\", \"db\": \"meiqu618_20150211\", \"collection\": \"tips\"&#125;,\"index\": &#123; \"name\": \"meiqu_v1\", \"type\": \"tips\" &#125;&#125; 用户表 1234567891011121314PUT /_river/mongodb_users/_meta&#123; \"type\": \"mongodb\", \"mongodb\": &#123; \"host\": \"192.168.1.119\", \"port\": \"27017\", \"db\": \"meiqu618_20150211\", \"collection\": \"users\" &#125;, \"index\": &#123; \"name\": \"meiqu_v1\", \"type\": \"users\" &#125;&#125; 数据同步完毕后，创建索引别名： 1PUT /meiqu_v1/_alias/meiqu 这个别名很重要，我们需要使用这个别名来指向当前的索引，代码中也是使用这个别名来进行查询。 简单查询 使用以下查询，能够查询出食物名称中包含”番茄”的所有食物: 123456789GET /meiqu/foods/_search&#123; \"query\": &#123; \"match\": &#123; \"name\": \"番茄\" &#125; &#125;, \"_source\": [\"_id\", \"name\", \"calory\", \"description\", \"units\", \"userName\"]&#125; 使用以下查询，能够查询出运动名称中包含”跑”的所有运动: 123456789GET /meiqu/activities/_search&#123; \"query\": &#123; \"match\": &#123; \"name\": \"跑\" &#125; &#125;, \"_source\": [\"_id\", \"name\", \"mets\", \"description\"]&#125; 使用以下查询，能够查询出贴士名称中包含”白领”的所有贴士: 12345678910111213141516171819202122232425262728293031GET /meiqu/tips/_search&#123; \"query\": &#123; \"bool\": &#123; \"must\": [ &#123; \"term\": &#123; \"status\": 1 &#125; &#125;, &#123; \"range\": &#123; \"effDate\": &#123; \"lt\": new Date().getTime() &#125; &#125; &#125;, &#123; \"dis_max\": &#123; \"queries\": [ &#123; \"match\": &#123; \"title\":\"白领\" &#125;&#125;, &#123; \"match\": &#123; \"tags\":\"白领\" &#125;&#125; ], \"tie_breaker\": 0.3 &#125; &#125; ] &#125; &#125;, \"_source\": [\"_id\", \"title\", \"cover\", \"summary\", \"tags\", \"effDate\"]&#125; 使用以下查询，能够查询出用户昵称中包含”test”的所有用户: 12345678910111213141516GET /meiqu/users/_search&#123; \"query\": &#123; \"filtered\": &#123; \"query\": &#123; \"match\": &#123; \"profile.nickname\": \"test\" &#125; &#125;, \"filter\": &#123; \"term\": &#123; \"status\": 1 &#125; &#125; &#125; &#125;, \"_source\": [\"_id\", \"diaries\", \"fans\", \"idols\", \"profile.nickname\", \"profile.icon\"]&#125; 在开发环境中使用ElasticSearch 直接cd到项目根目录执行： 1npm install elasticsearch 搞定。 nodejs下一个搜索用户的例子： 123456789101112131415161718192021222324252627282930313233343536373839//搜索用户var searchUser = function (req, res, next) &#123; var skip = parseInt(req.query.offset); skip = isNaN(skip) ? 0 : skip; var limit = parseInt(req.query.size); limit = isNaN(limit) ? 10 : limit; var nickname = req.query.nickname; var query = &#123; \"index\": \"meiqu\", \"type\": \"users\", \"from\": skip, \"size\": limit, \"body\": &#123; \"query\": &#123; \"filtered\": &#123; \"query\": &#123; \"match\": &#123; \"profile.nickname\": nickname &#125;&#125;, \"filter\": &#123; \"term\": &#123; \"status\": 1 &#125;&#125; &#125; &#125; &#125;, \"_source\": ['_id', 'diaries', 'fans', 'idols', 'profile.nickname', 'profile.icon'] &#125;; client.search(query, function (error, response) &#123; if (error)&#123; return next(error); &#125; var users = []; response.hits.hits.forEach(function(data)&#123; users.push(data._source); &#125;); req.users = users; return next(); &#125;);&#125;;","categories":[{"name":"web后端","slug":"web后端","permalink":"https://nullcc.github.io/categories/web后端/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://nullcc.github.io/tags/ElasticSearch/"}]},{"title":"倒排索引基础知识","slug":"倒排索引基础知识","date":"2017-05-17T16:00:00.000Z","updated":"2022-04-15T03:41:13.029Z","comments":true,"path":"2017/05/18/倒排索引基础知识/","link":"","permalink":"https://nullcc.github.io/2017/05/18/倒排索引基础知识/","excerpt":"ElasticSearch的全文搜索是基于Apache Lucene的，Apache Lucene是一个全文搜索库，其核心处理步骤一般是：","text":"ElasticSearch的全文搜索是基于Apache Lucene的，Apache Lucene是一个全文搜索库，其核心处理步骤一般是： 获得一个文档，对需要进行全文搜索的字段进行处理(分词、标记过滤、字符映射)。 对最终获得的标记进行词频分析和建立倒排索引，并入库。 当有全文索引请求到来时，查询倒排索引，获得相关文档并评分返回。 本篇重点介绍倒排索引的概念。 1.单词-文档矩阵单词-文档矩阵是一种表达两者之间包含关系的模型，这种模型非常适合于全文索引的场景。下图中行表示词，列表示文档，打钩表示某文档包含某词： 一般来讲，我们可以从两个角度来看这个矩阵： 横向角度：表示有哪些文档包含了某词。 纵向角度：表示某个文档包含了哪些词。 搜索引擎中建立的索引可说就是单词-文档矩阵的某种模型，现在应用得比较多的就是倒排索引了。 2.倒排索引的基本概念 文档(Document)：这里文档的概念比较宽泛，所有能被存储的文本信息都可被称为文档。例如网页、XML、Word文件、一封Email、一条微博等等。 文档集合(Document Collection)：由若干文档构成的集合就叫文档集合。 文档编号(Document ID)：搜索引擎会为文档集合中的每个文档赋予一个唯一的内部编号(在它所处的文档集合中是唯一的)，以方便处理。 倒排索引(Inverted Index)：是一种实现单词-文档矩阵的具体方式，通过倒排索引可以通过单词信息快速获取到包含这个单词的所有文档列表。倒排索引主要由两个部分组成：“单词词典”和“倒排文件”。 单词词典(Lexicon)：搜索引擎索引单位一般是单词(或多个单词组合)，单词词典是由文档集合中出现过的所有单词构成的一个字符串集合，单词词典内每条索引项记载着单词本身的信息以及指向倒排列表的指针。 倒排列表(Posting List)：倒排列表记录了出现过某个单词的所有文档的文档列表和改单词在这些文档中出现的位置信息，每条记录称为一个“倒排项”，根据倒排列表，就可以获知有哪些文档包含了某个单词。 倒排文件(Inverted File)：所有单词的倒排列表一般顺序存储在磁盘的某个文件中，这就是倒排文件，倒排文件是存储倒排索引的物理文件。 示例如下图： 3.倒排索引实例从理论上来解释倒排索引还是比较清晰的，不过还是再来看一个真是的示例。 上图包含5个文档，我们要对这5个文档建立倒排索引。 首先使用分词器、标记过滤器、字符映射器处理文本，得到一个个单词，然后建立如下倒排索引： 更近一步，我们还可以记录每个单词在包含它的文档中出现的次数(词频)，词频对于在搜索排序中计算和查询文档相似度是一个重要的因子，事先将其放在倒排索引中可以方便后续的相关计算。 再进一步，还可以记录文档频率信息(文档集合中有多少个文档包含某个单词)和目标单词在文档中出现的位置信息等。 基本上大部分搜索引擎的倒排索引就是这个结构，只是实现方式有所不同罢了。 一个基本的流程如下： 用户输入某个单词进行搜索。 搜索引擎查询事先准备好的倒排索引，找到对应的倒排文件，获得所有包含查询单词的文档列表。 获得文档列表后，根据单词频率信息、文档频率信息计算相似性并对这些文档排序。 输出给用户。 4.单词词典的实现比较常见的实现有： 哈希链表 B树/B+树 哈希链表： B+树","categories":[{"name":"web后端","slug":"web后端","permalink":"https://nullcc.github.io/categories/web后端/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://nullcc.github.io/tags/ElasticSearch/"}]},{"title":"(译)package.json详解","slug":"(译)package.json详解","date":"2017-05-09T16:00:00.000Z","updated":"2022-04-15T03:41:13.011Z","comments":true,"path":"2017/05/10/(译)package.json详解/","link":"","permalink":"https://nullcc.github.io/2017/05/10/(译)package.json详解/","excerpt":"本文翻译了package.json。","text":"本文翻译了package.json。 概述本文囊括了所有package.json文件中你需要知道的细节。注意package.json必须是纯JSON的，而不仅仅是一个JavaScript对象字面量。该文件描述的很多行为都受npm-config中的配置影响。 下面分别介绍package.json中各个字段的含义和用法。 namename和version字段是package.json文件中最重要的字段。这是必须的字段，如果你的npm包没有指定这两个字段，将无法被安装。name和version字段被假定组合成一个唯一的标识符。包内容的更改和包版本的更改是同步的。 name字段的含义不需要过多解释，就是npm包名。 几个规则： name的长度必须小于等于214个字符。 name不能以”.”(点)或者”_”(下划线)开头。 name中不能包含大写字母。 name最终将被用作URL的一部分、命令行的参数和文件夹名。因此，name不能含有非URL安全的字符。 几个建议： 不要使用已存在的name作为包名。 不要在name中使用”js”和”node”，这会假定这是js文件，一旦你写一个package.json文件，你就可以在”engines”字段中指定解释器引擎。 name字段可能会被作为传输传递给require()函数，因此它最好是简短的、自描述的。 你可能会需要在深入开发一个包之前先检查npm的registry来确认某个name是否被使用过，可以参考https://www.npmjs.com/。 一个name可以用scope来指定一个前缀，比如@myorg/mypackage，可以参考npm-scope。 versionname和version字段是package.json文件中最重要的字段。这是必须的字段，如果你的npm包没有指定这两个字段，将无法被安装。name和version字段被假定组合成一个唯一的标识符。包内容的更改和包版本的更改是同步的。 version字段必须能够被node-semver解析，node-semver作为依赖项被捆绑进了npm中。(可以使用npm install semver来使用) 关于版本号和范围的信息可以参考semver。 descriptionnpm包的描述，description是一个字符串。它可以帮助人们在使用npm search时找到这个包。 keywordsnpm包的关键字，keywords是一个字符串的数组。它可以帮助人们在使用npm search时找到这个包。 homepage项目主页的url。 注意： 这和”url”不一样。如果你放一个”url”字段，registry会以为是一个跳转到你发布在其他地方的地址，然后喊你滚粗。嗯，滚粗，没开玩笑。 bugs改项目的issue跟踪页面或这报告issue的email地址。这对使用这个包遇到问题的用户会有帮助。 差不多是这样： { &quot;url&quot; : &quot;https://github.com/owner/project/issues&quot;, &quot;email&quot; : &quot;project@hostname.com&quot; } 你可以择其一或者两个都写上。如果只想提供一个url，你可以对”bugs”字段指定一个字符串而不是object。 如果提供了一个url，它会被用于npm bugs命令。 license你应该对你的包指定一个license来让用户知道他们的使用权利和和任何限制。 如果你正在使用BSD-2-Clause或MIT这样的通用许可证，可以为你的license添加一个当前SPDX的许可证标识符，比如： { &quot;license&quot; : &quot;BSD-3-Clause&quot; } 你可以查看SPDX许可证标识符的完整列表，理想情况下你应该挑选一个经过OSI核准的标识符。 如果你的包在多个通用许可证下被授权，使用一个(SPDX许可证表达式语法v2.0)[https://npmjs.com/package/spdx]，比如： { &quot;license&quot; : &quot;(ISC OR GPL-3.0)&quot; } 如果你正在使用的许可未被授予一个SPDX标识符，或者你正在使用自定义的许可证，使用如下： { &quot;license&quot; : &quot;SEE LICENSE IN &lt;filename&gt;&quot; } 然后在包的根目录下提供一个叫的许可证文件。 一些旧的包使用license对象或一个”license”属性包含一个license的数组： // Not valid metadata { &quot;license&quot; : { &quot;type&quot; : &quot;ISC&quot;， &quot;url&quot; : &quot;http://opensource.org/licenses/ISC&quot; } } // Not valid metadata { &quot;licenses&quot; : [ { &quot;type&quot;: &quot;MIT&quot;, &quot;url&quot;: &quot;http://www.opensource.org/licenses/mit-license.php&quot; }, { &quot;type&quot;: &quot;Apache-2.0&quot;, &quot;url&quot;: &quot;http://opensource.org/licenses/apache2.0.php&quot; } ] } 上述这种风格的写法已经被废弃了，取而代之的是SPDX表达式： { &quot;license&quot;: &quot;ISC&quot; } { &quot;license&quot;: &quot;(MIT OR Apache-2.0)&quot; } 最后，如果你不希望授权别人以任何形式使用私有包或未发布的包，可以这样写： { &quot;license&quot;: &quot;UNLICENSED&quot;} 也可以设置： &quot;private&quot;: true 来防止意外的发布。 关于人的字段: author, contributorsauthor是一个人，contributors是一些人的数组。person是一个对象，拥有必须的name字段和可选的url和email字段，像这样： { &quot;name&quot; : &quot;Barney Rubble&quot;, &quot;email&quot; : &quot;b@rubble.com&quot;, &quot;url&quot; : &quot;http://barnyrubble.tumblr.com/&quot; } 或者你也可以使用单个字符串的精简形式，npm会帮你解析它： &quot;Barney Rubble &lt;b@rubble.com&gt; (http://barnyrubble.tumblr.com/)&quot; 这里email和url也是可选的。 npm也会使用你提供的npm用户信息来设置一个顶级的”maintainers”字段。 filesfiles字段是一个被项目包含的文件名数组，如果你在里面放一个文件夹名，那么这个文件夹中的所有文件都会被包含进项目中(除非是那些在其他规则中被忽略的文件)。 你还可以在包的根目录或子目录下提供一个”.npmignore”文件来忽略项目包含文件，即使这些文件被包含在files字段中。.npmignore文件和.gitignore的功能很像。 某些文件总是被包含的，不论是否在规则中指定了它们： package.json README (and its variants) CHANGELOG (and its variants) LICENSE / LICENCE 相反地，一些文件总是被忽略： .git CVS .svn .hg .lock-wscript .wafpickle-N *.swp .DS_Store ._* npm-debug.log mainmain字段指定了模块的入口程序文件。就是说，如果你的模块名叫”foo”，用户安装了它，并且调用了 require(“foo”)，则这个main字段指定的模块的导出对象会被返回。 这应该是一个相对于包根目录的模块标识。 对于大部分模块来说，main字段除了指定一个主入口文件以外没什么其他用处了。 bin许多包有一个或多个可执行文件希望被安装到系统路径。在npm下要这么做非常容易(事实上，npm就是这么运行的)。 这需要在你的package.json中提供一个bin字段，它是一个命令名和本地文件名的映射。在安装时，如果是全局安装，npm将会使用符号链接把这些文件链接到prefix/bin，如果是本地安装，会链接到./node_modules/.bin/。 比如，要使用myapp作为命令时可以这么做： { &quot;bin&quot; : { &quot;myapp&quot; : &quot;./cli.js&quot; } } 这么一来，当你安装myapp，npm会从cli.js文件创建一个到/usr/local/bin/myapp的符号链接(这使你可以直接在命令行执行myapp)。 如果你只有一个可执行文件，那么它的名字应该和包名相同，此时只需要提供这个文件路径(字符串)，比如： { &quot;name&quot;: &quot;my-program&quot;, &quot;version&quot;: &quot;1.2.5&quot;, &quot;bin&quot;: &quot;./path/to/program&quot; } 这和以下这种写法相同： { &quot;name&quot;: &quot;my-program&quot;, &quot;version&quot;: &quot;1.2.5&quot;, &quot;bin&quot; : { &quot;my-program&quot; : &quot;./path/to/program&quot; } } man指定一个单一的文件名或一个文件名数组来让man程序使用。 如果只给man字段提供一个文件，则安装完毕后，它就是man 的结果，这和此文件名无关，比如： { &quot;name&quot; : &quot;foo&quot;, &quot;version&quot; : &quot;1.2.3&quot;, &quot;description&quot; : &quot;A packaged foo fooer for fooing foos&quot;, &quot;main&quot; : &quot;foo.js&quot;, &quot;man&quot; : &quot;./man/doc.1&quot; } 上面这个配置将会在执行man foo时就会使用./man/doc.1这个文件。 如果指定的文件名并未以包名开头，那么它会被冠以前缀，像这样： { &quot;name&quot; : &quot;foo&quot;, &quot;version&quot; : &quot;1.2.3&quot;, &quot;description&quot; : &quot;A packaged foo fooer for fooing foos&quot;, &quot;main&quot; : &quot;foo.js&quot;, &quot;man&quot; : [ &quot;./man/foo.1&quot;, &quot;./man/bar.1&quot; ] } 这将会为man foo和man foo-bar创建文件。 man文件必须以一个数字结尾，和一个可选的.gz后缀(当它被压缩时)。这个数字说明了这个文件被安装到哪个节中。 { &quot;name&quot; : &quot;foo&quot;, &quot;version&quot; : &quot;1.2.3&quot;, &quot;description&quot; : &quot;A packaged foo fooer for fooing foos&quot;, &quot;main&quot; : &quot;foo.js&quot;, &quot;man&quot; : [ &quot;./man/foo.1&quot;, &quot;./man/foo.2&quot; ] } 会为使用man foo和man 2 foo而创建。 directories CommonJS Packages规范说明了几种你可以用directories对象来标示你的包结构的方法。如果你去看npm’s package.json，你会看到它标示出出doc、lib和man。 在未来，这个信息可能会被用到。 directories.lib告诉你库文件夹的位置，目前没有什么地方需要用到lib文件夹，但是这是重要的元信息。 directories.bin如果你在directories.bin中指定一个bin目录，在这个目录中的所有文件都会被当做在bin来使用。 由于bin指令的工作方式，同时指定一个bin路径和设置directories.bin将是一个错误。如果你想指定独立的文件，使用bin，如果想执行某个文件夹里的所有文件，使用directories.bin。 directories.mandirectories.man指定的文件夹里都是man文件，系统通过遍历这个文件夹来生成一个man的数组。 directories.doc把markdown文件放在这。也许某一天这些文件将被漂亮地展示出来，不过这仅仅是也许。 directories.example把示例脚本放在这。也许某一天会被用到。 repository指明你的代码被托管在何处，这对那些想要参与到这个项目中的人来说很有帮助。如果git仓库在github上，用npm docs命令将会找到你。 像这样： &quot;repository&quot; : { &quot;type&quot; : &quot;git&quot;, &quot;url&quot; : &quot;https://github.com/npm/npm.git&quot; } &quot;repository&quot; : { &quot;type&quot; : &quot;svn&quot;, &quot;url&quot; : &quot;https://v8.googlecode.com/svn/trunk/&quot; } url应该是公开且可用的(可能是只读的)，这个url应该可以被版本控制系统不经修改地处理。不应该是一个在浏览器中打开的html项目页面，这个url是给计算机使用的。 对于github、github gist、Bitbucket或GitLab的仓库，你可以在npm install中使用相同的缩写形式： &quot;repository&quot;: &quot;npm/npm&quot; &quot;repository&quot;: &quot;gist:11081aaa281&quot; &quot;repository&quot;: &quot;bitbucket:example/repo&quot; &quot;repository&quot;: &quot;gitlab:another/repo&quot; scriptsscripts字段是一个由脚本命令组成的字典，这些命令运行在包的各个生命周期中。这里的键是生命周期事件名，值是要运行的命令。可以参考(npm-scripts)[https://docs.npmjs.com/files/package.json#directorieslib]获取配置scripts的更多信息。 configconfig字段是一个对象，可以用来配置包脚本中的跨版本参数，比如如下这个实例： { &quot;name&quot; : &quot;foo&quot;, &quot;config&quot; : { &quot;port&quot; : &quot;8080&quot; } } 然后有一个start命令引用npm_package_config_port环境变量，用户也可以用如下方式改写： npm config set foo:port 8001 可以参考(npm-config)[https://docs.npmjs.com/misc/config]和(npm-scripts)[https://docs.npmjs.com/misc/scripts]获得更多关于包配置的信息。 dependenciesdependencies字段是一个对象，它指定了依赖的包名和其版本范围的映射。版本范围是个有一个或多个空白分隔描述符的字符串。dependencies字段还可以用tarball或者git URL。 请不要将测试或过渡性的依赖放到dependencies中，请参考下面的devDependencies。 可以参考semver获取更多关于指定版本范围的细节信息。 version 必须确切匹配这个version >version 必须大于这个version >=version 必须大于等于这个version &lt; version 必须小于这个version &lt;=version 必须小于等于这个version ~version 大约相当于version，参考semver ^version 与version兼容，参考semver 1.2.x 可以是1.2.0、1.2.1等，但不能是1.3.0 http://… 参考下面的URL作为依赖项 * 匹配任何版本 “”(空字符串) 匹配任何版本，和*一样 version1 - version2 相当于 &gt;=version1 &lt;=version2 range1 || range2 range1或range2其中一个满足时采用该version git… 参考下面的Git URL作为依赖项 user/repo 参考下面的GitHub URLs tag 一个以tag发布的指定版本，参考npm-tag path/path/path 参考下面的本地Paths 举个栗子，下面这种写法是合法的： { &quot;dependencies&quot; :{ &quot;foo&quot; : &quot;1.0.0 - 2.9999.9999&quot;, &quot;bar&quot; : &quot;&gt;=1.0.2 &lt;2.1.2&quot;, &quot;baz&quot; : &quot;&gt;1.0.2 &lt;=2.3.4&quot;, &quot;boo&quot; : &quot;2.0.1&quot;, &quot;qux&quot; : &quot;&lt;1.0.0 || &gt;=2.3.1 &lt;2.4.5 || &gt;=2.5.2 &lt;3.0.0&quot;, &quot;asd&quot; : &quot;http://asdf.com/asdf.tar.gz&quot;, &quot;til&quot; : &quot;~1.2&quot;, &quot;elf&quot; : &quot;~1.2.3&quot;, &quot;two&quot; : &quot;2.x&quot;, &quot;thr&quot; : &quot;3.3.x&quot;, &quot;lat&quot; : &quot;latest&quot;, &quot;dyl&quot; : &quot;file:../dyl&quot; } } URLs作为依赖项可以在version上指定一个压缩包的url。 当执行npm install时这个压缩包会被下载并且安装到本地。 Git URLs作为依赖项Git URLs可以是如下几种形式之一： git://github.com/user/project.git#commit-ish git+ssh://user@hostname:project.git#commit-ish git+ssh://user@hostname/project.git#commit-ish git+http://user@hostname/project/blah.git#commit-ish git+https://user@hostname/project/blah.git#commit-ish commit-ish可以是任何tag、sha或者branch，并作为一个参数提供给git进行checkout，默认值是master。 GitHub URLs从1.1.65版本开始，你可以引用Github urls作为版本号，比如”foo”: “user/foo-project”。也可以包含一个commit-ish后缀，举个栗子： { &quot;name&quot;: &quot;foo&quot;, &quot;version&quot;: &quot;0.0.0&quot;, &quot;dependencies&quot;: { &quot;express&quot;: &quot;visionmedia/express&quot;, &quot;mocha&quot;: &quot;visionmedia/mocha#4727d357ea&quot; } } 本地路径从版本2.0.0开始你可以提供一个包的本地路径。本地路径可以在你使用npm install -S或npm install –save时被保存，具体形式如下： ../foo/bar ~/foo/bar ./foo/bar /foo/bar 在下面这种情况下它会被规范化成为一个相对路径并且加入到你的package.json文件中，比如： { &quot;name&quot;: &quot;baz&quot;, &quot;dependencies&quot;: { &quot;bar&quot;: &quot;file:../foo/bar&quot; } } 这个特性有助于当你不想从一个外部服务器安装npm包的情况，比如本地离线开发和创建测试，但最好不要在发布包到公共registry时这样使用。 devDependencies如果有人计划在他们的项目中下载和使用你的模块，但他们可能并不想或并不需要你开发所使用的外部测试和文档框架。 在这种情况下，最好将这些附加的项放在devDependencies中。 这些项将会在根目录下执行npm link或npm install时被安装，并且可以像其他npm配置参数一样被管理。可以参考npm-config获得更多信息。 对于那些非特定平台的构建步骤，比如编译CoffeeScript或把其他语言转换成JavaScript，可以使用prepublish脚本来处理，并且把这个过程的依赖包放在devDependencies中。 举个栗子： { &quot;name&quot;: &quot;ethopia-waza&quot;, &quot;description&quot;: &quot;a delightfully fruity coffee varietal&quot;, &quot;version&quot;: &quot;1.2.3&quot;, &quot;devDependencies&quot;: { &quot;coffee-script&quot;: &quot;~1.6.3&quot; }, &quot;scripts&quot;: { &quot;prepublish&quot;: &quot;coffee -o lib/ -c src/waza.coffee&quot; }, &quot;main&quot;: &quot;lib/waza.js&quot; } prepublish脚本会在publishing前运行，这样用户就可以不用自己去require来编译就能使用。在开发模式下(比如本地运行npm install)，将会执行这个脚本，这样测试就非常方便了。 peerDependencies在某些情况下，当一个主机无法require依赖包时，你会想要告诉它还有哪些工具或库与这个依赖包兼容。这通常被成为一个插件。尤其是在host文档中声明的模块会暴露一个特定的接口。 举个栗子： { &quot;name&quot;: &quot;tea-latte&quot;, &quot;version&quot;: &quot;1.3.5&quot;, &quot;peerDependencies&quot;: { &quot;tea&quot;: &quot;2.x&quot; } } 这将确保tea-latte这个包只会和2.x版本的tea一起被安装。执行npm install tea-latte可能产生以下关系图： ├── tea-latte@1.3.5 └── tea@2.2.0 注意：如果没有在依赖树中显式声明比它们更高的依赖版本，版本1和版本2的npm将会自动安装peerDependencies。在npm的下一个大版本npm3中，情况将完全不同。你将收到一个警告，告诉你peerDependency还没有被安装。在npm1和npm2中这个行为经常会导致混乱，新的npm版本的设计将会极力避免这种情况。 试图安装一个有冲突的依赖项的插件将会导致一个错误。因此你必须确保你的插件的依赖项版本范围尽可能大，并且不要把版本锁死在一个特点的补丁版本上。 假设主机使用semver进行编译，只改变这个包的主版本将会导致你的插件不可用。因此，如果你的插件的某个依赖包运行在每个1.x版本下，使用”^1.0”或”1.x”。如果你需要的功能在1.5.2版本中，使用”&gt;= 1.5.2 &lt; 2”。 bundledDependencies在发布包时，包名的数组会被打包进去。 如果拼写成”bundleDependencies”(少个d)，也是可以的。 optionalDependencies如果一个依赖项可用，但希望在这个依赖项无法被找到或者安装时失败npm还能继续处理(不中断)，那么你可以把它放在optionalDependencies中。和dependencies一样，optionalDependencies是一个包名和版本号或url的映射。区别在于optionalDependencies中的依赖构建失败时不会导致npm整体安装失败。 但是你的程序依然有责任处理这种缺失的依赖项，比如这样： try { var foo = require(&apos;foo&apos;) var fooVersion = require(&apos;foo/package.json&apos;).version } catch (er) { foo = null } if ( notGoodFooVersion(fooVersion) ) { foo = null } // .. then later in your program .. if (foo) { foo.doFooThings() } optionalDependencies中的项会覆盖dependencies中的同名项，所以一个特定名字的项最好只出现在一个地方。 engines你可以指定node的工作版本： { &quot;engines&quot; : { &quot;node&quot; : &quot;&gt;=0.10.3 &lt;0.12&quot; } } 和dependencies类似，如果你不指定一个node版本(或者你用’*’指定)，则任何一个node版本都可以。 如果你指定了一个’engines’字段，则npm将会在某处包含这个node版本。如果忽略’engines’字段，则npm只会仅仅假设这个包工作在node下。 你还可以使用’engines’字段来指定可以安装这个包的npm版本，举个栗子： { &quot;engines&quot; : { &quot;npm&quot; : &quot;~1.0.20&quot; } } 请注意，除非用户设置了engine-strict标记，否则这个字段只是一个建议值。 engineStrict这个特性在npm 3.0.0中已经废弃。 npm 3.0.0之前的版本，这个特性用来处理那些设置了engine-strict标记的包。 os可以指定模块运行的操作系统： &quot;os&quot; : [ &quot;darwin&quot;, &quot;linux&quot; ] 也可以使用操作系统黑名单来替代白名单，只要在前面加个’!’： &quot;os&quot; : [ &quot;!win32&quot; ] 主机的操作系统可以通过process.platform来确定。 虽然找不到什么很好的理由支持这么做，但是这里还可以黑名单和白名单混用。 cpu如果你的代码只能运行在特定的cpu架构上，你可以指明： &quot;cpu&quot; : [ &quot;x64&quot;, &quot;ia32&quot; ] 和os选项类似，你还可以使用黑名单： &quot;cpu&quot; : [ &quot;!arm&quot;, &quot;!mips&quot; ] 主机的cpu架构可以通过process.arch来确定。 preferGlobal如果你的包是一个需要进行全局安装的命令行应用，需要设置preferGlobal为true，如果这个包被本地安装会报出一个警告。 这个选项并不会阻止用户本地安装这个包，但这么做确实能在包未按照预期被安装造成诸多麻烦时提供一些提示。 private如果你在包的package.json中设置”private”: true，则npm会拒绝发布它。 这是防止私有包被以外发布的一种方法。如果你希望包装某个包只能被发布到特定的一个registry中(比如，一个内部的registry)，则可以使用下面的publishConfig字典来描述以在publish-time重写registry配置参数。 publishConfig这是一个在publish-time时会用到的配置集合。当你想设置tag、registry或access时特别有用，所以你可以确保一个给定的包无法在没有被打上”latest”标记时就被发布到全局公共的registry。 任何配置都可以被覆盖，当然可能只有”tag”, “registry”和”access”和发布意图有关。 参考npm-config来查看那些可以被覆盖的配置项列表。 DEFAULT VALUESnpm会基于包内容设置一些默认值。 “scripts”: {“start”: “node server.js”} 如果包的根目录中有一个server.js，那么npm会用它来作为入口文件：运行node server.js。 “scripts”:{“preinstall”: “node-gyp rebuild”} 如果包的根目录中有一个binding.gyp文件，那么npm会在运行preinstall命令编译时使用它。 “contributors”: […] 如果包的根目录中有一个AUTHORS文件，那么npm会把它的每一个行格式化成Name \\&lt; email > (url)的形式，其中email和url是可选的。以一个#或者空白符开头的行将被忽略。 参考资料semvernpm-initnpm-versionnpm-confignpm-confignpm-helpnpm-faqnpm-installnpm-publishnpm-uninstall","categories":[{"name":"文档翻译","slug":"文档翻译","permalink":"https://nullcc.github.io/categories/文档翻译/"}],"tags":[{"name":"node","slug":"node","permalink":"https://nullcc.github.io/tags/node/"}]},{"title":"(译)npm-scripts详解","slug":"(译)npm-scripts详解","date":"2017-05-09T16:00:00.000Z","updated":"2022-04-15T03:41:13.011Z","comments":true,"path":"2017/05/10/(译)npm-scripts详解/","link":"","permalink":"https://nullcc.github.io/2017/05/10/(译)npm-scripts详解/","excerpt":"本文翻译了npm-scripts。","text":"本文翻译了npm-scripts。 概述npm在package.json中支持脚本属性，有以下几种脚本： prepublish：在包被发布前运行。(也会运行在无任何参数的本地npm install时) publish, postpublish：在包被发布后运行。 preinstall：在包被安装前运行。 install, postinstall：在包被安装后运行。 preuninstall, uninstall：在被被卸载前运行。 postuninstall：在包被卸载后运行。 preversion, version：更改包版本前运行。 postversion：更改包版本后运行。 pretest, test, posttest：在运行npm test时会运行。 prestop, stop, poststop：在运行npm stop时会运行。 prestart, start, poststart：在运行npm start时会运行。 prerestart, restart, postrestart在运行npm restart时会运行。注意，如果没有提供npm restart及脚本，npm restart会运行npm stop和npm start脚本。 而且，还可以使用npm run-script 来运行任意脚本。名字中带有pre和post的脚本也会在相应的脚本被运行前后被执行(例如premyscript, myscript, postmyscript这三个脚本会顺序被执行)。 一般用法如果你需要在你的包被使用前执行一些操作，并且不依赖于操作系统或目标系统的架构，可以用一个prepublish脚本，在里面包含类似如下的操作： 把CoffeeScript源码编译成JavaScript。 创建JavaScript代码的压缩版本。 获取包需要用到的远端资源。 这么做的好处是，这些操作会在固定的时间执行一次，从而降低复杂性和可变现。而且这意味着，你可以把coffee-script作为一个devDependency，因此你的用户并不需要安装它。你不需要在包中包含压缩版本的代码，这可以减小包的大小。你不需要依赖于目标用户的机器支持curl或wget等系统工具。 默认值npm会基于包内容设置一些默认值。 “scripts”: {“start”: “node server.js”} 如果包的根目录中有一个server.js，那么npm会用它来作为入口文件：运行node server.js。 “install”: “node-gyp rebuild”: 如果包的根目录中有一个binding.gyp文件，那么npm会在使用node-gyp编译时执行install命令。 USER如果用root权限使用npm，则会将uid变成root用户或者在用户配置中指定的值，默认是无用户。可以设置一个不安全标志来在使用root权限运行脚本时提示用户。 ENVIRONMENT在包脚本的运行环境中，会展示很多关于当前包安装状态和进度的信息。 path如果你的依赖包定义了可执行脚本，比如测试套件，那么这些可执行文件将会被加入到脚本执行路径中。所以，如果你的package.json中有这样的信息： { &quot;name&quot; : &quot;foo&quot;, &quot;dependencies&quot; : { &quot;bar&quot; : &quot;0.1.x&quot; }, &quot;scripts&quot;: { &quot;start&quot; : &quot;bar ./test&quot; } } 那么你可以运行npm start来执行这个bar脚本，这个bar会在执行npm install时被导入到node_modules/.bin目录中去。 package.json的变量package.json中的字段会被附加上npm_package_前缀。举个例子，比如你的package.json中有{“name”:”foo”, “version”:”1.2.5”}这样的信息，那么你的包脚本会包含值为”foo”的环境变量npm_package_name，而npm_package_version的值为”1.2.5”。 配置配置参数都会被冠以npm_config_的前缀。比如环境变量npm_config_root存放的就是root配置。 特殊的package.json “config” 对象package.json中的config字段可以被这种形式改写：[@]:。举个例子，如果package.json中是这样的： { &quot;name&quot; : &quot;foo&quot;, &quot;config&quot; : { &quot;port&quot; : &quot;8080&quot; }, &quot;scripts&quot; : { &quot;start&quot; : &quot;node server.js&quot; } } 然后server.js是这样的： http.createServer(...).listen(process.env.npm_package_config_port) 那么用户可以用这种方式来改写： npm config set foo:port 80 当前生命周期事件最后，环境变量npm_lifecycle_event会被设置成具体某个生命周期阶段。所以，你可以写一个在不同生命周期阶段执行不同操作的的脚本。 对象会被展平表示出来，比如package.json中有一个{“scripts”:{“install”:”foo.js”}}，那么你在脚本中可以这么写： process.env.npm_package_scripts_install === &quot;foo.js&quot; 退出脚本是通过给sh传递参数来运行的。 如果脚本的退出代码不是0，那么sh会终止脚本进程。 注意这些脚本文件不需要一定要是nodejs或者javascript程序。它们只要是某种可执行文件即可。 钩子脚本如果你想在所有包的特定生命周期事件中运行特定的脚本，那么你可以使用钩子脚本。 在node_modules/.hooks/{eventname}下放一个可执行文件，则该根目录下安装的所有包在到达包生命周期的这个事件阶段时会执行这个可执行文件。 钩子脚本的运行机制和package.json的脚本一样。它们会和上述环境运行在不同的子进程中。 最佳实践 如果不是非常需要请不要以一个非零错误码退出。除了卸载脚本，这么做会导致npm运行出错，而且可能会导致回滚。如果只是少数几个错误或者只是禁止使用某些可选特性，比较好的做法是打印一个警告并且安全退出。 不要写脚本去做那些npm本身能帮你做的事情。请通过查看package.json来确定所有那些你能通过简单指定和描述做的事情。一般来说，这更具有鲁棒性和一致性。 检查env来决定往什么地方安装东西。具体来说，如果环境变量npm_config_binroot被设置成了/home/user/bin，那么久不要尝试去安装可执行文件到/usr/local/bin。因为用户可能是为了某种原因才这么设置的。 不要用sudo来运行你的脚本命令。如果因为某种原因需要root权限，那么就报告一个错误，用户会转而用sudo来运行npm。 不要使用install脚本。请使用一个.gyp文件来编译，用prepublish做一些杂事。你不应该明确地设置一个preinstall或者install脚本。如果你正在这么做，请考虑一下是否有其他选项。install和preinstall脚本的唯一合法使用方式是在目标架构上进行编译。 参考资料 npm-run-scriptpackage.jsonnpm-developersnpm-install","categories":[{"name":"文档翻译","slug":"文档翻译","permalink":"https://nullcc.github.io/categories/文档翻译/"}],"tags":[{"name":"node","slug":"node","permalink":"https://nullcc.github.io/tags/node/"}]},{"title":"MongoDB索引(1)——入门篇：学习使用MongoDB数据库索引","slug":"MongoDB索引(1)——入门篇：学习使用MongoDB数据库索引","date":"2017-05-09T16:00:00.000Z","updated":"2022-04-15T03:41:13.015Z","comments":true,"path":"2017/05/10/MongoDB索引(1)——入门篇：学习使用MongoDB数据库索引/","link":"","permalink":"https://nullcc.github.io/2017/05/10/MongoDB索引(1)——入门篇：学习使用MongoDB数据库索引/","excerpt":"介绍了MongoDB索引索引的相关知识。","text":"介绍了MongoDB索引索引的相关知识。 1. 准备工作在学习使用MongoDB数据库索引之前，有一些准备工作要做，之后的探索都是基于这些准备工作。 首先需要建立一个数据库和一些集合，这里我就选用一个国内手机号归属地的库，大约32W条记录，数据量不大，不过做一些基本的分析是够了。 首先我们建立一个数据库，叫做db_phone，然后导入测试数据。测试数据就是一些手机号归属地的信息。单个文档长这个样子： { &quot;_id&quot; : ObjectId(&quot;57bd12ba085bed84151ca203&quot;), &quot;prefix&quot; : &quot;1898852&quot;, &quot;province&quot; : &quot;广东&quot;, &quot;city&quot; : &quot;佛山&quot;, &quot;isp&quot; : &quot;中国电信&quot; } 2. 学会分析MongoDB的查询默认情况下，每个MongoDB文档都有一个_id字段，这个字段是唯一的。系统能保证在单台机器上这个字段是唯一的(而且是递增)，有兴趣的同学可以去看看_id的生成方式。 (1) 用_id字段作为查询条件在MongoDB shell中，利用一些查询语句来对数据库进行查询，比如想要找到刚才那个文档，可以执行： db.phonehomes.find({_id:ObjectId(&quot;57bd12ba085bed84151ca203&quot;)}) 我们利用explain()来分析这个查询： db.phonehomes.find({_id:ObjectId(&quot;57bd12ba085bed84151ca203&quot;)}).explain() 这个查询分析不会返回找到的文档，而是返回该查询的分析文档： { &quot;cursor&quot; : &quot;IDCursor&quot;, &quot;n&quot; : 1, &quot;nscannedObjects&quot; : 1, &quot;nscanned&quot; : 1, &quot;indexOnly&quot; : false, &quot;millis&quot; : 0, &quot;indexBounds&quot; : { &quot;_id&quot; : [ [ ObjectId(&quot;57bd12ba085bed84151ca203&quot;), ObjectId(&quot;57bd12ba085bed84151ca203&quot;) ] ] }, &quot;server&quot; : &quot;zhangjinyideMac-Pro.local:27017&quot; } 解释一些比较重要的几个字段： 1. “cursor” : “IDCursor”cursor的本意是游标，在这里它表示用的是什么索引，或者没用索引。没用索引就是全表扫描了，后面会看到。这里的cursor是”IDCursor”，这是_id特有的一个索引。默认情况下，数据库会为_id创建索引，因此在查询中如果用_id作为查询条件，效率是非常高的。 2. “n” : 1返回文档的个数。这个查询本身只返回了一个文档(因为_id是不能重复的)。 3. “nscannedObjects” : 1实际查询的文档数。 4. “nscanned” : 1表示使用索引扫描的文档数，如果没有索引，这个值是整个集合的所有文档数。 5. “indexOnly” : false表示是否只有索引即可完成查询，当查询的字段都存在一个索引中并且返回的字段也在同一索引中即为true。如果执行： db.phonehomes.find({_id:ObjectId(&quot;57bd12ba085bed84151ca203&quot;)}, {&quot;_id&quot;: 1}).explain() 则indexOnly会为true。 6. “millis” : 0,查询耗时，单位毫秒，为0说明这个查询太快了。由于索引会被加载到内存中，直接利用内存中的索引是非常高效的，可能只用到了纳秒级别的时间(1ms = 1000000ns)，因此就显示为0了。 7. “indexBounds”索引的使用情况，即文档中key的上下界。 (2) 用未被索引的prefix字段作为查询条件接下来我们使用一个没有索引的字段：prefix，查询语句如下： db.phonehomes.find({prefix: &apos;1899950&apos;}).explain() 返回结果： { &quot;cursor&quot; : &quot;BasicCursor&quot;, &quot;isMultiKey&quot; : false, &quot;n&quot; : 1, &quot;nscannedObjects&quot; : 327664, &quot;nscanned&quot; : 327664, &quot;nscannedObjectsAllPlans&quot; : 327664, &quot;nscannedAllPlans&quot; : 327664, &quot;scanAndOrder&quot; : false, &quot;indexOnly&quot; : false, &quot;nYields&quot; : 2559, &quot;nChunkSkips&quot; : 0, &quot;millis&quot; : 92, &quot;server&quot; : &quot;zhangjinyideMac-Pro.local:27017&quot;, &quot;filterSet&quot; : false } 这次的字段比较多，还是看来一些重要的(有些和之前查询完全重复就不列举了)： 1. “cursor” : “BasicCursor”查询使用索引的信息，为”BasicCursor”表示未使用索引，即全表扫描了。 2. “n” : 1返回的文档数为1。 3. “nscannedObjectsAllPlans” : 327664所有查询计划的查询文档数。 4. “nscannedAllPlans” : 327664所有查询计划的查询文档数。 5. “scanAndOrder” : false是否对返回的结果排序，当直接使用索引的顺序返回结果时其值为false。如果使用了sort()，则为true。 6. “nYields” : 2559表示查询暂停的次数。这是由于mongoDB的其他操作使得查询暂停，使得这次查询放弃了读锁以等待写操作的执行。 7. “nChunkSkips” : 0表示的略过的文档数量，当在分片系统中正在进行的块移动时会发生。 8. “filterSet” : false表示是否应用了索引过滤。 需要特别说明的是，上面3个重要的文档数量指标的关系为：nscanned &gt;= nscannedObjects &gt;= n，也就是扫描数（也可以说是索引条目） &gt;= 查询数（通过索引到硬盘上查询的文档数） &gt;= 返回数（匹配查询条件的文档数）。 可以看到由于prefix字段没有索引，导致了全表扫描。当文档数量很小（只有32W条）时，耗时不大（92ms），不过一旦文档数量非常大，查询耗时就会增长到一个无法忍受的程度。 (3) 用有索引的prefix字段作为查询条件为prefix字段增加索引： db.phonehomes.ensureIndex({&quot;prefix&quot;: 1}) 成功建立索引后，执行之前那个查询语句： db.phonehomes.find({prefix: &apos;1899950&apos;}).explain() 返回结果： { &quot;cursor&quot; : &quot;BtreeCursor prefix_1&quot;, &quot;isMultiKey&quot; : false, &quot;n&quot; : 1, &quot;nscannedObjects&quot; : 1, &quot;nscanned&quot; : 1, &quot;nscannedObjectsAllPlans&quot; : 1, &quot;nscannedAllPlans&quot; : 1, &quot;scanAndOrder&quot; : false, &quot;indexOnly&quot; : false, &quot;nYields&quot; : 0, &quot;nChunkSkips&quot; : 0, &quot;millis&quot; : 0, &quot;indexBounds&quot; : { &quot;prefix&quot; : [ [ &quot;1899950&quot;, &quot;1899950&quot; ] ] }, &quot;server&quot; : &quot;zhangjinyideMac-Pro.local:27017&quot;, &quot;filterSet&quot; : false， // 略去一部分暂时不讨论的内容 } 重点看下”cursor”字段： &quot;cursor&quot; : &quot;BtreeCursor prefix_1&quot; 这个查询使用了一个prefix的索引。由于索引的使用，使得这个查询变得非常高效，从以下这几个字段可以很明显地看出： &quot;n&quot; : 1, &quot;nscannedObjects&quot; : 1, &quot;nscanned&quot; : 1, &quot;nscannedObjectsAllPlans&quot; : 1, &quot;nscannedAllPlans&quot; : 1, &quot;millis&quot; : 0, (4) 有多个单独索引的情况执行查询： db.phonehomes.find({province: &apos;福建&apos;, &apos;isp&apos;: &apos;中国电信&apos;}).explain() 返回结果： { &quot;cursor&quot; : &quot;BasicCursor&quot;, &quot;isMultiKey&quot; : false, &quot;n&quot; : 2667, &quot;nscannedObjects&quot; : 327664, &quot;nscanned&quot; : 327664, &quot;nscannedObjectsAllPlans&quot; : 327664, &quot;nscannedAllPlans&quot; : 327664, &quot;scanAndOrder&quot; : false, &quot;indexOnly&quot; : false, &quot;nYields&quot; : 2559, &quot;nChunkSkips&quot; : 0, &quot;millis&quot; : 138, &quot;server&quot; : &quot;zhangjinyideMac-Pro.local:27017&quot;, &quot;filterSet&quot; : false, // 略去一部分暂时不讨论的内容 } 可以看到是全表扫描。 先给”isp”字段加索引： db.phonehomes.ensureIndex({&quot;isp&quot;: 1}) 再执行一次： db.phonehomes.find({province: &apos;福建&apos;, &apos;isp&apos;: &apos;中国电信&apos;}).explain() 返回结果： { &quot;cursor&quot; : &quot;BtreeCursor isp_1&quot;, &quot;isMultiKey&quot; : false, &quot;n&quot; : 2667, &quot;nscannedObjects&quot; : 59548, &quot;nscanned&quot; : 59548, &quot;nscannedObjectsAllPlans&quot; : 59548, &quot;nscannedAllPlans&quot; : 59548, &quot;scanAndOrder&quot; : false, &quot;indexOnly&quot; : false, &quot;nYields&quot; : 465, &quot;nChunkSkips&quot; : 0, &quot;millis&quot; : 64, &quot;indexBounds&quot; : { &quot;isp&quot; : [ [ &quot;中国电信&quot;, &quot;中国电信&quot; ] ] }, &quot;server&quot; : &quot;zhangjinyideMac-Pro.local:27017&quot;, &quot;filterSet&quot; : false, // 略去一部分暂时不讨论的内容 } 发现”cursor”为”BtreeCursor isp_1”，这个查询用到了isp的索引，扫描了59548个文档。 为了进一步提高查询效率，可以再对”province”字段建立索引： db.phonehomes.ensureIndex({&quot;province&quot;: 1}) 再次执行： db.phonehomes.find({province: &apos;福建&apos;, &apos;isp&apos;: &apos;中国电信&apos;}).explain() 返回结果： { &quot;cursor&quot; : &quot;BtreeCursor province_1&quot;, &quot;isMultiKey&quot; : false, &quot;n&quot; : 2667, &quot;nscannedObjects&quot; : 10223, &quot;nscanned&quot; : 10223, &quot;nscannedObjectsAllPlans&quot; : 10324, &quot;nscannedAllPlans&quot; : 10425, &quot;scanAndOrder&quot; : false, &quot;indexOnly&quot; : false, &quot;nYields&quot; : 81, &quot;nChunkSkips&quot; : 0, &quot;millis&quot; : 13, &quot;indexBounds&quot; : { &quot;province&quot; : [ [ &quot;福建&quot;, &quot;福建&quot; ] ] }, &quot;server&quot; : &quot;zhangjinyideMac-Pro.local:27017&quot;, &quot;filterSet&quot; : false, // 略去一部分暂时不讨论的内容 } 可以发现一个有意思的现象，我们同时拥有province和isp字段的单独索引，但是这个查询用了province的索引而不使用isp的索引。同时，扫描的文档数只有10223个，这比使用isp索引扫描的59548个文档要少。 使用province索引效率高于使用isp索引的原因是，这个集合中的包含的省份数为31个(部分地区未收入)，isp为4个(中国移动、中国联通、中国电信和虚拟运营商)，因此province对文档的区分度大于isp。 这两种情况的具体过程如下： 使用isp索引 先用isp索引，获取到isp为”中国电信”的文档(59548个)，然后再对这部分文档做扫描，筛选出province为”福建”的所有文档(2667个)。 使用province索引 先用province索引，获取到province为”福建”的文档(10223个)，然后再对这部分文档做扫描，筛选出isp为”中国电信”的所有文档(2667个)。 对比一下就知道，用isp索引要比用province索引多扫描4W+个文档(这里忽略了用索引筛选文档的代价，因为这个代价相比扫描大量文档要小得多)。 MongoDB会自动province索引的原因，个人猜测是MongoDB在真正执行查询时会现有一个预执行阶段，会先分析这个查询使用哪个索引最高效。 (5) 使用联合索引刚才都是用单独索引，现在要介绍联合索引。顾名思义，联合索引使用多个字段作为索引。 我们先把刚才建的索引删除： db.phonehomes.dropIndex({&quot;province&quot;:1}) db.phonehomes.dropIndex({&quot;isp&quot;:1}) 建立一个province和isp的联合索引： db.phonehomes.ensureIndex({&quot;province&quot;: 1, &quot;isp&quot;: 1}) 再次执行刚才那个查询： db.phonehomes.find({province: &apos;福建&apos;, &apos;isp&apos;: &apos;中国电信&apos;}).explain() 返回结果： { &quot;cursor&quot; : &quot;BtreeCursor province_1_isp_1&quot;, &quot;isMultiKey&quot; : false, &quot;n&quot; : 2667, &quot;nscannedObjects&quot; : 2667, &quot;nscanned&quot; : 2667, &quot;nscannedObjectsAllPlans&quot; : 2667, &quot;nscannedAllPlans&quot; : 2667, &quot;scanAndOrder&quot; : false, &quot;indexOnly&quot; : false, &quot;nYields&quot; : 20, &quot;nChunkSkips&quot; : 0, &quot;millis&quot; : 3, &quot;indexBounds&quot; : { &quot;province&quot; : [ [ &quot;福建&quot;, &quot;福建&quot; ] ], &quot;isp&quot; : [ [ &quot;中国电信&quot;, &quot;中国电信&quot; ] ] }, &quot;server&quot; : &quot;zhangjinyideMac-Pro.local:27017&quot;, &quot;filterSet&quot; : false, // 略去一部分暂时不讨论的内容 } 建立了province和isp的联合索引后，查询分析的”cursor”为”BtreeCursor province_1_isp_1”,即使用了这个联合索引，其他数据也表现了此索引在这个查询上的高效： n&quot; : 2667, &quot;nscannedObjects&quot; : 2667, &quot;nscanned&quot; : 2667, &quot;nscannedObjectsAllPlans&quot; : 2667, &quot;nscannedAllPlans&quot; : 2667, 就算改变查询条件的顺序也没关系： db.phonehomes.find({&apos;isp&apos;: &apos;中国电信&apos;, province: &apos;福建&apos;}).explain() 返回结果： { &quot;cursor&quot; : &quot;BtreeCursor province_1_isp_1&quot;, &quot;isMultiKey&quot; : false, &quot;n&quot; : 2667, &quot;nscannedObjects&quot; : 2667, &quot;nscanned&quot; : 2667, &quot;nscannedObjectsAllPlans&quot; : 2667, &quot;nscannedAllPlans&quot; : 2667, &quot;scanAndOrder&quot; : false, &quot;indexOnly&quot; : false, &quot;nYields&quot; : 20, &quot;nChunkSkips&quot; : 0, &quot;millis&quot; : 2, &quot;indexBounds&quot; : { &quot;province&quot; : [ [ &quot;福建&quot;, &quot;福建&quot; ] ], &quot;isp&quot; : [ [ &quot;中国电信&quot;, &quot;中国电信&quot; ] ] }, &quot;server&quot; : &quot;zhangjinyideMac-Pro.local:27017&quot;, &quot;filterSet&quot; : false， // 略去一部分暂时不讨论的内容 } 由于我们刚才删除了province和isp的单独索引，所以我们要来实验一下，如果使用单个字段查询，能否利用到联合索引。 先执行查询： db.phonehomes.find({province: &apos;福建&apos;}).explain() 返回结果： { &quot;cursor&quot; : &quot;BtreeCursor province_1_isp_1&quot;, &quot;isMultiKey&quot; : false, &quot;n&quot; : 10223, &quot;nscannedObjects&quot; : 10223, &quot;nscanned&quot; : 10223, &quot;nscannedObjectsAllPlans&quot; : 10223, &quot;nscannedAllPlans&quot; : 10223, &quot;scanAndOrder&quot; : false, &quot;indexOnly&quot; : false, &quot;nYields&quot; : 79, &quot;nChunkSkips&quot; : 0, &quot;millis&quot; : 9, &quot;indexBounds&quot; : { &quot;province&quot; : [ [ &quot;福建&quot;, &quot;福建&quot; ] ], &quot;isp&quot; : [ [ { &quot;$minElement&quot; : 1 }, { &quot;$maxElement&quot; : 1 } ] ] }, &quot;server&quot; : &quot;zhangjinyideMac-Pro.local:27017&quot;, &quot;filterSet&quot; : false, // 略去一部分暂时不讨论的内容 } 可以发现这个查询使用了province_1_isp_1联合索引： &quot;cursor&quot; : &quot;BtreeCursor province_1_isp_1&quot; 再执行： db.phonehomes.find({isp: &apos;中国电信&apos;}).explain() 返回结果： { &quot;cursor&quot; : &quot;BasicCursor&quot;, &quot;isMultiKey&quot; : false, &quot;n&quot; : 59548, &quot;nscannedObjects&quot; : 327664, &quot;nscanned&quot; : 327664, &quot;nscannedObjectsAllPlans&quot; : 327664, &quot;nscannedAllPlans&quot; : 327664, &quot;scanAndOrder&quot; : false, &quot;indexOnly&quot; : false, &quot;nYields&quot; : 2559, &quot;nChunkSkips&quot; : 0, &quot;millis&quot; : 106, &quot;server&quot; : &quot;zhangjinyideMac-Pro.local:27017&quot;, &quot;filterSet&quot; : false } 然而，这个查询并没有使用任何索引，而是来了个全表扫描： &quot;cursor&quot; : &quot;BasicCursor&quot; 这是怎么回事，难道用单独用isp做查询条件就不能使用province_1_isp_1联合索引吗？ 对于联合索引来说，确实能为某些查询提供索引支持，但这要看是什么查询。全字段满足的查询(查询字段顺序无关)肯定是可以使用相应的联合索引的，这点毋庸置疑，刚才也看到了实例。那究竟怎么利用联合索引呢，在给出答案前我们再看一个例子。 这个例子需要建立一个province-city-isp的联合索引 ： db.phonehomes.ensureIndex({&quot;province&quot;: 1, &quot;city&quot;: 1, &quot;isp&quot;: 1}) 然后分别执行4个查询： db.phonehomes.find({‘province’: ‘福建’, ‘isp’: ‘中国电信’}).explain() db.phonehomes.find({‘isp’: ‘中国电信’, ‘province’: ‘福建’}).explain() db.phonehomes.find({‘city’: ‘厦门’, ‘isp’: ‘中国电信’}).explain() db.phonehomes.find({‘isp’: ‘中国电信’, ‘city’: ‘厦门’}).explain() db.phonehomes.find({‘province’: ‘福建’, ‘city’: ‘厦门’}).explain() db.phonehomes.find({‘city’: ‘厦门’, ‘province’: ‘福建’}).explain() 然后我们只考察”cursor”字段。 第一个查询的”cursor”为： &quot;cursor&quot;: &quot;BtreeCursor province_1_city_1_isp_1&quot; 第二个查询的”cursor”为： &quot;cursor&quot;: &quot;BtreeCursor province_1_city_1_isp_1&quot; 第三个查询的”cursor”为： &quot;cursor&quot;: &quot;BasicCursor&quot; 第四个查询的”cursor”为： &quot;cursor&quot;: &quot;BasicCursor&quot; 第五个查询的”cursor”为： &quot;cursor&quot;: &quot;BtreeCursor province_1_city_1_isp_1&quot; 第六个查询的”cursor”为： &quot;cursor&quot;: &quot;BtreeCursor province_1_city_1_isp_1&quot; 仔细观察可以发现几个规律： 在字段相同的查询中，使用索引的情况和查询中字段摆放的顺序无关(参看1和2、3和4、5和6做对比)。 MongoDB中，一个给定的联合索引能否被某个查询使用，要看这个查询中字段是否满足”最左前缀匹配”。具体来说就是，当查询条件精确匹配索引的最左边连续或不连续的几个列时，该查询可以使用索引。 其中第一项很好理解，主要是第二项。 在上面第1和第2个查询中，查询条件为(查询字段顺序无关)： {&apos;province&apos;: &apos;福建&apos;, &apos;isp&apos;: &apos;中国电信&apos;} 这满足了province_1_city_1_isp_1联合索引的”最左前缀匹配”原则(虽然并不是连续的，少了中间的city列)。 在上面第3和第4个查询中，查询条件为(查询字段顺序无关)： {&apos;city&apos;: &apos;厦门&apos;, &apos;isp&apos;: &apos;中国电信&apos;} 这不满足province_1_city_1_isp_1联合索引的”最左前缀匹配”原则，因为并没有匹配到最左边的province列。 在上面第5和第6个查询中，查询条件为(查询字段顺序无关)： {&apos;province&apos;: &apos;福建&apos;, &apos;city&apos;: &apos;厦门&apos;} 这满足了province_1_city_1_isp_1联合索引的”最左前缀匹配”原则(是连续的) 因此可以总结MongoDB中联合索引的使用方法：在MongoDB中，一个给定的联合索引能否被某个查询使用，要看这个查询中字段是否满足”最左前缀匹配”。具体来说就是，当查询条件精确匹配索引的最左边连续或不连续的几个列时，该查询可以使用索引。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://nullcc.github.io/categories/数据库/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://nullcc.github.io/tags/MongoDB/"},{"name":"数据库索引","slug":"数据库索引","permalink":"https://nullcc.github.io/tags/数据库索引/"}]},{"title":"Python中金额计算的小问题","slug":"python中金额计算的小问题","date":"2017-05-09T16:00:00.000Z","updated":"2022-04-15T03:41:13.026Z","comments":true,"path":"2017/05/10/python中金额计算的小问题/","link":"","permalink":"https://nullcc.github.io/2017/05/10/python中金额计算的小问题/","excerpt":"由于二进制对浮点运算存在精度问题，所以一些浮点计算经常会出现以下情况：","text":"由于二进制对浮点运算存在精度问题，所以一些浮点计算经常会出现以下情况： 12345# -*- coding: utf-8 -*-a = 1b = 0.9print(a-b) 结果： 10.09999999999999998 我们期望的结果应该是0.1。为了解决这个问题，可以引入python的decimal库： 12345678# -*- coding: utf-8 -*-from decimal import getcontext, Decimalgetcontext().prec = 10a = 1b = 0.9print(Decimal(a)-Decimal(b)) 结果： 10.1000000000 getcontext().prec = 10把精度设置为10位，注意不是小数点后的位数，而是整个数字的位数。如果需要去掉后面的0，需要用float()转换一下。在具体的计算中，还需要用Decimal包装计算的所有数字。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://nullcc.github.io/tags/Python/"}]},{"title":"node.js的C++扩展入门","slug":"node.js的C++扩展入门","date":"2017-05-09T16:00:00.000Z","updated":"2022-04-15T03:41:13.025Z","comments":true,"path":"2017/05/10/node.js的C++扩展入门/","link":"","permalink":"https://nullcc.github.io/2017/05/10/node.js的C++扩展入门/","excerpt":"声明：本文主要翻译自node.js官方API：C++ Addons。部分解释为作者自己添加。","text":"声明：本文主要翻译自node.js官方API：C++ Addons。部分解释为作者自己添加。 编程环境： 操作系统 Mac OS X 10.9.5 node.js v4.4.2 npm v3.9.2 本文将介绍node.js中编写C++扩展的入门知识。 1. 基本知识介绍在node.js中，除了用js写代码以外，还可以使用C++编写扩展，这有点类似DLL，动态链接进js代码中。使用上也相当方便，只需用require包含，这和一般的js模块并没有什么区别。C++扩展为js和C++代码的通信提供了一个接口。 要编写node.js的C++扩展，需要了解一些基本知识： V8： Google出品的大名鼎鼎的V8引擎，它实际上是一个C++类库，用来和 JavaScript 交互，比如创建对象，调用函数等等。V8的API大部分都声明在v8.h头文件中。 libuv：一个C实现的事件循环库，node.js使用libuv来实现自己的事件循环、工作线程和所有的异步行为。它是一个跨平台的，高度抽象的lib，提供了简单易用的、POSIX-like的方式来让操作系统和系统任务进行交互。比如和文件系统、sockets、定时器和系统事件。libuv还提供了POSIX threads线程级别的抽象来增强标准事件循环中不具备的复杂异步能力。我们鼓励C++扩展的作者思考如何通过转换I/O或其他耗时操作到非阻塞系统操作来避免阻塞事件循环。 node.js内部lib，node.js本身提供了很多C/C++ API来给扩展使用，比如最重要的一个：node::ObjectWrap类。 node.js包含了很多静态链接库，比如OpenSSL。这些库都放在node.js代码树的deps/目录下。只有V8和OpenSSL标识符被有意地被node.js重复导出来被各种扩展使用。 下面快速地来看一个实例。 2. 第一个例子Hello下面的例子是一个简单的C++扩展，其功能相当于js的如下代码： module.exports.hello = () =&gt; &apos;world&apos;; 首先创建一个hello.cc： 12345678910111213141516171819202122// hello.cc#include &lt;node.h&gt;namespace demo &#123; using v8::FunctionCallbackInfo; using v8::Isolate; using v8::Local; using v8::Object; using v8::String; using v8::Value; void Method(const FunctionCallbackInfo&lt;Value&gt;&amp; args) &#123; Isolate* isolate = args.GetIsolate(); args.GetReturnValue().Set(String::NewFromUtf8(isolate, \"world\")); &#125; void init(Local&lt;Object&gt; exports) &#123; NODE_SET_METHOD(exports, \"hello\", Method); &#125; NODE_MODULE(addon, init)&#125; // namespace demo 这个最简单的例子，已经出现了一些我们完全没有接触过的东西。大致解释一下： 函数Method的参数类型是FunctionCallbackInfo&lt;Value&gt;&amp;。 Isolate，英文意思是“隔离”，在这里Isolate指的是一个独立的V8 runtime，可以理解为一个独立的V8执行环境，它包括了自己的堆管理器、GC等组件。后续的很多操作都要依赖于这个Isolate，后面我们会看到在很多操作中，都会使用Isolate的实例作为一个上下文传入。(注：一个给定的Isolate在同一时间只能被一个线程访问，但如果有多个不同的Isolate，就可以给多个线程同时访问。不过，一个Isolate还不足以运行脚本，你还需要一个全局对象，一个执行上下文通过指定一个全局对象来定义一个完整的脚本执行环境。因此，可以有多个执行上下文存在于一个Isolate中，而且它们还可以简单安全地共享它们的全局对象。这是因为这个全局对象实际上属于Isolate，而却这个全局对象被Isolate的互斥锁保护着。) 返回值需要用args.GetReturnValue().Set()来设置。 向外导出方法需要在扩展的初始化函数中使用NODE_SET_METHOD(exports, Method_Name, Method);。如果有多个方法需要导出，就写多个NODE_SET_METHOD。 注意到node.js的C++扩展都必须按以下形式导出一个初始化函数(该函数名字可以随便设置一个)： void Initialize(Local&lt;Object&gt; exports); NODE_MODULE(module_name, Initialize) NODE_MODULE这行后面并没有分号(;)，因为它并不是一个函数，你可以认为这是一个声明。module_name必须匹配最后生成的二进制文件的文件名(不包括.node后缀)。在hello.cc这个例子中，初始化函数是init，扩展模块名是addon。 构建(Building) 写好源代码后我们就要把它编译成二进制的addon.node文件了。binding.gyp文件用来描述我们模块的构建配置，这个文件的内容是JSON形式的： 12345678&#123; \"targets\": [ &#123; \"target_name\": \"addon\", \"sources\": [ \"hello.cc\" ] &#125; ]&#125; 实施构建操作需要用到node-gyp，如果尚未安装的话，需要运行(可能要用到sudo)： npm install -g node-gyp 来全局安装node-gyp。 编写完binding.gyp文件，我们使用： node-gyp configure 来生成对应项目在当前平台的build目录。这将会在build目录下生成一个Makefile(Unix-like系统)或者一个vcxproj文件(Windows系统)还有一部分其他文件。 接着，运行： node-gyp build 来生成一个编译过的addon.node文件，这个文件会被放在build/Release/目录下。 build成功后，这个二进制的C++扩展就可以在node.js中使用require包含进来： // hello.js const addon = require(&apos;./build/Release/addon&apos;); console.log(addon.hello()); // &apos;world&apos; 由于扩展的二进制文件的存放位置会根据编译方式不同而变化(有可能放在build/Debug/目录)，所以可以用这种方式来引入扩展： 12345try &#123; return require('./build/Release/addon.node');&#125; catch (err) &#123; return require('./build/Debug/addon.node');&#125; 但是个人觉得这种引入方式很奇怪，在能保证正确性的情况下，如果是开发模式，用Debug目录下的，生产模式用Release下的。 链接node.js依赖node.js使用一些静态链接库，比如V8、libuv和OpenSSL。所有扩展都必须链接V8，还有可能需要链接一些其他的库。典型情况下，使用#include &lt;…&gt;来include这些库(比如链接V8就是#include &lt;v8.h&gt;)，node-gyp会自动找到这些库。然而，有几个注意事项需要说明： node-gyp运行时，它会检测node.js的版本并且下载全部源码文件或者只是下载头文件。如果下载了全部源码文件，扩展就可以使用node.js的所有依赖，如果仅仅下载了头文件，则只有node.js导出的那些东西可以被使用。 node-gyp可以使用–nodedir选项来指定本地node.js映像，使用这个选项时，扩展可以使用全部的node.js依赖。 使用require加载C++扩展经过编译的node.js C++扩展的后缀名是.node(类似.so和.dll)，require()函数会查找这些.node文件并像初始化动态链接库那样初始化它们。 当使用reqiure()时，.node后缀可以被省略。需要注意的是，node.js在使用reqiure()加载模块时，会优先加载js后缀的文件。比如说一个目录下有一个addon.js和一个addon.node，当使用require(‘addon’)时，node.js会优先加载addon.js。 3.对node.js的原生抽象(这个暂略)4.第二个例子以下的几个例子的binding.gyp都使用： 12345678&#123; \"targets\": [ &#123; \"target_name\": \"addon\", \"sources\": [ \"addon.cc\" ] &#125; ]&#125; 如果有多于一个的C++文件，可以把所有文件放在sources数组中： &quot;sources&quot;: [&quot;addon.cc&quot;, &quot;myexample.cc&quot;] 写好binding.gyp后，可以使用以下命令来一次性地配置和构建C++扩展： node-gyp configure build 函数参数C++扩展可以暴露函数和对象出来让node.js访问。当从js中调用C++扩展中的函数时，入参和返回值必须映射到C/C++事先声明好的代码中。 以下代码展示了C++扩展代码如何读取从js传递过来的函数入参和如何返回值： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// addon.cc#include &lt;node.h&gt;namespace demo &#123; using v8::Exception; using v8::FunctionCallbackInfo; using v8::Isolate; using v8::Local; using v8::Number; using v8::Object; using v8::String; using v8::Value; // This is the implementation of the \"add\" method // Input arguments are passed using the // const FunctionCallbackInfo&lt;Value&gt;&amp; args struct void Add(const FunctionCallbackInfo&lt;Value&gt;&amp; args) &#123; Isolate* isolate = args.GetIsolate(); // Check the number of arguments passed. if (args.Length() &lt; 2) &#123; // Throw an Error that is passed back to JavaScript isolate-&gt;ThrowException(Exception::TypeError( String::NewFromUtf8(isolate, \"Wrong number of arguments\"))); return; &#125; // Check the argument types if (!args[0]-&gt;IsNumber() || !args[1]-&gt;IsNumber()) &#123; isolate-&gt;ThrowException(Exception::TypeError( String::NewFromUtf8(isolate, \"Wrong arguments\"))); return; &#125; // Perform the operation double value = args[0]-&gt;NumberValue() + args[1]-&gt;NumberValue(); Local&lt;Number&gt; num = Number::New(isolate, value); // Set the return value (using the passed in // FunctionCallbackInfo&lt;Value&gt;&amp;) args.GetReturnValue().Set(num); &#125; void Init(Local&lt;Object&gt; exports) &#123; NODE_SET_METHOD(exports, \"add\", Add); &#125; NODE_MODULE(addon, Init)&#125; // namespace demo 编译成功后，这个扩展可以被node.js使用require()包含并使用： 123// test.jsconst addon = require('./build/Release/addon');console.log('This should be eight:', addon.add(3, 5)); 回调函数一种很常见的做法是从js传递回调函数给C++调用，下面这个示例展示了如何做： 1234567891011121314151617181920212223242526272829// addon.cc#include &lt;node.h&gt;namespace demo &#123; using v8::Function; using v8::FunctionCallbackInfo; using v8::Isolate; using v8::Local; using v8::Null; using v8::Object; using v8::String; using v8::Value; void RunCallback(const FunctionCallbackInfo&lt;Value&gt;&amp; args) &#123; Isolate* isolate = args.GetIsolate(); Local&lt;Function&gt; cb = Local&lt;Function&gt;::Cast(args[0]); const unsigned argc = 1; Local&lt;Value&gt; argv[argc] = &#123; String::NewFromUtf8(isolate, \"hello world\") &#125;; cb-&gt;Call(Null(isolate), argc, argv); &#125; void Init(Local&lt;Object&gt; exports, Local&lt;Object&gt; module) &#123; NODE_SET_METHOD(module, \"exports\", RunCallback); &#125; NODE_MODULE(addon, Init)&#125; // namespace demo 解释： 传递回调函数，其实和传递普通参数没什么大的区别，使用 Local cb = Local::Cast(args[0]); 可以获得这个回调函数。然后需要显式声明这个回调函数的参数个数和参数数组： const unsigned argc = 1; Local&lt;Value&gt; argv[argc] = { String::NewFromUtf8(isolate, &quot;hello world&quot;) }; 调用这个回调函数需要传入isolate、参数个数argc、参数数组argv： cb-&gt;Call(Null(isolate), argc, argv); Init函数和之前有点不同，上面这个扩展的Init()使用了两个参数的形式(之前都是单参数)，其中第二个参数接受一个module对象： void Init(Local exports, Local module) { NODE_SET_METHOD(module, &quot;exports&quot;, RunCallback); // 相当于直接导出整个模块作为方法 } 这将允许扩展使用单个函数的形式代替之前往exports中添加函数作为属性的方式来完全地重写exports。因此可以直接用扩展的名字作为函数名来调用，这适用于此扩展只对外暴露一个方法的情况： 12345// test.jsconst addon = require('./build/Release/addon');addon((msg) =&gt; &#123; console.log(msg); // 'hello world'&#125;); 作为演示，在这个示例中只是同步地调用回调函数。 对象工厂在下面的示例中，扩展可以使用C++创建并返回新对象。下面的例子中，createObject()函数接受一个string类型的参数，然后创建一个一模一样的string，并在一个对象的msg属性中返回这个string： 12345678910111213141516171819202122232425262728// addon.cc#include &lt;node.h&gt;namespace demo &#123; using v8::FunctionCallbackInfo; using v8::Isolate; using v8::Local; using v8::Object; using v8::String; using v8::Value; void CreateObject(const FunctionCallbackInfo&lt;Value&gt;&amp; args) &#123; Isolate* isolate = args.GetIsolate(); Local&lt;Object&gt; obj = Object::New(isolate); obj-&gt;Set(String::NewFromUtf8(isolate, \"msg\"), args[0]-&gt;ToString()); args.GetReturnValue().Set(obj); &#125; void Init(Local&lt;Object&gt; exports, Local&lt;Object&gt; module) &#123; NODE_SET_METHOD(module, \"exports\", CreateObject); &#125; NODE_MODULE(addon, Init)&#125; // namespace demo 解释： 创建一个新对象，也需要把isolate作为参数传入并设置对象属性msg为第一个入参： Local obj = Object::New(isolate); obj-&gt;Set(String::NewFromUtf8(isolate, “msg”), args[0]-&gt;ToString()); Init函数中导出CreateObject作为模块函数。 测试上面扩展的js代码： 123456// test.jsconst addon = require('./build/Release/addon');var obj1 = addon('hello');var obj2 = addon('world');console.log(obj1.msg + ' ' + obj2.msg); // 'hello world' 函数工厂还有一种常见的行为是创建包装了C++函数的js函数，并返回给js： 1234567891011121314151617181920212223242526272829303132333435363738// addon.cc#include &lt;node.h&gt;namespace demo &#123; using v8::Function; using v8::FunctionCallbackInfo; using v8::FunctionTemplate; using v8::Isolate; using v8::Local; using v8::Object; using v8::String; using v8::Value; void MyFunction(const FunctionCallbackInfo&lt;Value&gt;&amp; args) &#123; Isolate* isolate = args.GetIsolate(); args.GetReturnValue().Set(String::NewFromUtf8(isolate, \"hello world\")); &#125; void CreateFunction(const FunctionCallbackInfo&lt;Value&gt;&amp; args) &#123; Isolate* isolate = args.GetIsolate(); Local&lt;FunctionTemplate&gt; tpl = FunctionTemplate::New(isolate, MyFunction); Local&lt;Function&gt; fn = tpl-&gt;GetFunction(); // omit this to make it anonymous fn-&gt;SetName(String::NewFromUtf8(isolate, \"theFunction\")); args.GetReturnValue().Set(fn); &#125; void Init(Local&lt;Object&gt; exports, Local&lt;Object&gt; module) &#123; NODE_SET_METHOD(module, \"exports\", CreateFunction); &#125; NODE_MODULE(addon, Init)&#125; // namespace demo 解释： CreateFunction中使用v8::FunctionTemplate创建函数模板(传入参数MyFunction)，并创建一个函数，其中函数命名是可选的： Local tpl = FunctionTemplate::New(isolate, MyFunction); Local fn = tpl-&gt;GetFunction(); // omit this to make it anonymous fn-&gt;SetName(String::NewFromUtf8(isolate, “theFunction”)); 测试一下： 12345// test.jsconst addon = require('./build/Release/addon');var fn = addon();console.log(fn()); // 'hello world' 包装C++对象还可以使用js的new操作符创建由C++包装的对象或类： 123456789101112131415// addon.cc#include &lt;node.h&gt;#include \"myobject.h\"namespace demo &#123; using v8::Local; using v8::Object; void InitAll(Local&lt;Object&gt; exports) &#123; MyObject::Init(exports); &#125; NODE_MODULE(addon, InitAll)&#125; // namespace demo 在上面的myobject.h中，包装类继承自node::ObjectWrap： 12345678910111213141516171819202122232425// myobject.h#ifndef MYOBJECT_H#define MYOBJECT_H#include &lt;node.h&gt;#include &lt;node_object_wrap.h&gt;namespace demo &#123; class MyObject : public node::ObjectWrap &#123; public: static void Init(v8::Local&lt;v8::Object&gt; exports); private: explicit MyObject(double value = 0); ~MyObject(); static void New(const v8::FunctionCallbackInfo&lt;v8::Value&gt;&amp; args); static void PlusOne(const v8::FunctionCallbackInfo&lt;v8::Value&gt;&amp; args); static v8::Persistent&lt;v8::Function&gt; constructor; double value_; &#125;;&#125; // namespace demo#endif 在myobject.cc中，实现了那些被暴露出去的方法。下面的代码通过把plusOne()添加到构造函数的prototype来暴露它： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071// myobject.cc#include \"myobject.h\"namespace demo &#123; using v8::Context; using v8::Function; using v8::FunctionCallbackInfo; using v8::FunctionTemplate; using v8::Isolate; using v8::Local; using v8::Number; using v8::Object; using v8::Persistent; using v8::String; using v8::Value; Persistent&lt;Function&gt; MyObject::constructor; MyObject::MyObject(double value) : value_(value) &#123; &#125; MyObject::~MyObject() &#123; &#125; void MyObject::Init(Local&lt;Object&gt; exports) &#123; Isolate* isolate = exports-&gt;GetIsolate(); // Prepare constructor template Local&lt;FunctionTemplate&gt; tpl = FunctionTemplate::New(isolate, New); tpl-&gt;SetClassName(String::NewFromUtf8(isolate, \"MyObject\")); tpl-&gt;InstanceTemplate()-&gt;SetInternalFieldCount(1); // Prototype NODE_SET_PROTOTYPE_METHOD(tpl, \"plusOne\", PlusOne); constructor.Reset(isolate, tpl-&gt;GetFunction()); exports-&gt;Set(String::NewFromUtf8(isolate, \"MyObject\"), tpl-&gt;GetFunction()); &#125; void MyObject::New(const FunctionCallbackInfo&lt;Value&gt;&amp; args) &#123; Isolate* isolate = args.GetIsolate(); if (args.IsConstructCall()) &#123; // Invoked as constructor: `new MyObject(...)` double value = args[0]-&gt;IsUndefined() ? 0 : args[0]-&gt;NumberValue(); MyObject* obj = new MyObject(value); obj-&gt;Wrap(args.This()); args.GetReturnValue().Set(args.This()); &#125; else &#123; // Invoked as plain function `MyObject(...)`, turn into construct call. const int argc = 1; Local&lt;Value&gt; argv[argc] = &#123; args[0] &#125;; Local&lt;Context&gt; context = isolate-&gt;GetCurrentContext(); Local&lt;Function&gt; cons = Local&lt;Function&gt;::New(isolate, constructor); Local&lt;Object&gt; result = cons-&gt;NewInstance(context, argc, argv).ToLocalChecked(); args.GetReturnValue().Set(result); &#125; &#125; void MyObject::PlusOne(const FunctionCallbackInfo&lt;Value&gt;&amp; args) &#123; Isolate* isolate = args.GetIsolate(); MyObject* obj = ObjectWrap::Unwrap&lt;MyObject&gt;(args.Holder()); obj-&gt;value_ += 1; args.GetReturnValue().Set(Number::New(isolate, obj-&gt;value_)); &#125;&#125; // namespace demo 解释： 在MyObject::Init中，使用v8::FunctionTemplate创建一个函数模板(传入参数New)，并给这个模板设置一个类名MyObject，SetInternalFieldCount用来设定类的内部储存多少个内部变量，这里是1： Local tpl = FunctionTemplate::New(isolate, New); tpl-&gt;SetClassName(String::NewFromUtf8(isolate, “MyObject”)); tpl-&gt;InstanceTemplate()-&gt;SetInternalFieldCount(1); 然后使用： NODE_SET_PROTOTYPE_METHOD(tpl, &quot;plusOne&quot;, PlusOne); 来设置prototype中的plusOne方法。 代码： constructor.Reset(isolate, tpl-&gt;GetFunction()); exports-&gt;Set(String::NewFromUtf8(isolate, &quot;MyObject&quot;), tpl-&gt;GetFunction()); 第一行相当于js中的 XXX.prototype.constructor = XXX; 然后导出这个MyObject类。 在MyObject::New中，情况略微复杂一些。首先判断是否是构造调用(使用js中的new操作符)，如果是构造调用，运行以下代码： MyObject* obj = new MyObject(value); 来new一个MyObject实例，value是构造入参，然后返回这个实例。 js中的函数如果不是构造调用就是普通的函数调用。 在MyObject::PlusOne中，通过以下代码获取MyObject实例： MyObject* obj = ObjectWrap::Unwrap(args.Holder()); obj-&gt;value_ += 1; 然后返回加1后的数值结果。 为了构建这个例子，需要把myobject.cc加入binding.gyp： 1234567891011&#123; \"targets\": [ &#123; \"target_name\": \"addon\", \"sources\": [ \"addon.cc\", \"myobject.cc\" ] &#125; ]&#125; 测试： 1234567// test.jsconst addon = require('./build/Release/addon');var obj = new addon.MyObject(10);console.log(obj.plusOne()); // 11console.log(obj.plusOne()); // 12console.log(obj.plusOne()); // 13 包装对象工厂另外，还可以使用工厂模式来避免显式使用new操作符创建对象实例： 123var obj = addon.createObject();// instead of:// var obj = new addon.Object(); 首先，需要在addon.cc中实现createObject()方法： 12345678910111213141516171819202122232425// addon.cc#include &lt;node.h&gt;#include \"myobject.h\"namespace demo &#123; using v8::FunctionCallbackInfo; using v8::Isolate; using v8::Local; using v8::Object; using v8::String; using v8::Value; void CreateObject(const FunctionCallbackInfo&lt;Value&gt;&amp; args) &#123; MyObject::NewInstance(args); &#125; void InitAll(Local&lt;Object&gt; exports, Local&lt;Object&gt; module) &#123; MyObject::Init(exports-&gt;GetIsolate()); NODE_SET_METHOD(module, \"exports\", CreateObject); &#125; NODE_MODULE(addon, InitAll)&#125; // namespace demo 在myobject.h中，加入静态方法NewInstance()来处理实例化对象的操作，我们将用NewInstance()替代js的new操作符： 123456789101112131415161718192021222324252627// myobject.h#ifndef MYOBJECT_H#define MYOBJECT_H#include &lt;node.h&gt;#include &lt;node_object_wrap.h&gt;namespace demo &#123; class MyObject : public node::ObjectWrap &#123; public: static void Init(v8::Isolate* isolate); static void NewInstance(const v8::FunctionCallbackInfo&lt;v8::Value&gt;&amp; args); private: explicit MyObject(double value = 0); ~MyObject(); static void New(const v8::FunctionCallbackInfo&lt;v8::Value&gt;&amp; args); static void PlusOne(const v8::FunctionCallbackInfo&lt;v8::Value&gt;&amp; args); static v8::Persistent&lt;v8::Function&gt; constructor; double value_; &#125;;&#125; // namespace demo#endif myobject.cc中的实现和前面差不多： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182// myobject.cc#include &lt;node.h&gt;#include \"myobject.h\"namespace demo &#123; using v8::Context; using v8::Function; using v8::FunctionCallbackInfo; using v8::FunctionTemplate; using v8::Isolate; using v8::Local; using v8::Number; using v8::Object; using v8::Persistent; using v8::String; using v8::Value; Persistent&lt;Function&gt; MyObject::constructor; MyObject::MyObject(double value) : value_(value) &#123; &#125; MyObject::~MyObject() &#123; &#125; void MyObject::Init(Isolate* isolate) &#123; // Prepare constructor template Local&lt;FunctionTemplate&gt; tpl = FunctionTemplate::New(isolate, New); tpl-&gt;SetClassName(String::NewFromUtf8(isolate, \"MyObject\")); tpl-&gt;InstanceTemplate()-&gt;SetInternalFieldCount(1); // Prototype NODE_SET_PROTOTYPE_METHOD(tpl, \"plusOne\", PlusOne); constructor.Reset(isolate, tpl-&gt;GetFunction()); &#125; void MyObject::New(const FunctionCallbackInfo&lt;Value&gt;&amp; args) &#123; Isolate* isolate = args.GetIsolate(); if (args.IsConstructCall()) &#123; // Invoked as constructor: `new MyObject(...)` double value = args[0]-&gt;IsUndefined() ? 0 : args[0]-&gt;NumberValue(); MyObject* obj = new MyObject(value); obj-&gt;Wrap(args.This()); args.GetReturnValue().Set(args.This()); &#125; else &#123; // Invoked as plain function `MyObject(...)`, turn into construct call. const int argc = 1; Local&lt;Value&gt; argv[argc] = &#123; args[0] &#125;; Local&lt;Function&gt; cons = Local&lt;Function&gt;::New(isolate, constructor); Local&lt;Context&gt; context = isolate-&gt;GetCurrentContext(); Local&lt;Object&gt; instance = cons-&gt;NewInstance(context, argc, argv).ToLocalChecked(); args.GetReturnValue().Set(instance); &#125; &#125; void MyObject::NewInstance(const FunctionCallbackInfo&lt;Value&gt;&amp; args) &#123; Isolate* isolate = args.GetIsolate(); const unsigned argc = 1; Local&lt;Value&gt; argv[argc] = &#123; args[0] &#125;; Local&lt;Function&gt; cons = Local&lt;Function&gt;::New(isolate, constructor); Local&lt;Context&gt; context = isolate-&gt;GetCurrentContext(); Local&lt;Object&gt; instance = cons-&gt;NewInstance(context, argc, argv).ToLocalChecked(); args.GetReturnValue().Set(instance); &#125; void MyObject::PlusOne(const FunctionCallbackInfo&lt;Value&gt;&amp; args) &#123; Isolate* isolate = args.GetIsolate(); MyObject* obj = ObjectWrap::Unwrap&lt;MyObject&gt;(args.Holder()); obj-&gt;value_ += 1; args.GetReturnValue().Set(Number::New(isolate, obj-&gt;value_)); &#125;&#125; // namespace demo 解释： 这个例子和之前那个差不太多，只不过在扩展中提供了CreateObject()工厂方法来创建MyObject实例，CreateObject()在内部又使用MyObject::NewInstance()来创建对象。 再强调一次，为了构建这个例子，需要把myobject.cc加入binding.gyp： 1234567891011&#123; \"targets\": [ &#123; \"target_name\": \"addon\", \"sources\": [ \"addon.cc\", \"myobject.cc\" ] &#125; ]&#125; 测试： 123456789101112// test.jsconst createObject = require('./build/Release/addon');var obj = createObject(10);console.log(obj.plusOne()); // 11console.log(obj.plusOne()); // 12console.log(obj.plusOne()); // 13var obj2 = createObject(20);console.log(obj2.plusOne()); // 21console.log(obj2.plusOne()); // 22console.log(obj2.plusOne()); // 23 传递包装对象为了进一步包装和返回C++对象，可以利用node.js的helper函数node::ObjectWrap::Unwrap来展开包装对象。下面的例子展示了一个接受两个MyObject对象作为参数的函数add()： 1234567891011121314151617181920212223242526272829303132333435363738394041// addon.cc#include &lt;node.h&gt;#include &lt;node_object_wrap.h&gt;#include \"myobject.h\"namespace demo &#123; using v8::FunctionCallbackInfo; using v8::Isolate; using v8::Local; using v8::Number; using v8::Object; using v8::String; using v8::Value; void CreateObject(const FunctionCallbackInfo&lt;Value&gt;&amp; args) &#123; MyObject::NewInstance(args); &#125; void Add(const FunctionCallbackInfo&lt;Value&gt;&amp; args) &#123; Isolate* isolate = args.GetIsolate(); MyObject* obj1 = node::ObjectWrap::Unwrap&lt;MyObject&gt;( args[0]-&gt;ToObject()); MyObject* obj2 = node::ObjectWrap::Unwrap&lt;MyObject&gt;( args[1]-&gt;ToObject()); double sum = obj1-&gt;value() + obj2-&gt;value(); args.GetReturnValue().Set(Number::New(isolate, sum)); &#125; void InitAll(Local&lt;Object&gt; exports) &#123; MyObject::Init(exports-&gt;GetIsolate()); NODE_SET_METHOD(exports, \"createObject\", CreateObject); NODE_SET_METHOD(exports, \"add\", Add); &#125; NODE_MODULE(addon, InitAll)&#125; // namespace demo 在myobject.h中，加入一个新的public方法value()来获取private变量： 123456789101112131415161718192021222324252627// myobject.h#ifndef MYOBJECT_H#define MYOBJECT_H#include &lt;node.h&gt;#include &lt;node_object_wrap.h&gt;namespace demo &#123; class MyObject : public node::ObjectWrap &#123; public: static void Init(v8::Isolate* isolate); static void NewInstance(const v8::FunctionCallbackInfo&lt;v8::Value&gt;&amp; args); inline double value() const &#123; return value_; &#125; private: explicit MyObject(double value = 0); ~MyObject(); static void New(const v8::FunctionCallbackInfo&lt;v8::Value&gt;&amp; args); static v8::Persistent&lt;v8::Function&gt; constructor; double value_; &#125;;&#125; // namespace demo#endif myobject.cc的实现也和之前类似： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// myobject.cc#include &lt;node.h&gt;#include \"myobject.h\"namespace demo &#123; using v8::Context; using v8::Function; using v8::FunctionCallbackInfo; using v8::FunctionTemplate; using v8::Isolate; using v8::Local; using v8::Object; using v8::Persistent; using v8::String; using v8::Value; Persistent&lt;Function&gt; MyObject::constructor; MyObject::MyObject(double value) : value_(value) &#123; &#125; MyObject::~MyObject() &#123; &#125; void MyObject::Init(Isolate* isolate) &#123; // Prepare constructor template Local&lt;FunctionTemplate&gt; tpl = FunctionTemplate::New(isolate, New); tpl-&gt;SetClassName(String::NewFromUtf8(isolate, \"MyObject\")); tpl-&gt;InstanceTemplate()-&gt;SetInternalFieldCount(1); constructor.Reset(isolate, tpl-&gt;GetFunction()); &#125; void MyObject::New(const FunctionCallbackInfo&lt;Value&gt;&amp; args) &#123; Isolate* isolate = args.GetIsolate(); if (args.IsConstructCall()) &#123; // Invoked as constructor: `new MyObject(...)` double value = args[0]-&gt;IsUndefined() ? 0 : args[0]-&gt;NumberValue(); MyObject* obj = new MyObject(value); obj-&gt;Wrap(args.This()); args.GetReturnValue().Set(args.This()); &#125; else &#123; // Invoked as plain function `MyObject(...)`, turn into construct call. const int argc = 1; Local&lt;Value&gt; argv[argc] = &#123; args[0] &#125;; Local&lt;Context&gt; context = isolate-&gt;GetCurrentContext(); Local&lt;Function&gt; cons = Local&lt;Function&gt;::New(isolate, constructor); Local&lt;Object&gt; instance = cons-&gt;NewInstance(context, argc, argv).ToLocalChecked(); args.GetReturnValue().Set(instance); &#125; &#125; void MyObject::NewInstance(const FunctionCallbackInfo&lt;Value&gt;&amp; args) &#123; Isolate* isolate = args.GetIsolate(); const unsigned argc = 1; Local&lt;Value&gt; argv[argc] = &#123; args[0] &#125;; Local&lt;Function&gt; cons = Local&lt;Function&gt;::New(isolate, constructor); Local&lt;Context&gt; context = isolate-&gt;GetCurrentContext(); Local&lt;Object&gt; instance = cons-&gt;NewInstance(context, argc, argv).ToLocalChecked(); args.GetReturnValue().Set(instance); &#125;&#125; // namespace demo 解释： addon.cc中使用户如下代码来获取包装对象： MyObject* obj1 = node::ObjectWrap::Unwrap(args[0]-&gt;ToObject()); 测试： 12345678// test.jsconst addon = require('./build/Release/addon');var obj1 = addon.createObject(10);var obj2 = addon.createObject(20);var result = addon.add(obj1, obj2);console.log(result); // 30 AtExit钩子一个AtExit钩子是这样一种函数：它会在node.js事件循环结束后、js虚拟机被终止前或node.js停机前被调用。AtExit钩子需要被使用node::AtExit来注册。 函数声明如下： void AtExit(callback, args) callback: void ()(void)，一个在exit时被调用的函数的函数指针。args: void*，一个传递给callback的指针。 AtExit钩子运行在事件循环之后和js虚拟机被kill掉之前。 AtExit钩子接受两个参数：一个回调函数的函数指针和一个传递给回调函数的隐式上下文数据的指针。 回调函数的调用方式是后进先出(LIFO)，和栈一样。 以下的addon.cc实现了AtExit钩子： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// addon.cc#undef NDEBUG#include &lt;assert.h&gt;#include &lt;stdlib.h&gt;#include &lt;node.h&gt;namespace demo &#123; using node::AtExit; using v8::HandleScope; using v8::Isolate; using v8::Local; using v8::Object; static char cookie[] = \"yum yum\"; static int at_exit_cb1_called = 0; static int at_exit_cb2_called = 0; static void at_exit_cb1(void* arg) &#123; Isolate* isolate = static_cast&lt;Isolate*&gt;(arg); HandleScope scope(isolate); Local&lt;Object&gt; obj = Object::New(isolate); assert(!obj.IsEmpty()); // assert VM is still alive assert(obj-&gt;IsObject()); at_exit_cb1_called++; &#125; static void at_exit_cb2(void* arg) &#123; assert(arg == static_cast&lt;void*&gt;(cookie)); at_exit_cb2_called++; &#125; static void sanity_check(void*) &#123; assert(at_exit_cb1_called == 1); assert(at_exit_cb2_called == 2); &#125; void init(Local&lt;Object&gt; exports) &#123; AtExit(sanity_check); AtExit(at_exit_cb2, cookie); AtExit(at_exit_cb2, cookie); AtExit(at_exit_cb1, exports-&gt;GetIsolate()); &#125; NODE_MODULE(addon, init);&#125; // namespace demo 解释： 上面例子定义了4个AtExit函数： void init(Local exports) { AtExit(sanity_check); AtExit(at_exit_cb2, cookie); AtExit(at_exit_cb2, cookie); AtExit(at_exit_cb1, exports-&gt;GetIsolate()); } 根据LIFO特性，在时间循环之后，VM停机之前，会依次执行： AtExit(at_exit_cb1, exports-&gt;GetIsolate()); AtExit(at_exit_cb2, cookie); AtExit(at_exit_cb2, cookie); AtExit(sanity_check); sanity_check会检查at_exit_cb1和at_exit_cb2的调用次数： assert(at_exit_cb1_called == 1); assert(at_exit_cb2_called == 2); 测试： 12// test.jsconst addon = require('./build/Release/addon');","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"node","slug":"node","permalink":"https://nullcc.github.io/tags/node/"}]},{"title":"web推荐系统知识介绍","slug":"web推荐系统知识介绍","date":"2017-05-09T16:00:00.000Z","updated":"2022-04-15T03:41:13.026Z","comments":true,"path":"2017/05/10/web推荐系统知识介绍/","link":"","permalink":"https://nullcc.github.io/2017/05/10/web推荐系统知识介绍/","excerpt":"利用数据提供某种建议是很常见的需求，它可以改善用户体验，好的推荐系统可以比较精准地为用户提供信息，增加购买率。","text":"利用数据提供某种建议是很常见的需求，它可以改善用户体验，好的推荐系统可以比较精准地为用户提供信息，增加购买率。 目前常见的推荐算法有以下三种： 1. 基于热度进行推荐 2. 基于用户的协同过滤方法 3. 基于物品的协同过滤方法 1. 基于热度进行推荐优点是简单粗暴，利用目前访问量最大的一些数据做推荐，技术上可以结合redis一类的内存型NoSQL系统。但缺点也很明显，不够个性化，我们不清楚每个用户的兴趣和习惯，精准率也低。不过在面对一些新用户的时候，由于我们对他们一无所知，所以基于热度进行推荐可能是对于这种情况比较好的一种推荐方法。 2. 基于用户的协同过滤方法这种方法一般会利用用户兴趣爱好，找到有类似兴趣爱好的人，然后根据这些人的爱好向你推荐你可能感兴趣的东西。 为此我们需要找到一种度量两个用户之间相似程度的方法，这可以用“余弦相似度”，从数学的角度理解就是两个向量夹角的余弦值(初中知识) 计算公式如下： 于是可知，cosθ ∈ [-1, 1] 直观上来说，它用来测量两个向量之间的“角度”，如果两个向量指向同一方向，那么余弦值相似度就是1，如果相反，则是-1，如果两个向量互相垂直，为0，这表示两个向量之间没有任何关系。我们可以把这部分概念应用到实际中，但是我们只考虑相关和不想关，即只取0和1两种值。0表示不相关，1表示相关，负相关我们暂时不去考虑，因为这比判定正相关要复杂很多，例如我们可以很明显地看出手办爱好者和动漫迷的关系是正相关(有可能他们都被打了动漫的标签)，但是就很难去判断厨师和程序猿的关系，所以这种关系不明确的情况，一般就赋予0值。 刚才只考虑了两个向量的情况，现在来考虑更普遍的情形。我们先根据两个用户的爱好来计算他们之间的相似度，的在此之前需要知道Jaccard公式： N(u)表示用户u的爱好集合，N(v)表示用户v的爱好集合，Jaccard公式可以计算用户u和v的相似度。 还有一个公式计算余弦相似度： |N(u)|和|N(v)|分表表示用户u和用户v爱好集合的大小。 (1)推荐相似用户有了上面这两个公式，就可以着手计算出每两个用户之间的相似度了。 为此可以建立一个用户-爱好表，比如有A、B、C、D四个用户，以及a、b、c、d、e 5个爱好，这四个用户具有的爱好情况如下： A | a b d B | a c C | b e D | c d e 据此可以建立物品-用户倒排表了： a | A B b | A C c | B D d | A D e | C D 然后建立一个用户-用户矩阵，矩阵的行和列都是用户，i行j列的数字表示这个元素对应行的用户和对应列的用户的喜欢相同物品的个数，需要注意的是，用户自身不和自己进行比较： # A B C D A 0 1 1 1 B 1 0 0 1 C 1 0 0 1 D 1 1 1 0 我们使用余弦相似度的公式来计算： 根据这个相似度矩阵，就可以很直观地看出任意两个用户之间的相似度了。比如对于用户A来说，他与B、C、D三个用户的相似分别是 A&lt;-&gt;B 0.4082482904638631 A&lt;-&gt;C 0.4082482904638631 A&lt;-&gt;D 0.3333333333333333 由此可见A和B、C两个用户比较相似度较高，和D用户相似度较低。 (2)推荐物品对于物品，我们也可以利用相似性来推荐，刚才我们计算出了每两个用户之间的相似性，每个结果都是一个数值，这个数值的取值范围是[0, 1]。于是我们可以大胆地猜测，相似的人可能会喜欢相似的物品，虽然这不一定完全正确，但总能为我们提供一些有用的信息。 给定一个用户u，首先要从除了u的所有用户中选出K个和他最相似的用户，用集合S(u, K)表示。然后将S中用户喜欢的物品全部提取出来，并除去u喜欢的物品，得到一个物品列表，对于每个候选物品i，可以用如下公式计算用户u对它感兴趣的程度： w(uv)表示用户u和用户v的相似度，r(vi)表示用户v对物品i的喜爱程度。 这里给定一个用户A，相似用户是B、C、D，候选物品为c、e，并取r(vi)都为1，那么根据上面的公式： 据此可以给出一个量化标准说明某用户对某物品的可能感兴趣程度。 (3)基于用户的协同过滤算法的一些问题在二维向量的情况下，余弦相似度公式等价于： 如果推广到更一般的形式： 这里就有一个问题，在高维向量空间中，绝大多数向量之间都离得非常远，因此让他们之间的方向也悬殊很大，也就是说，当兴趣的个数增大时，即使是与给定用户“最相似的用户”，实际上很可能也没有什么相似之处。因此当兴趣数量很多的时候，基于用户的协同过滤方法不是很实用。 3. 基于物品的协同过滤方法这种方法是先计算两个物品之间的相似度，然后将与用户当前物品相似的物品放到一起，并从中为用户推荐可能感兴趣的东西。 假设有ABCD四个用户和abcde五个物品，他们的关系如下(用户喜欢物品)： A | a b d B | a c C | b e D | c d e 用户-物品矩阵 行-用户列-物品 # a b c d e A 1 1 0 1 0 B 1 0 1 0 0 C 0 1 0 0 1 D 0 0 1 1 1 物品-用户矩阵(是用户-物品矩阵的转置) 行-物品列-用户 # A B C D a 1 1 0 0 b 1 0 1 0 c 0 1 0 1 d 1 0 0 1 e 0 0 1 1 如果两个用户的兴趣矩阵完全重合，那么他们的相似度为1，如果完全不重合，相似度为0。 接下来需要计算物品之间的相似度，我们根据余弦相似度公式可以考察两两物品间的相似度(注意物品对自己的相似度要置为0)： # a b c d e a 0 1/2 1/2 1/2 0 b 1/2 0 0 1/2 1/2 c 1/2 0 0 1/2 1/2 d 1/2 1/2 1/2 0 1/2 e 0 1/2 1/2 1/2 0 这个矩阵就是物品相似度矩阵。 一个实例这个实例会考察： 1.使用基于用户的协同过滤算法来给指定用户推荐兴趣 2.使用基于物品的协同过滤算法来给指定用户推荐兴趣 我们有一个用户兴趣列表users_interests，users_interests中的每一个元素都代表某个用户的兴趣列表。 users_interests = [ [&quot;Hadoop&quot;, &quot;Big Data&quot;, &quot;HBase&quot;, &quot;Java&quot;, &quot;Spark&quot;, &quot;Storm&quot;, &quot;Cassandra&quot;], [&quot;NoSQL&quot;, &quot;MongoDB&quot;, &quot;Cassandra&quot;, &quot;HBase&quot;, &quot;Postgres&quot;], [&quot;Python&quot;, &quot;scikit-learn&quot;, &quot;scipy&quot;, &quot;numpy&quot;, &quot;statsmodels&quot;, &quot;pandas&quot;], [&quot;R&quot;, &quot;Python&quot;, &quot;statistics&quot;, &quot;regression&quot;, &quot;probability&quot;], [&quot;machine learning&quot;, &quot;regression&quot;, &quot;decision trees&quot;, &quot;libsvm&quot;], [&quot;Python&quot;, &quot;R&quot;, &quot;Java&quot;, &quot;C++&quot;, &quot;Haskell&quot;, &quot;programming languages&quot;], [&quot;statistics&quot;, &quot;probability&quot;, &quot;mathematics&quot;, &quot;theory&quot;], [&quot;machine learning&quot;, &quot;scikit-learn&quot;, &quot;Mahout&quot;, &quot;neural networks&quot;], [&quot;neural networks&quot;, &quot;deep learning&quot;, &quot;Big Data&quot;, &quot;artificial intelligence&quot;], [&quot;Hadoop&quot;, &quot;Java&quot;, &quot;MapReduce&quot;, &quot;Big Data&quot;], [&quot;statistics&quot;, &quot;R&quot;, &quot;statsmodels&quot;], [&quot;C++&quot;, &quot;deep learning&quot;, &quot;artificial intelligence&quot;, &quot;probability&quot;], [&quot;pandas&quot;, &quot;R&quot;, &quot;Python&quot;], [&quot;databases&quot;, &quot;HBase&quot;, &quot;Postgres&quot;, &quot;MySQL&quot;, &quot;MongoDB&quot;], [&quot;libsvm&quot;, &quot;regression&quot;, &quot;support vector machines&quot;] ] 使用基于用户的协同过滤算法来给指定用户推荐兴趣1.余弦相似度函数： def cosine_similarity(v, w): return dot(v, w) / math.sqrt(dot(v, v) * dot(w, w)) 2.对兴趣排重，获得兴趣列表： unique_interests = sorted(list({ interest for user_interests in users_interests for interest in user_interests })) 这会生成一个唯一兴趣列表： [ &apos;Big Data&apos;, &apos;C++&apos;, &apos;Cassandra&apos;, &apos;HBase&apos;, &apos;Hadoop&apos;, &apos;Haskell&apos;, &apos;Java&apos;, &apos;Mahout&apos;, &apos;MapReduce&apos;, &apos;MongoDB&apos;, &apos;MySQL&apos;, &apos;NoSQL&apos;, &apos;Postgres&apos;, &apos;Python&apos;, &apos;R&apos;, &apos;Spark&apos;, &apos;Storm&apos;, &apos;artificial intelligence&apos;, &apos;databases&apos;, &apos;decision trees&apos;, &apos;deep learning&apos;, &apos;libsvm&apos;, &apos;machine learning&apos;, &apos;mathematics&apos;, &apos;neural networks&apos;, &apos;numpy&apos;, &apos;pandas&apos;, &apos;probability&apos;, &apos;programming languages&apos;, &apos;regression&apos;, &apos;scikit-learn&apos;, &apos;scipy&apos;, &apos;statistics&apos;, &apos;statsmodels&apos;, &apos;support vector machines&apos;, &apos;theory&apos; ] 3.创建用户-兴趣向量,向量长度是unique_interests的长度： def make_user_interest_vector(user_interests): return [1 if interest in user_interests else 0 for interest in unique_interests] 4.生成用户-兴趣矩阵： user_interest_matrix = map(make_user_interest_vector, users_interests) 得到： [ [1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0] ] 5.生成兴趣相似性矩阵： user_similarities = [[cosine_similarity(interest_vector_i, interest_vector_j) for interest_vector_j in user_interest_matrix] for interest_vector_i in user_interest_matrix] 得到： [[1.0, 0.3380617018914066, 0.0, 0.0, 0.0, 0.1543033499620919, 0.0, 0.0, 0.1889822365046136, 0.5669467095138409, 0.0, 0.0, 0.0, 0.1690308509457033, 0.0], [0.3380617018914066, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0], [0.0, 0.0, 1.0, 0.18257418583505536, 0.0, 0.16666666666666666, 0.0, 0.20412414523193154, 0.0, 0.0, 0.23570226039551587, 0.0, 0.47140452079103173, 0.0, 0.0], [0.0, 0.0, 0.18257418583505536, 1.0, 0.22360679774997896, 0.3651483716701107, 0.4472135954999579, 0.0, 0.0, 0.0, 0.5163977794943222, 0.22360679774997896, 0.5163977794943222, 0.0, 0.2581988897471611], [0.0, 0.0, 0.0, 0.22360679774997896, 1.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258], [0.1543033499620919, 0.0, 0.16666666666666666, 0.3651483716701107, 0.0, 1.0, 0.0, 0.0, 0.0, 0.20412414523193154, 0.23570226039551587, 0.20412414523193154, 0.47140452079103173, 0.0, 0.0], [0.0, 0.0, 0.0, 0.4472135954999579, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2886751345948129, 0.25, 0.0, 0.0, 0.0], [0.0, 0.0, 0.20412414523193154, 0.0, 0.25, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1889822365046136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 1.0, 0.25, 0.0, 0.5, 0.0, 0.0, 0.0], [0.5669467095138409, 0.0, 0.0, 0.0, 0.0, 0.20412414523193154, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.23570226039551587, 0.5163977794943222, 0.0, 0.23570226039551587, 0.2886751345948129, 0.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0], [0.0, 0.0, 0.0, 0.22360679774997896, 0.0, 0.20412414523193154, 0.25, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.47140452079103173, 0.5163977794943222, 0.0, 0.47140452079103173, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0], [0.1690308509457033, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.2581988897471611, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]] 6.获取和某个用户最相似的用户： def most_similar_users_to(user_id): pairs = [(other_user_id, similarity) # find other for other_user_id, similarity in # users with enumerate(user_similarities[user_id]) # nonzero if user_id != other_user_id and similarity &gt; 0] # similarity return sorted(pairs, # sort them key=lambda (_, similarity): similarity, # most similar reverse=True) # first 考察和第0个用户相似的用户： print &quot;User based similarity&quot; print &quot;most similar to 0&quot; print most_similar_users_to(0) 得到： User based similarity most similar to 0 [ (9, 0.5669467095138409), (1, 0.3380617018914066), (8, 0.1889822365046136), (13, 0.1690308509457033), (5, 0.1543033499620919) ] 7.使用基于用户的协同算法为某个用户推荐兴趣： def user_based_suggestions(user_id, include_current_interests=False): # sum up the similarities suggestions = defaultdict(float) for other_user_id, similarity in most_similar_users_to(user_id): for interest in users_interests[other_user_id]: suggestions[interest] += similarity # convert them to a sorted list suggestions = sorted(suggestions.items(), key=lambda (_, weight): weight, reverse=True) # and (maybe) exclude already-interests if include_current_interests: return suggestions else: return [(suggestion, weight) for suggestion, weight in suggestions if suggestion not in users_interests[user_id]] 为第0位用户推荐兴趣： print &quot;Suggestions for 0&quot; print user_based_suggestions(0) 得到： [ (&apos;MapReduce&apos;, 0.5669467095138409), (&apos;MongoDB&apos;, 0.50709255283711), (&apos;Postgres&apos;, 0.50709255283711), (&apos;NoSQL&apos;, 0.3380617018914066), (&apos;neural networks&apos;, 0.1889822365046136), (&apos;deep learning&apos;, 0.1889822365046136), (&apos;artificial intelligence&apos;, 0.1889822365046136), (&apos;databases&apos;, 0.1690308509457033), (&apos;MySQL&apos;, 0.1690308509457033), (&apos;programming languages&apos;, 0.1543033499620919), (&apos;Python&apos;, 0.1543033499620919), (&apos;Haskell&apos;, 0.1543033499620919), (&apos;C++&apos;, 0.1543033499620919), (&apos;R&apos;, 0.1543033499620919) ] 第0位用户的兴趣如下： [&quot;Hadoop&quot;, &quot;Big Data&quot;, &quot;HBase&quot;, &quot;Java&quot;, &quot;Spark&quot;, &quot;Storm&quot;, &quot;Cassandra&quot;] 发现他对大数据和数据库系统比较感兴趣，再看推荐兴趣的前三名： (&apos;MapReduce&apos;, 0.5669467095138409) (&apos;MongoDB&apos;, 0.50709255283711) (&apos;Postgres&apos;, 0.50709255283711) 直观上还是比较合适的一个推荐。 使用基于物品的协同过滤算法来给指定用户推荐兴趣1.生成兴趣-用户矩阵： interest_user_matrix = [[user_interest_vector[j] for user_interest_vector in user_interest_matrix] for j, _ in enumerate(unique_interests)] 得到： [ [1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] ] 2.获得和某个兴趣最相似的几个兴趣： interest_similarities = [[cosine_similarity(user_vector_i, user_vector_j) for user_vector_j in interest_user_matrix] for user_vector_i in interest_user_matrix] 得到(数据很多，仅仅是列出来，不用去细看了)： [[1.0, 0.0, 0.4082482904638631, 0.3333333333333333, 0.8164965809277261, 0.0, 0.6666666666666666, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.5773502691896258, 0.4082482904638631, 0.0, 0.0, 0.4082482904638631, 0.0, 0.0, 0.0, 0.4082482904638631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.7071067811865475, 0.4082482904638631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35355339059327373, 0.35355339059327373, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4082482904638631, 0.7071067811865475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4082482904638631, 0.0, 1.0, 0.8164965809277261, 0.5, 0.0, 0.4082482904638631, 0.0, 0.0, 0.5, 0.0, 0.7071067811865475, 0.5, 0.0, 0.0, 0.7071067811865475, 0.7071067811865475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3333333333333333, 0.0, 0.8164965809277261, 1.0, 0.4082482904638631, 0.0, 0.3333333333333333, 0.0, 0.0, 0.8164965809277261, 0.5773502691896258, 0.5773502691896258, 0.8164965809277261, 0.0, 0.0, 0.5773502691896258, 0.5773502691896258, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8164965809277261, 0.0, 0.5, 0.4082482904638631, 1.0, 0.0, 0.8164965809277261, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7071067811865475, 0.7071067811865475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.7071067811865475, 0.0, 0.0, 0.0, 1.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6666666666666666, 0.4082482904638631, 0.4082482904638631, 0.3333333333333333, 0.8164965809277261, 0.5773502691896258, 1.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.2886751345948129, 0.2886751345948129, 0.5773502691896258, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7071067811865475, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5773502691896258, 0.0, 0.0, 0.0, 0.7071067811865475, 0.0, 0.5773502691896258, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.5, 0.8164965809277261, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7071067811865475, 0.7071067811865475, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7071067811865475, 1.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.7071067811865475, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7071067811865475, 0.0, 1.0, 0.7071067811865475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.5, 0.8164965809277261, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7071067811865475, 0.7071067811865475, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.35355339059327373, 0.0, 0.0, 0.0, 0.5, 0.2886751345948129, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.7071067811865475, 0.2886751345948129, 0.5, 0.2886751345948129, 0.35355339059327373, 0.5, 0.2886751345948129, 0.35355339059327373, 0.0, 0.0], [0.0, 0.35355339059327373, 0.0, 0.0, 0.0, 0.5, 0.2886751345948129, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35355339059327373, 0.2886751345948129, 0.5, 0.2886751345948129, 0.0, 0.0, 0.5773502691896258, 0.35355339059327373, 0.0, 0.0], [0.5773502691896258, 0.0, 0.7071067811865475, 0.5773502691896258, 0.7071067811865475, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5773502691896258, 0.0, 0.7071067811865475, 0.5773502691896258, 0.7071067811865475, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4082482904638631, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.4082482904638631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7071067811865475, 1.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.7071067811865475, 0.7071067811865475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4082482904638631, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.4082482904638631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7071067811865475, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8164965809277261, 0.0, 0.0, 0.0, 0.0, 0.7071067811865475, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7071067811865475, 0.0, 0.5, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.4082482904638631, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 1.0], [0.4082482904638631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7071067811865475, 0.0, 0.0, 0.0, 0.7071067811865475, 1.0, 0.0, 0.7071067811865475, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7071067811865475, 0.35355339059327373, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7071067811865475, 1.0, 0.0, 0.0, 0.0, 0.5, 0.7071067811865475, 0.0, 0.5, 0.0, 0.0], [0.0, 0.4082482904638631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2886751345948129, 0.2886751345948129, 0.0, 0.0, 0.4082482904638631, 0.0, 0.0, 0.4082482904638631, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5773502691896258], [0.0, 0.7071067811865475, 0.0, 0.0, 0.0, 1.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2886751345948129, 0.2886751345948129, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.8164965809277261, 0.4082482904638631, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.5773502691896258, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35355339059327373, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.7071067811865475, 0.5, 0.0, 0.0, 0.0, 1.0, 0.7071067811865475, 0.0, 0.5, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7071067811865475, 0.0, 0.0, 0.0, 0.7071067811865475, 1.0, 0.0, 0.7071067811865475, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2886751345948129, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.4082482904638631, 0.0, 0.5773502691896258], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35355339059327373, 0.35355339059327373, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7071067811865475, 0.5, 0.0, 0.0, 0.0, 0.5, 0.7071067811865475, 0.4082482904638631, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 1.0]] 3.获得和某个兴趣最相似的几个兴趣: def most_similar_interests_to(interest_id): similarities = interest_similarities[interest_id] pairs = [(unique_interests[other_interest_id], similarity) for other_interest_id, similarity in enumerate(similarities) if interest_id != other_interest_id and similarity &gt; 0] return sorted(pairs, key=lambda (_, similarity): similarity, reverse=True) 我们考察和unique_interests[0]最相似的几个兴趣： print &quot;Item based similarity&quot; print &quot;most similar to&quot;, unique_interests[0] print most_similar_interests_to(0) 得到： Item based similarity most similar to Big Data [ (&apos;Hadoop&apos;, 0.8164965809277261), (&apos;Java&apos;, 0.6666666666666666), (&apos;MapReduce&apos;, 0.5773502691896258), (&apos;Spark&apos;, 0.5773502691896258), (&apos;Storm&apos;, 0.5773502691896258), (&apos;Cassandra&apos;, 0.4082482904638631), (&apos;artificial intelligence&apos;, 0.4082482904638631), (&apos;deep learning&apos;, 0.4082482904638631), (&apos;neural networks&apos;, 0.4082482904638631), (&apos;HBase&apos;, 0.3333333333333333) ] 发现和“Big Data”最相似的前三个兴趣分别是： Hadoop Java MapReduce 相关性还是比较强的。 4.使用基于物品的协同算法为用户推荐兴趣： def item_based_suggestions(user_id, include_current_interests=False): suggestions = defaultdict(float) user_interest_vector = user_interest_matrix[user_id] for interest_id, is_interested in enumerate(user_interest_vector): if is_interested == 1: similar_interests = most_similar_interests_to(interest_id) for interest, similarity in similar_interests: suggestions[interest] += similarity suggestions = sorted(suggestions.items(), key=lambda (_, similarity): similarity, reverse=True) if include_current_interests: return suggestions else: return [(suggestion, weight) for suggestion, weight in suggestions if suggestion not in users_interests[user_id]] 我们为第0位用户推荐兴趣： print &quot;suggestions for user 0&quot; print item_based_suggestions(0) 得到： suggestions for user 0 [ (&apos;MapReduce&apos;, 1.861807319565799), (&apos;Postgres&apos;, 1.3164965809277263), (&apos;MongoDB&apos;, 1.3164965809277263), (&apos;NoSQL&apos;, 1.2844570503761732), (&apos;programming languages&apos;, 0.5773502691896258), (&apos;MySQL&apos;, 0.5773502691896258), (&apos;Haskell&apos;, 0.5773502691896258), (&apos;databases&apos;, 0.5773502691896258), (&apos;neural networks&apos;, 0.4082482904638631), (&apos;deep learning&apos;, 0.4082482904638631), (&apos;C++&apos;, 0.4082482904638631), (&apos;artificial intelligence&apos;, 0.4082482904638631), (&apos;Python&apos;, 0.2886751345948129), (&apos;R&apos;, 0.2886751345948129) ] 这个推荐也是合理的。 基于用户还是基于物品？在针对大数据集生成推荐列表时，基于物品的过滤要比基于用户的过滤更快，效果也更好，但是基于物品的过滤算法更复杂一点，而且要维护一个物品相似度表，不过好在物品之间的相似度基本不怎么变化，所以很多电商都采用基于物品的协同过滤算法。 参考资料： [1] Joel Grus 数据科学入门 [M].人民邮电出版社，2016.[2] Create, Chen. 基于用户的协同过滤推荐算法原理和实现[EB/OL].http://www.cnblogs.com/technology/p/4467895.html.","categories":[{"name":"统计分析","slug":"统计分析","permalink":"https://nullcc.github.io/categories/统计分析/"}],"tags":[{"name":"统计分析","slug":"统计分析","permalink":"https://nullcc.github.io/tags/统计分析/"}]},{"title":"如何设计和实现一个web app","slug":"如何设计和实现一个web app","date":"2017-05-09T16:00:00.000Z","updated":"2022-04-15T03:41:13.033Z","comments":true,"path":"2017/05/10/如何设计和实现一个web app/","link":"","permalink":"https://nullcc.github.io/2017/05/10/如何设计和实现一个web app/","excerpt":"web app简介web app其实不算是什么新鲜的东西，相比于传统的web和传统的app，web app这种web和app相结合的产物有的优点如下：","text":"web app简介web app其实不算是什么新鲜的东西，相比于传统的web和传统的app，web app这种web和app相结合的产物有的优点如下： 开发上web app更有便捷性，ios开发一上来需要安装一堆东西，android开发也差不多，另外web app的学习成本要比平台客户端开发要低些，至少你不用去招聘ios和android程序员。只要具备基础web开发能力的人都可以比较快上手。 部署方便，在很多情况下，部署一个单页web app只需要一个index.html页面文件作为容器和一个前端资源打包文件（一般叫bundle.js）即可，然后把index.html放在自己服务器项目路径下，在其中引入bundle.js，至于bundle.js则可以放在CDN上，更新web app就等于是上传并刷新CDN缓存。这样一来前端部署极其简单，基本上不需要SSH到服务器去更换文件，也可以避免自己的服务器传输比较耗费带宽的前端资源文件。 单页web app如果技术选型得当，开发和维护成本都相对比较低。 可以适应更多环境。 凡事都有利弊，web app也有不尽如人意的地方，web app的主要缺点如下： 性能上不如原生app，这个不用多说，基本是不可改变的事实。 暂时还没想到其他的。 前期准备首先要进行技术选型，根据作者的经历，我选择的是react+flux，flux是一种数据流的架构方式，严格来说它并不是某一种特定的实现。比较常用的实现有facebook官方的flux实现，还有一些第三方实现(比如redux)。要注意的是，flux的具体实现基本不会影响到我们的项目开发，具体选择哪个实现就看你的喜好了（当然最好不要中途再做改变:D）。 那么就要来简单介绍一下react是什么，react可以说是一个改变前端生态系统的发明，传统web开发中，我们以页面为单位来设计模块，如果有多个页面都用到了某个功能，比如一个列表，那么就要重复地写HTML和JS实现它，代码抽象程度很低，冗余度很高，也不利于后期维护。react推出了UI组件的概念，让web前端开发人员可以封装UI组件用于复用。 flux这种数据流架构思想其实也不是什么新东西，但是真正在web前端大规模使用也是在开发人员使用react+flux之后的事情。flux制定了一些web前端数据获取和分发的规则，虽然刚开始看上去比较复杂，但是一旦你理解了它的思想，其实很简单，而且对于维护一个web app来说，react+flux可以说是相当不错的组合。 设计一个web app具体是在设计什么？刚才谈到了react+flux，其实UI组件的封装和数据流的架构，它们都帮开发人员规定好了，我们需要做的就是去好好运用它，那么还有什么可设计的呢？ 还是有的，而且不少。从技术角度来说，web app和ios app、android app这些原生app在技术设计上有很多共通之处： 页面生命周期管理 公共函数的设计 局部/全局事件和通知 UI组件API设计 常量定义 网络请求… web app也具备一些原生app所没有或不一样的特性，例如app路由、JS（ES5/ES6差异）、使用/刷新CDN缓存、前端资源打包、快速部署等。 react+flux是怎么工作的由于本篇文章并不是react和flux的教程，所以只会大致介绍一下它们的工作方式。 下面是官方给出的一个flux工作流程图： 详细版： 简单解释一下flux的工作流程： 在第一张图中，有两种流程： Action-&gt;Dispatcher-&gt;Store-&gt;View View-&gt;Action-&gt;Dispatcher-&gt;Store-&gt;View 在Action-&gt;Dispatcher-&gt;Store-&gt;View中，可以通过调用Action中的方法来执行一项操作，这个操作可以是向服务器请求数据，或仅仅在本地改变数据，操作完成后，Action会通知Dispatcher，然后由后者来分派动作和数据。在Store中，事先注册好对各个类型事件的回调函数，当Store接收到Dispatcher分发来的事件和数据时，就执行一些更新操作。另外View可以对Store注册监听器，一但Store中的数据有变化，会立即执行View预先设置好的监听器回调函数，这一般会是一个更新View的操作，这相当于一个发布-订阅模式。 在View-&gt;Action-&gt;Dispatcher-&gt;Store-&gt;View中，实际上是说指用户和View之间的交互导致数据变更(不管是请求服务器还是本地数据改变)，其他操作和Action-&gt;Dispatcher-&gt;Store-&gt;View基本一样。 光看这些是十分抽象的，如果没有深入去看一个demo或者自己实现一个项目，确实有点不好理解。我将在后续的文章中介绍react和flux –&gt; react+flux编程实践(一) 基础篇，并实际剖析一个官方demo：todo-mvc –&gt; react+flux编程实践(二) 实践篇。 本篇中react+flux基本介绍到这。下面要谈谈具体设计。 具体设计一个web app无非就是颠来倒去地做以下几件事： 调用网络API 展示页面 数据本地存储(这里一般指非持久化的那种，这和原生app有所不同) 接受用户输入并反馈 作为技术人员，我们首先要明确几点： 明确地知道业务放需要什么。 划分功能模块。 弄清楚各个问题之间的依赖关系，制定模块之间的通信规范。 适当考虑项目未来走向，对架构设计留有余地。 分析风险。 考虑如何解决依赖关系中最基础的部分，先实现基础模块(或者至少你要先对此有个大致的设计)，不断在此基础上完善整个架构。这部分开发人员会花费比较大的精力去做，因为这影响到整个项目未来的几乎所有事情。同时在这个过程中不断审视架构是否合理，及时调整。 单元测试，性能测试(如果项目需要且有时间的话)。 项目文件结构好的开发习惯其中一个就是要制定清晰的项目文件结构，并且从始至终保持下去。 适当的文档描述如果可能的话，适当写一些帮助性的架构文档，用简洁明确的词语传达你想要表达的，让后来的程序员可以快速上手。 思路和方法同一，不搞多元化在一个项目中，对同一类事情应该有一个统一的处理方法，包括代码风格、变量和方法的命名规范、调用规范、流程规范等，事先制定出来，并且要求所有人都要遵守。 尽量少的横向依赖，尽量减少跨层访问，降低模块间耦合度这部分内容react+flux已经帮我们做了很大一部分。 对业务方该限制的地方有明确的限制，该灵活的地方要给业务方创造灵活实现的条件可以通过良好的设计来保证这一点。 接下来会对划分功能模块、模块间通信规范、解决依赖关系这几个方面进行说明。 功能模块的划分在react+flux的项目中，不谈具体业务的情况下，有几个大的基础模块是一定要考虑的： 展示模块 网络请求模块 本地存储模块 路由模块 公用模块 1. 展示模块这是用户能直接看到的东西，我们用react封装各个UI组件提供出来，也可以引入第三方UI组件，这本身已经是一种进步，让我们可以在web前端“面向组件编程”，因此对UI组件的处理就尤为关键。ES6语法下，我们可以用extends react.Component来建立一个UI组件，然后在组件“类”中写初始化方法、渲染方法、生命周期函数、事件回调等方法，然后把它作为一个整体提供出去，这里就涉及到UI组件API的设计，UI组件可以接受属性值，这些属性应该尽可能的少和命名清晰、简单，保持简洁性很有必要。另外组件的展示离不开CSS和一些资源文件，作为一种封装，把CSS、图片等资源文件一并放在这个UI组件的文件夹下也是很有必要的。 2. 网络请求模块web app网络请求全靠ajax，在和服务器交互时，应该和服务端约定好返回数据的格式，如错误代码的含义、出错信息、详细的数据格式等，并且很有必要在web app端做一层封装，比如封装出一个request模块，在服务器返回数据后首先解析返回数据，如果出错就报错，成功就执行用户回调函数等。这部分作为整个项目对外的接口应该考虑到所有可能的网络情况：服务器致命错误、普通错误、成功、超时、无网络等。 3. 本地存储模块这部分主要交给flux处理，我们需要做的就是设计本地数据的结构，使其尽量合理并适应多种应用场景。 4.路由模块react+flux的单页web app一般采用react-router作为路由，这可以省下你不少时间和精力，react-router是一个非常成熟和优秀的路由模块。 5. 公共模块公共模块一般包括但不仅限于：公用函数、helper、常量定义、应用级别的功能(如展示loading框、警告框、确认框这些)。这部分功能也很重要，比如loading框，我们在发起网络请求时会展示它，数据返回时它会消失，那么这个框的出现就与上面说到的网络模块有关系，警告框和确认框也是常用的东西，这些组件都可应该在app层面上进行设计和整合，而不应该放在各个UI组件内部，因为这个是一个app中的通用功能。 模块间通信规范在react+flux中，使用一种单向数据流的方式来分发数据，这就让整个数据走向非常清楚，我们的web app模块间通信规范就是根据这个单向数据流的思想来的。 解决依赖关系react+flux中处理依赖关系时，用的比较多的方式无非3种：显式引入、基于事件(发布-订阅模式)、回调函数。 在要向模块中引入其他模块时(import)，使用显式引入，这种依赖关系最强也最明显。如果是依赖关系没有那么强，可以考虑用后面两种，这有助于代码的简介和模块解耦。 基于事件(发布-订阅模式)，举个简单的例子，flux中当Store中的数据变化时，要通知相对应的View更新页面，典型的处理方式是让Store成为一个EventEmitter，同时View对该Store addChangeListener，即成为它的订阅者，当Store改变时会自动调用该View的监听回调，让View更新界面。 基于回调函数，有一个非常典型的用例，在设计通用的警告框和确认框组件时，有时我们需要在用户点击“确定”和“取消”按钮时做一些事情，当然也可能什么也不做。这时候我们不要忘了JS中函数是一等公民这件事，我们可以把事件处理函数传递给警告框和确认框组件，这样就很巧妙的解决了跨组件、跨模块通信而不会使模块过于耦合了。","categories":[{"name":"web前端","slug":"web前端","permalink":"https://nullcc.github.io/categories/web前端/"}],"tags":[{"name":"webapp","slug":"webapp","permalink":"https://nullcc.github.io/tags/webapp/"}]},{"title":"用Redis实现优先级队列","slug":"用Redis实现优先级队列","date":"2017-05-09T16:00:00.000Z","updated":"2022-04-15T03:41:13.037Z","comments":true,"path":"2017/05/10/用Redis实现优先级队列/","link":"","permalink":"https://nullcc.github.io/2017/05/10/用Redis实现优先级队列/","excerpt":"在最近在面试过程中，张先森遇到一个面试官这么问，如果一个并发很大的消息应用，想要根据请求的优先级来处理，该怎么做。我当时只是笼统地回答用redis，面试官点了点头，这个问题就此通过。","text":"在最近在面试过程中，张先森遇到一个面试官这么问，如果一个并发很大的消息应用，想要根据请求的优先级来处理，该怎么做。我当时只是笼统地回答用redis，面试官点了点头，这个问题就此通过。 那么用redis究竟如何解决这个问题呢，下面就简单说一下吧。 首先抓出问题里面几个关键字，一是并发量大，二是请求的优先级。 先谈谈并发量大，对于一个消息系统，服务端必然会接受很多客户端的请求，这些请求一般来说都是异步的，用户不必等待请求被处理。对于这类需求，我们需要有一个能缓存住大量消息请求的东西，用redis来做这个是非常合适的。基本上来说，redis能缓存住的消息数量只取决于内存大小，而且我们需要的只是队列最基本的操作：进队和出队，它们的时间复杂度都是O(1)，因此性能上很高。 具体来说，redis里面有一个list结构，我们可以利用list构造一个FIFO(先进先出)的队列，所有请求就在这个队列里面排队等待处理。redis的list有lpush,rpush,lpop和rpop这么几个常用的操作，如果我们要构造FIFO队列，可以用lpush和rpop(或者用rpush和lpop)，注意进队和出队方向相反即可。 第二个关键字，请求的优先级。我们先假设一个最简单的场景，有三个优先级：高中低三级。可以设置3个list结构，比如叫queue_h，queue_m，queue_l，分别对应三个优先级。我们的代码流程可以这样来写： 首先设置3个优先级的list。 写入端： 根据请求的优先级往相应list里lpush数据。 读出端： 可以采用定时轮询的方式，按序依次检查高、中、低三个list的长度(可以使用llen命令)，如果该list长度大于0，说明当前队列需要立即被处理。 从这个list中rpop数据，然后处理数据。 需要注意的是，因为有分优先级，所以只有在高优先级的请求都被处理完以后才能去处理中低优先级的请求，这是一个大前提。 有人可能会问，如果我的优先级分类远大于3个呢，比如有1000个优先级怎么办，总不能设置1000个list吧，这样太蛋疼了。这种情况也不是完全没可能，也许有的系统就是这么多优先级呢。 这种需求我们可以结合分段来处理，比如0-99，100-199…900-999，先把优先级分成几个等分，然后在各个分段中使用有序集合，有序集合可以对集合内的元素排序，有序集合在插入一个元素的时候使用二分查找法，所以在比较大的数据量面前效率还是可以的，如果请求数实在太多，可以考虑进一步细分优先级的分段，以减少有序列表元素的数量。在一个请求进来时，首先确定它的优先级分段，把这个请求放到相应的有序集合中。在处理部分，需要有一个服务书按优先级高到低顺序遍历优先级的分段，然后直接取优先级最高的请求来处理(在有序集合中取最高或最低的元素时间复杂度都是O(1))。 下面是一些代码示例，用node.js编写，只分了三个优先级。 123456789101112131415161718192021222324252627// 生产者var redisClient = require(\"./lib/redis\");var redisConf = require(\"./config/config.json\").redis;redisClient.config(redisConf);var client = redisClient.client;// 优先级队列,低中高三个等级var priorityQueues = [\"queue_h\", \"queue_m\", \"queue_l\"];function getRandomNum(min, max) &#123; var range = max - min; var rand = Math.random(); return(min + Math.round(rand * range));&#125;// 每隔两秒产生10条数据setInterval(function()&#123; var count = 10; for (var i = 0; i &lt; count; i++) &#123; var idx = getRandomNum(0, 2); console.log(\"push: \" + priorityQueues[idx]); client.lpush(priorityQueues[idx], \"abc\"); &#125;&#125;, 2000); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 消费者var async = require(\"async\");var redisClient = require(\"./lib/redis\");var redisConf = require(\"./config/config.json\").redis;redisClient.config(redisConf);var client = redisClient.client;// 优先级队列,pushMessage低中高三个等级var priorityQueues = [\"queue_h\", \"queue_m\", \"queue_l\"];// 依次检查高中低三个优先级的list,遵循FIFOfunction getMessage()&#123; // 分别检查所有优先级队列中有没有数据 async.parallel([ function(callback)&#123; client.llen(priorityQueues[0], function(err, len)&#123; callback(null, len); &#125;); &#125;, function(callback)&#123; client.llen(priorityQueues[1], function(err, len)&#123; callback(null, len); &#125;); &#125;, function(callback)&#123; client.llen(priorityQueues[2], function(err, len)&#123; callback(null, len); &#125;); &#125; ], function(err, results)&#123; if (err) &#123; console.log(err); return; &#125; for (var i = 0; i &lt; results.length; i++)&#123; if (results[i] &gt; 0)&#123; client.rpop(priorityQueues[i], function(err, res)&#123; console.log(\"pop: \" + priorityQueues[i] + \" \" + res); &#125;); return; &#125; if (i == 2)&#123; console.log('No message can be handled.'); return; &#125; &#125; &#125;);&#125;// 每20ms获取一次数据setInterval(function()&#123; getMessage();&#125;, 20); 代码实现比较简单，主要实现了高中低三个优先级的情况。","categories":[{"name":"web后端","slug":"web后端","permalink":"https://nullcc.github.io/categories/web后端/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://nullcc.github.io/tags/Redis/"}]},{"title":"聚类分析知识介绍","slug":"聚类分析知识介绍","date":"2017-05-09T16:00:00.000Z","updated":"2022-04-15T03:41:13.038Z","comments":true,"path":"2017/05/10/聚类分析知识介绍/","link":"","permalink":"https://nullcc.github.io/2017/05/10/聚类分析知识介绍/","excerpt":"1.聚类理论当你观察某些数据源时，很可能会发现数据以某种形式形成聚类(cluster)。比如考察一个城市的城区中，居民收入的分布，一般可以发现低等收入、中等收入、高等收入和富豪人群在居住地上会形成聚类。类似的还有考察一个网站的用户分布情况，一般可以发现不同年龄的用户在关注点上形成聚类。","text":"1.聚类理论当你观察某些数据源时，很可能会发现数据以某种形式形成聚类(cluster)。比如考察一个城市的城区中，居民收入的分布，一般可以发现低等收入、中等收入、高等收入和富豪人群在居住地上会形成聚类。类似的还有考察一个网站的用户分布情况，一般可以发现不同年龄的用户在关注点上形成聚类。 聚类的一个通俗的定义是，将物理或抽象对象的集合分成由类似的对象组成的多个类的过程被称为聚类。俗话说：“物以类聚，人以群分”，由聚类所生成的簇是一组数据对象的集合，这些对象与同一个簇中的对象彼此相似，与其他簇中的对象相异。 一个聚类对商务分析的支持例子是，我们可以在用户样本中使用聚类分析，发现客户群，进而知道各类客户的特征，可以针对各种不同客户投放广告，这要比对所有客户都投放一模一样的广告要好得多。 (1)模型对我们来说，每个输入都是d维空间中的一个向量，因此我们必须想方设法地把我们的原始输入转化成数字(这有时不是那么简单)。 k-均值算法(k-means)是一种最简单的聚类分析方法，它通常需要首先选出聚类k的数目，然后把输入划分成集合S(1)-S(k)，并使得聚类中每个数据到其所在聚类的均值(中心对象)的距离的平方之和最小化，这可以借助迭代算法来解决。 步骤如下： 首先从d维空间中选出k个数据点作为初始聚类的均值(即中心点)。 计算每个数据点到这些聚类的均值(即聚类中心)的距离，然后把各个数据点分配给距离它最近的那个聚类。 如果所有数据点都不再被重新分配，就停止并保持现有聚类。 如果仍然有数据点被重新分配，则重新计算均值，并返回第2步。 K-均值算法的核心代码： 123456789101112131415161718192021222324252627282930313233class KMeans: def __init__(self, k): self.k = k # 聚类数目k self.means = None # 聚类均值列表 # 计算与输入最接近的聚类 def classify(self, input): return min(range(self.k), key=lambda i: squared_distance(input, self.means[i])) # 计算过程 def train(self, inputs): self.means = random.sample(inputs, self.k) # 初始时状态时,从inputs中随机选取k个不同元素 assignments = None while True: # 计算当前情况下,所有输入点的聚类分配情况 new_assignments = map(self.classify, inputs) # 如果所有数据点不再被重新分配,就保持现有聚类并返回 if assignments == new_assignments: return # 否则重新计算均值 assignments = new_assignments for i in range(self.k): # 分别获取每个聚类中的所有点 i_points = [p for p, a in zip(inputs, assignments) if a == i] # 为防止除零错误,要先判断每个聚类点集合长度是否为0 if i_points: self.means[i] = vector_mean(i_points) # 计算各个聚类中点坐标的均值 在解释上面KMeans的代码之前，有必要先来看几个公式和概念。 计算点和点之间的距离(这里用欧几里得距离)，下面公式中，m是聚类中心，x是数据点，维度为r： 更新簇平均值公式： 计算准则函数E： 代码解析： 在__init__方法(python中的类构造函数)中，需要事先指定聚类数self.k，怎么选择这个k值在后面会说明。self.means是各个聚类均值的列表，所以len(self.means) == self.k。 classify方法中，接受一个inputs参数，inputs参数在我们例子中是一个二维点坐标列表。关键的是下面这行代码： 1return min(range(self.k), key=lambda i: squared_distance(input, self.means[i])) classify会针对input(这里就是一个坐标)，计算它到所有聚类中心点的距离的最小值，并返回那个中心点所在聚类的下标。 计算过程，K-Means的计算是一个迭代过程，会不断逼近最优解。 12345678910111213141516171819202122# 计算过程def train(self, inputs): self.means = random.sample(inputs, self.k) # 初始时状态时,从inputs中随机选取k个不同元素 assignments = None while True: # 计算当前情况下,所有输入点的聚类分配情况 new_assignments = map(self.classify, inputs) # 如果所有数据点不再被重新分配,就保持现有聚类并返回 if assignments == new_assignments: return # 否则重新计算均值 assignments = new_assignments for i in range(self.k): # 分别获取每个聚类中的所有点 i_points = [p for p, a in zip(inputs, assignments) if a == i] # 为防止除零错误,要先判断每个聚类点集合长度是否为0 if i_points: self.means[i] = vector_mean(i_points) # 计算各个聚类中点坐标的均值 首先从inputs中随机选取k个不同元素作为聚类的起始中心点。然后进入一个while True无限循环，在这个循环中，会不断计算输入中各个点分配给k个聚类的情况，assignments变量保存的是每个输入元素上个循环时所对应的聚类下标，一旦当前assignments和新计算出的assignments相等(意思是各个元素不再被重新分配聚类)，则退出，表示聚类完毕。 在无限循环中，分别获取每个聚类中的所有点，然后计算各个聚类中点坐标的均值，这里会用到”更新簇平均值公式“，然后把每个聚类的中心点更新为这个均值。 执行代码1234567891011121314151617if __name__ == \"__main__\": inputs = [[-14,-5],[13,13],[20,23],[-19,-11],[-9,-16],[21,27],[-49,15],[26,13],[-46,5],[-34,-1],[11,15],[-49,0],[-22,-16],[19,28],[-12,-8],[-13,-19],[-41,8],[-11,-6],[-25,-9],[-18,-3]] random.seed(0) clusterer = KMeans(3) clusterer.train(inputs) print \"3-means:\" print clusterer.means print # 作图 means_xs, means_ys = zip(*clusterer.means) xs, ys = zip(*inputs) plt.plot(means_xs, means_ys, 'ro') plt.plot(xs, ys, 'bs') plt.axis([-60, 40, -30, 40]) plt.show() 结果如下： 3-means: [[-43.800000000000004, 5.4], [-15.888888888888888, -10.333333333333332], [18.333333333333332, 19.833333333333332]] 画图结果(蓝色点为输入数据点，红色为聚类中心)： 把聚类k值换成2试试： 123456789101112131415if __name__ == \"__main__\": inputs = [[-14,-5],[13,13],[20,23],[-19,-11],[-9,-16],[21,27],[-49,15],[26,13],[-46,5],[-34,-1],[11,15],[-49,0],[-22,-16],[19,28],[-12,-8],[-13,-19],[-41,8],[-11,-6],[-25,-9],[-18,-3]] random.seed(0) clusterer = KMeans(2) clusterer.train(inputs) print \"2-means:\" print clusterer.means means_xs, means_ys = zip(*clusterer.means) xs, ys = zip(*inputs) plt.plot(means_xs, means_ys, 'ro') plt.plot(xs, ys, 'bs') plt.axis([-60, 40, -30, 40]) plt.show() 结果如下： 2-means: [[-25.857142857142854, -4.714285714285714], [18.333333333333332, 19.833333333333332]] 画图结果： (2)聚类数目k值的选择刚才直接指定了k=3和k=2两种情况，有点完全靠蒙的感觉，因此一定有一种方法来帮助我们选择k值。 一个比较好理解的方法是以误差(即每个数据点到聚类中心的距离)的平方和作为k的函数，并画出该函数的图像，在其“弯曲”出寻找合适的k值。 12345678910111213141516171819202122# 聚类误差def squared_clustering_errors(inputs, k): \"\"\"finds the total squared error from k-means clustering the inputs\"\"\" clusterer = KMeans(k) clusterer.train(inputs) means = clusterer.means assignments = map(clusterer.classify, inputs) return sum(squared_distance(input,means[cluster]) for input, cluster in zip(inputs, assignments))# 绘制聚类误差def plot_squared_clustering_errors(plt): ks = range(1, len(inputs) + 1) errors = [squared_clustering_errors(inputs, k) for k in ks] plt.plot(ks, errors) plt.xticks(ks) plt.xlabel(\"k\") plt.ylabel(\"total squared error\") plt.show() 1234567# 计算所有k产生的误差(k取值范围是1-len(inputs))print \"errors as a function of k\"for k in range(1, len(inputs) + 1): print k, squared_clustering_errors(inputs, k)printplot_squared_clustering_errors(plt) # 画图 结果： errors as a function of k 1 15241.35 2 4508.73809524 3 1209.05555556 4 986.638888889 5 940.333333333 6 633.833333333 7 430.75 8 279.0 9 183.583333333 10 304.583333333 11 192.666666667 12 442.666666667 13 234.833333333 14 82.0 15 120.5 16 42.0 17 73.0 18 12.5 19 65.0 20 0.0 图像： 在k=3出有一个拐点，可以看到从k=1到k=3的误差下降趋势非常明显，从k=3之后，误差下降趋于平缓，到k=20时误差为0(这等于每个数据点自己形成一个聚类，自己是自己的中心点)。从数学角度来看，这个误差函数是k的函数，当它的导数从一个很大的值突然趋于0时，这个值就很适合作为聚类数目。","categories":[{"name":"统计分析","slug":"统计分析","permalink":"https://nullcc.github.io/categories/统计分析/"}],"tags":[{"name":"统计分析","slug":"统计分析","permalink":"https://nullcc.github.io/tags/统计分析/"}]},{"title":"koa2中controller实现类似sleep的延迟功能","slug":"koa2中controller实现类似sleep的延迟功能","date":"2017-05-08T16:00:00.000Z","updated":"2022-04-15T03:41:13.024Z","comments":true,"path":"2017/05/09/koa2中controller实现类似sleep的延迟功能/","link":"","permalink":"https://nullcc.github.io/2017/05/09/koa2中controller实现类似sleep的延迟功能/","excerpt":"今天有同事问我如何在koa2中的controller中使用延迟执行的功能，他直接在controller中使用setTimeout，但是没效果。","text":"今天有同事问我如何在koa2中的controller中使用延迟执行的功能，他直接在controller中使用setTimeout，但是没效果。 错误的代码类似下面这样： 123456// 错误的方法exports.test = async(ctx) =&gt; &#123; setTimeout(async function()&#123; await ctx.render('home/test.njk'); &#125;, 2000);&#125;; 问题在于，这里的controller会直接返回，并不会返回给客户端任何信息。因此请求这个接口的路由会返回404。 要真正做到在controller处理请求时延迟执行某些操作，需要实现一个delay函数，这个函数返回一个Promise，在这个Promise中调用setTimeout，像下面这样： 123456789101112// 正确的实现exports.test = async(ctx) =&gt; &#123; async function delay(time) &#123; return new Promise(function(resolve, reject) &#123; setTimeout(function()&#123; resolve(); &#125;, time); &#125;); &#125;; await delay(2000); await ctx.render('home/test.njk');&#125;; 上面代码会在2000毫秒后再渲染模版并返回给客户端。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"node","slug":"node","permalink":"https://nullcc.github.io/tags/node/"},{"name":"koa2","slug":"koa2","permalink":"https://nullcc.github.io/tags/koa2/"}]},{"title":"《NoSQL精粹》读书笔记","slug":"《NoSQL精粹》读书笔记","date":"2017-05-08T16:00:00.000Z","updated":"2022-04-15T03:41:13.027Z","comments":true,"path":"2017/05/09/《NoSQL精粹》读书笔记/","link":"","permalink":"https://nullcc.github.io/2017/05/09/《NoSQL精粹》读书笔记/","excerpt":"本文是《NoSQL精粹》的读书笔记。","text":"本文是《NoSQL精粹》的读书笔记。 NoSQL数据库数据模型的一般分类： 键值数据模型 文档数据模型 列族数据模型 图数据模型 常见NoSQL数据库： Redis, Cassandra, MongoDB, Neo4J, Riak… 数据库应用趋势： 由于数据量越来越大，大型系统的扩展方式由数据库在单一计算机上的纵向扩展-&gt;在计算机集群中的横向扩展 混合持久化(关系型数据库 + NoSQL数据库) 第一部分第1章 为什么使用NoSQL 关系型数据库和应用程序之间的“阻抗不匹配”。关系模型和应用程序程序中内存数据结构的不一致导致这种状况，因此涌现出很多对象-关系映射(ORM)框架，但滥用ORM可能会导致性能问题。 “集成数据库”和“应用程序数据库”，前者是一种多应用程序分享使用同一数据库的方式，特点是所有应用程序都可以获得自己想要的东西，但由于要兼容所有应用程序，因此数据库的设计可能会过分复杂导致不好维护，并且也需要应对系型数据库和应用程序之间的“阻抗不匹配”。后者衍生除了web服务，应用程序之间以接口的方式进行交互，通常使用XML或JSON来交换数据，此时就可以使用结构丰富的数据格式(数组、嵌套结构等)了。 web服务经常使用传输文本信息的HTTP协议，对一些性能要求较高的情况，可以使用二进制协议。 关系型数据库和计算机集群的格格不入。关系型数据库能把数据划分为几个集合，分别部署在各自独立的服务器上运行，这可以有效地对数据库分片。但这么做也有缺点，应用程序必须控制所有分片，它要知道数据库中的每份小数据存放在哪里才行。而且，查询、参照完整性、事务、一致性控制等操作也无法再跨分片的环境下执行。 在集群中使用关系型数据库有可能遇到成本的问题(商用关系型数据库一般按服务器台数计费)。 出现了以谷歌和亚马逊为首的典型计算机集群用户，为此谷歌研发了BigTable，亚马逊则是Dynamo。 混合持久化需要把集成数据库迁移到应用程序数据库上，一般来说，应用程序数据库都可以采用NoSQL数据库，但集成数据库不适合使用NoSQL数据库。企业可以考虑把原来存放在集成数据库中的数据转移到各个应用程序数据库，然后使用web服务来连接这些系统。这既降低了应用之间的耦合度，也降低了成本和维护难度。 选用NoSQL数据库的两个主要原因： 待处理的数据量很大，对数据访问的效率要求很高，从而必须把数据放在集群上。 希望采用一种更为方便的数据交互方式来提高应用程序的开发效率。 NoSQL的特征： 不使用关系模型 在集群中运行良好 开源 适用于21世纪的互联网公司 无模式 第2章 聚合数据模型 聚合结构可以把关联数据直接嵌入在某个聚合中，从而方便查找。聚合使数据库在集群上管理数据存储更为方便。 对某些数据交互有用的聚合结构，可能会阻碍另一些数据交互。在对数据建模时，需要考虑到哪种场景占主导地位，据此来设计数据聚合的方式。 选用面向聚合模型的决定性因素，就是它很适合在集群上运行。我们需要把采集数据时所需的节点数将至最小。如果在数据库中明确包含聚合结构，那么数据库就知道这些数据就会被一起操作，并放在同一节点中。 通常情况下，面向聚合的数据库不支持跨越多个聚合的ACID事务。聚合是作为交互单元的数据集合，数据库中的ACID操作以聚合为界。 键值数据库、文档数据库、列族数据库都属于面向聚合的数据库。 如果数据交互大都在同一聚合内执行，则应使用面向聚合的数据库；如果数据交互操作需要使用多种不同格式的数据，那么最好选用“聚合无知式数据库”。 第3章 数据模型详解图数据库 常见的图数据库有：FlockDB, Neo4J, Infinite Graph。 图数据库对于处理复杂关系的场景较为理想，例如社交网络、产品偏好、资格认定规则等包含复杂关系的数据。 对关系型数据库来说，操作内部相互关系比较紧密的数据模型通常需要使用很多JOIN语句，效率也较差。使用图数据库遍历关系，则非常迅速。主要原因是由于图数据库会多花一些时间用于插入关系数据，以此来缩短遍历关系时所需的时间。在一些查询效率高于插入效率的场合，这种权衡非常重要。 图数据库使用边和节点来存储数据，使用图数据库时，大部分时候都是在浏览各种关系。 图数据库和面向聚合的数据库的明显差别，在于图数据库重视数据间的“关系”，这种数据模型上的差异也导致了其他一些区别。图数据库一般运行在单一服务器上而非集群中。为了使数据保持一致，ACID事务需要涵盖多个节点与边。相似之处是，它们都不适用关系模型。 无模式数据库 各种形式的NoSQL都有一个特性，就是它们都是无模式的。在关系型数据库中，我们首先要定义好模式，就是用一种预定义结构向数据库说明，有哪些表，表中有哪些列，每列存放什么类型的数据，必须先定义好模式，才能使用数据库。相反，无模式数据库就显得相当随意了，可以在一个键下存放任意数据。 无模式数据库没有关系型数据库的那么多限制，但是它本身也存在一些问题，不管数据库无模式到什么程度，它都存在“隐含模式”，它是指在编写数据操作代码时，对数据结构所做的一系列假设。虽然我们可以在无模式数据库中以合法名称的键值存放数据，但是这也就假定了程序需要知道这些字段，除非我们只是无脑地遍历数据结构依次打印键值对，然而这基本没有什么意义。 应用程序代码中的隐含模式也会带来一些问题，它意味着要想理解数据库中的数据，必须深入研究应用程序的代码才行，如果代码很清晰易懂，那么你可以根据它推断出数据的模式，然而这一点却无法保证，因为这完全取决于你的代码是否清晰。举个简单的例子，某个字段，在一些聚合中保存为字符串，在另一些聚合中保存为对象(因为数据库是无模式的，理论上完全可能出现这种情况)，如果你不知晓代码的逻辑，基本无法判断出这个字段的这两种数据类型究竟有什么区别。而且，由于数据库感知不到模式，也就无法自行验证数据，这就少了一道保障。不同应用程序可能也会以完全不同的方式使用某个字段。当然了，在验证数据方面，我们还必须在所有使用这个数据库的应用程序中都加上完全相同的验证逻辑来确保数据正确性和安全性。 从本质上说，无模式数据库是把模式交由访问其数据的应用程序代码来处理，如果有多个应用程序需要访问同一个数据库，可能会出现问题。一种缓解此问题的方法是把数据互动操作封装成独立的应用程序，并通过web服务的形式将它和其他应用程序集成。另一种方法是在聚合中明确划分出不同应用程序的区域。 物化视图 关系型数据库可以利用视图来展示一些需要复杂计算才能给出的数据，视图就好比一张关系表，只不过它是通过基表计算得来的，在访问视图时，数据库会计算出视图中的数据，这种形式的封装很方便。有了视图这种机制，客户端不需要担心它访问到的是基本数据还是派生数据，不过生成视图需要大量计算，比较消耗性能。 物化视图是一种事先计算好并缓存起来的视图，如果数据读取频繁，且对实时性要求不高的话，可以使用物化视图来优化性能。在NoSQL中，可以利用物化视图来处理这种需求。面向聚合的数据库更强调这个问题，因为大多数应用程序都要处理某种与聚合机构不相符的查询操作。 构建物化视图有两种方法，第一种是在每次基础数据变更的时候都去更新物化视图，如果读取物化视图的次数远比写入的多，且想获得更为实时的数据，那么这种方法比较合适。第二种是通过批处理来定时更新物化视图，这需要根据业务需求来制定方案，需要考察业务能容忍过时多久的数据。 面向聚合的数据库通常能够用不同方式重组主聚合的数据，以计算出各种物化视图，计算过程一般使用map-reduce的方式。 构建数据存储模型 使用列族数据库建模时，应该按照查询需求而不是写入需求来处理。建模的通则是要便于查询，而且在写入数据时要对其执行“反规范化”操作。 第4章 分布式模型 面向聚合数据库非常适合于横向扩展方式，因为聚合此时就自然成了数据分布单元。 数据分布有两条途径： 复制 分片 复制，就是将同一份数据拷贝至多个节点。分片，就是将不同数据存放在不同节点中。复制和分片是两项正交的技术，它们既可以选其一使用，也可以结合使用。 单一服务器 在大多数情况下，最简单的分布形式就是没有分布，采用单一服务器的形式，特点是简单。 分片 分片是把数据的各个部分放在不同的服务器(节点)中，以此实现横向扩展。分片技术能有效实现负载均衡。为了最大程度地利用分片技术，我们必须保证需要同时访问的数据都存放在同一个节点上，并且节点必须排布好这些数据块，使访问速度最优。这些措施包括，使服务器服务于地理位置上较近的用户、最大程度实现负载均衡，某些情况下可以把需要依次读取的聚合放在一起。 经常有人通过应用程序来处理分片，比如把用户姓氏首字母按照A-D，E-G之类的方式存放在不同分片中，但这样做就把编程模型复杂化了，因为应用程序需要把查询操作分布到多个分片上。如果想调整分片，就需要修改应用程序的相关逻辑并迁移数据。很多NoSQL数据库都提供了“自动分片”来让数据库自己负责把数据分布到各分片，并将查询请求路由至适当分片中。 单一使用分片技术在应对数据库故障时比较力不从心，一旦某个节点崩溃，则该分片上的数据就不可访问，虽说可能只是其中一个分片出问题，但比较已经无法对外提供完整的服务了。后面会看到采用分片结合复制的技术构造冗余可以缓解此问题。 主从复制 在主从复制中，有一个节点叫节点，其余的叫从节点。主节点存放权威数据，复制操作就是要让从节点和主节点的数据保持同步。 主节点负责处理数据的读写，从节点只负责数据读。这样只要增加从节点就可以实现横向扩展。 可以采用手动指定主节点和自动选择主节点两种模式，手动指定需要我们自己配置集群，自动选择比较简单，会让它们自动选举出主节点，如果之后这个主节点崩溃了，会自动指派新的主节点，减少故障时间，手动指定则无此福利。 主从复制有助于增强读取操作的故障恢复能力。 主从复制对于写入操作非常频繁的场合，虽然能将读取操作分流，稍稍缓解主节点写入数据的压力，但对写入操作的性能并没有什么显著的提升。 复制技术再带来好处的同时也有一个致命弱点，就是数据的不一致。如果主节点更新了一个数据，但还未通知从节点，那么用户从从节点读出的数据就不是最新的，甚至由于各从节点同步速度的快慢数据还是各异的。更极端的情况是，主节点崩溃了，此时有部分数据尚未同步到从节点，这部分数据就会丢失。 主节点仍然是系统的瓶颈和弱点。 对等复制 对等复制中的所有节点都是平等的，不存在主从节点一说，所有节点都可以接受读写请求，节点会将自身的写入操作通知给其他所有节点。 增加节点可以轻易提升性能。 对等复制也存在数据一致性问题，由于两个不同节点可以同时处理写入操作，所以可能出现两位用户同时对同一条记录的情况，这就导致“写入冲突”。数据读取的不一致性也存在，但持续时间相对较短，因为最终都会归于一致，写入不一致却总是存在。 写入不一致的几种解决方案思路： 不管何时写入数据，各副本之间总能相互协调，确保不发生冲突，这需要花费一定的网络流量来协调写入操作。 设法处理这些不一致的写入操作，比如合并这些操作。 结合分片和复制技术 每个系统有多个主节点，但对于每项数据来说，负责它的主节点又只有一个。根据配置的需要，同一个节点既可以作为某些数据的主节点，也可以充当其他数据的从节点，也可以指派全职的主节点和从节点。 第5章 一致性 面向集群的NoSQL数据库需要面对一致性的问题，关系型数据库试图通过“强一致性”来避免各种不一致问题，在NoSQL领域中，需要我们自己思考系统需要何种一致性。 更新一致性 多个请求同时更新同一条数据会产生写冲突问题，在并发环境下维护数据一致性一般有两种方式：悲观方式和乐观方式。 悲观方式，就是要避免冲突。常见做法是使用写入锁，确保同一时间只有一个请求可以获取该锁。但注意可能导致死锁问题。 乐观方式，就是允许冲突发生，然后检测冲突并对发生冲突的操作排序，再进一步处理。通常使用“条件更新”，就是任意用户在执行更新操作前，都要先检测数据的当前值和上一次读入的是否相同。如果相同，就更新，如果不同，则更新失败，此时需要先更新到最新在做更新。可参考git等分布式版本管理系统处理冲突的方式。 悲观和乐观的方式，都有一个先决条件，就是更新顺序必须一致，这在单机环境下显然成立，但在分布式集群下，可能两个节点会以不同的顺序执行操作，最终数据就会不一致。 在分布式系统的并发问题上，需要有“顺序一致性”，就是所有节点都要保证以相同的次序执行操作。 读取一致性 数据库必须具有更新一致性，但就这样还不够，它未必能保证用户所提交的访问请求总是能得到一致的响应。典型场景是一个用户A的一个更新动作需要顺序操作两张表的数据，如果是面向聚合的数据库则此处无法使用ACID事务，因此是依次更新。如果在更新完第一张表但还未更新第二张表时，用户B访问了第二张表的那条数据，就会看到一个不一致的数据。这种一致性叫“逻辑一致性”。 并不是所有NoSQL数据库都不支持ACID事务，只有面向聚合的数据库不支持，图数据库是支持ACID事务的。 面向聚合数据库可以“原子地”更新一个聚合的数据，但仅限于单一聚合内部。所以说，“逻辑一致性”可以在某个聚合内部保持，但各聚合之间则不行。在多个聚合之间更新数据就存在一个时间空档，在此空档内读出的相关数据不满足“逻辑一致性”，这个空档叫“不一致窗口”。NoSQL系统的“不一致窗口”一般很短暂。 在引入复制的场景下，就会遭遇一种全新的不一致问题，叫“复制不一致”。如果有多个节点，在个节点上更新了数据，在所有节点尚未全部同步数据前，就会有部分用户访问到过期的数据。但最终，更新还是会传播到所有节点上，这叫“最终一致性”。 “复制一致性”和“逻辑一致性”虽说是两个独立的问题，不过如果“复制”过程中的“不一致窗口”太长，就会加剧“逻辑不一致”问题。两个时间间隔很短且内容不同的更新操作，在主节点中留下的“不一致窗口”也就几毫秒而已，但由于网络延迟，这个“不一致窗口”在从节点上会比在主节点上长得多。 “会话一致性”是指在用户会话内部保持“照原样读出所写内容的一致性”。要确保“会话一致性”，其中一种方法是使用“粘性会话”，就是把会话绑定到某个固定的节点，但缺点是会降低负载均衡器的效率。另一种方法是使用“版本戳”，这个之后会详述。 放宽“一致性”约束 一致性很重要，不过有时必须舍弃它。在设计系统时，我们经常需要牺牲一致性来换取其他特性。 关系型数据库一般用事务来加强一致性，但是事务会影响系统性能。 CAP定理，其中CAP的意思是 一致性(Consistency)，具体含义之前说过。 可用性(Availability)，这里可以指响应的效率，或者说延时。 分区耐受性(Partition tolerance)，如果发生通信故障，导致整个集群被分割成多个无法通信的分区时(也叫脑裂)，集群仍然可用。 CAP定理所表述的是，给定一致性、可用性和分区耐受性这三个属性，我们只能同时满足其中两个属性。 放宽“持久性”约束 某些数据可以不持久化或延迟持久化，比如用户session或者一些临时数据可以保存在redis中，生成和更新非常频繁但又不是那么重要的数据可以延迟持久化，比如定时持久化写入。 是否放宽“持久化”约束需要根据具体需求来确定。 仲裁 假设某份数据需要复制到3个节点中，为了保证”强一致性”，不需要所有节点都确认写入操作，只需其中两个节点(超过半数)确认即可。就是说如果发生两个发生冲突的写入操作，那么只有其中一个操作能为超过半数的节点所认可(W&gt;N/2)。这就叫写入仲裁。 读取仲裁，是指想要读取保证能够读到最新的数据，必须联系多少个节点才行。 在采用“复制”技术的分布式模型中执行数据操作时，无需联系所有副本，只要为足够多的副本所认可，就能保持“强一致性”了。 执行读取操作时所需联系的节点数(R)、确认写入操作时需要联系的节点数(W)和复制因子(N)之间可以用一个不等式来表达：R+W&gt;N。 第6章 版本戳 版本戳用户标识数据的版本，典型情况是使用计数器版本戳，如果当前数据库中某条数据的版本戳是3，而用户请求更新的数据版本戳是2，说明在用户上一次读取到更新之间，该条数据发生了一次更新，可能是有其他人在此时更新了数据，所以这个用户的更新会失败。 版本戳一般可以使用计数器、GUID、内容哈希值、上一次更新的时间戳来表示，这几种方案各有优劣。也可以结合起来使用，比如CouchDB的版本戳就结合使用了内容哈希和计数器。 除了可以避免“更新冲突”之外，版本戳也有助于维护“会话一致性”。 在分布式环境中，可以使用“由版本戳构成的数组”，来检测不同节点之间是否发生了“相互冲突的更新操作”。 第7章 Map-reduce(映射-化简) 映射-化简是一种在集群上执行并发计算所用的模式。 映射操作从聚合中读出数据，将之缩减为相关键值对。映射操作每次只能读取一条记录，所以可在存放记录的节点上并发执行。 映射任务会生成很多具备同一关键字的值，而化简任务则将它们化简为单一的输出值。每个化简函数只操作与单个键相关的映射结果，所以多个化简函数可以根据关键字执行并发化简。 输入数据与输出数据形式相同的多个“化简函数”可归并为“管道”，以提高并发执行能力，并减少所需传输的数据量。 若某个“化简输出”的结果是下一个“映射操作”的输入，那么可以用“管道”组合映射-化简操作。 如果需要广泛使用映射-化简的计算结果，那么可以将其存储为“物化视图”。 可使用增量式映射-化简操作来更新“物化视图”，这样做只需要计算视图中发生改变的那部分数据即可，无需把全部数据从头算一遍。 第8章 键值数据库键值数据库和关系型数据库的对比 Oracle Raik 数据库实例 Raik集群 表 存储区 行 键值对 rowid 键 什么是“键值数据库” 从API的角度来看，键值数据库是最简单的NoSQL数据库。客户端可以根据键查询值，设置键所对应的值。 Redis等键值数据库中，所存储的数据不一定非要是领域对象，任何数据结构都行。Redis可以存储list、set、hash等数据结构，还可以求差集、并集、交集和获取某个范围内的数值。 键值数据库特性 一致性，只有针对单个键的操作才具备一致性，一般就是set、get或del。 事务，不同键值数据库其事务规范不同，一般无法保证写入一致性。比如Raik采用仲裁。 查询功能，所有键值数据库都可以按照关键字来查询，但无法根据列值来查询。 数据结构，键值数据库一般不关心值，值可以是二进制、文本、JSON等。 可扩展性，很多键值数据库都可以采用分片技术。采用这种技术后，键的名字决定了负责存储该键的节点。像Raik这样的数据库，可以控制“CAP”定理中的参数，N(存放键值对的副本节点数)、R(顺利完成读取操作所需的最小节点数)和W(顺利完成写入操作所需的最小节点数)。 适用案例 存放会话信息 用户配置信息 购物车数据 不适用场合 数据间关系 含有多项操作的事务 查询数据 操作关键字集合 第9章 文档数据库文档数据库和关系型数据库的对比 Oracle MongoDB 数据库实例 MongoDB实例 模式 数据库 表 集合 行 文档 rowid _id join DBRef 特性 一致性，为了在MongoDB数据库中确保一致性，可以配置副本集，也可以规定写入操作必须等待所写数据复制到全部或是给定数量的从节点后，才能返回。提升一致性会降低写入效率。可以配置以增加副本集的读取效率。 事务，只支持单文档级别的事务，即原子事务。 可用性，文档数据库试图用主从式数据复制技术来增强可用性。多个节点保有同一份数据，即使主节点故障，客户端也依然可以获取数据，应用程序一般不需要检测主节点是否可用。所有请求都由主节点处理，而其数据会复制到从节点。如果主节点故障，副本集中剩余的节点会在其自身范围内选举出新的主节点。副本集通常用于处理数据冗余、自动故障切换、灾难恢复等事项。 查询功能，文档数据库可以查询文档中的数据，而不用像键值数据库那样，必须根据关键字获取整个文档，然后再检视其内容。MongoDB还可以基于“内嵌子文档”来查询。 可扩展性，增加更多的“读取从节点”，将全部读取操作都引导至从节点上，这样可以扩展数据库应对频繁读取的能力了。如果想扩展写入能力，则可以把数据分片，分片操作根据特定字段来划分数据(该字段的选择很重要)，这些数据要移动到不同的节点中。为了让各分片负载保持均衡，需要在节点之间动态转移数据，向集群中加入更多节点，并提高可写入的节点数，就可以横向扩展写入能力。把每个分片都做成副本集可以提高读取效率。 适用案例 事件记录 内容管理系统和博客平台 网站分析和实时分析 电子商务应用程序 不适用场合 包含多项操作的复杂事务 查询持续变化的聚合结构 第10章 列族数据库 关系型数据库 Cassandra 数据库实例 集群 数据库 键空间 表 列族 行 行 列 列 特性 一致性，Cassandra收到写入请求后，会先将待写入数据记录到“提交日志”中，然后将其写入内存中一个名为“内存表”的结构中。写入操作在写入“提交日志”和“内存表”后就算成功了。写入请求成批堆积在内存中，并定期写入一种叫做“SSTable”的结构中，该结构中的缓存一旦写入数据库，就不会再向其继续写入了。若其数据变动，则需新写一张SSTable。无用的SSTable可由“压缩”操作回收。 事务，Cassandra没有传统意义上的事务，它的写入操作在行级别的原子的。 可用性，Cassandra具备高可用性，因为集群中没有主节点，减少操作请求的一致性即可提示集群的可用性。可用性受制于 R+W&gt;N 这一公式。W是成功执行写入操作所需的最小节点数，R是顺利执行读取操作所需获取的最小应答节点数，N是参与数据复制的节点数。 查询功能，Cassandra没有功能丰富的查询语言。在列族插入数据后，每行中的数据都会按照列名排序。如果一列的获取次数比其他列更加频繁，可以考虑将其值用作行健以提高性能。 基本查询，有GET、SET和DEL。 高级查询和索引编订，Cassandra列族可以用关键字以外的其他列当索引。这些索引以位映射图的形式出现，在列中频繁出现重复值的情况下效果较好。 Cassandra查询语言(CQL)，提供查询功能，但未包含SQL的全部功能。 可扩展性，由于没有主节点，所以只要向Cassandra集群中新增节点就能改善其服务能力。 适用案例 事件记录 内容管理系统和博客平台 计数器 限期使用 不适用场合 需要以“ACID事务”执行写入和读取操作的系统。 根据查询结构聚合数据的场景。 开发早期、原型期和技术初探期。开发初期无法确定查询模式的变化情况，查询模式一旦变化，列族的设计也要随之修改。注意，关系型数据库改变数据模式的成本很高，但查询模式的修改成本较低，Cassandra则相反。 第11章 图数据库特性 一致性，由于图数据库操作互相连接的节点，所以大部分图数据库通常不支持把节点分布在不同服务器上。图数据库通过事务来保证一致性，它们不允许出现“悬挂关系”：所有关系必须具备起始节点和终止节点，而且在删除节点前，必须先移除其上的关系。 事务，Neo4J是兼容ACID事务的数据库，修改节点或向现有节点新增关系前，必须先启动事务。 可用性，Neo4J从1.8开始，支持“副本从节点”，这些从节点可以处理写入操作，向其写入后，它会先将所写数据同步至当前主节点，然后主节点再同步至其他从节点。也可以配合ZooKeeper来记录每个丛节点和当前主节点中最新的事务ID。 查询功能，图数据库可以使用Gremlin等查询语言，Neo4J还可以使用Cypher语言来查询图。 可扩展性，图数据库要运用分片技术比较难，因为它并不是面向聚合的，而是面向关系的。由于任何节点都有可能关联其他节点，因此把相关节点放在同一台机器上，遍历图时比较方便，放在多台机器上性能不好。扩展图数据库一般有三种方式： 给服务器配置足量内存，使之完全可以容纳“工作集”中的全部节点和关系。只有在这些内存量的数值比较合理时，这项技术才有用。 增加仅能读取数据的从节点，即可改善数据的读取能力，所有写入操作仍由主节点负责。 若数据集太大，导致多节点复制不大现实，可采用“领域特定知识”在应用程序端对其分片，比如按照地理位置分片等。 适用场合 互联数据 安排运输路线、分派货物和基于位置的服务 推荐引擎 不适用的场合 更新全部或某个子集内实体的场合 涉及整张图的操作 第12章 模式迁移 若要迁移关系型数据库等“强模式”的数据库，可将历次模式变更及其数据迁移操作保存于“版本控制序列”中。 因程序代码要依照“隐含模式”来访问无模式数据库的数据，故其数据迁移仍需谨慎处理。 无模式数据库亦可借用强模式数据库的迁移技术。 无模式数据库可使用“增量迁移”技术更新数据，以便在不影响应用程序读取数据的前提下，修改数据的隐含模式。 第13章 混合持久化 混合持久化旨在使用不同数据库技术处理多种数据存储需求。 混合持久化既可为企业中多个程序所用，也可以运用在单个应用程序中。 将数据访问封装成服务，可以减少数据库变动对系统其它部分的影响。 新增数据库技术会让编程和操作更复杂，所以要权衡引入新数据库带来的好处和引入它带来的复杂度的利弊。 第14章 超越NoSQL 文件系统 事件溯源 内存映像 版本控制 XML数据库 对象数据量 第15章 选择合适的数据库 通过使用更符合应用程序需求的数据库来改善程序员的工作效率。 以能处理大数据量、降低延迟且增进数据吞吐量的某种技术组合来改善数据访问性能。 在决定使用某个NoSQL技术之前，一定要测试其是否如预期般改善了程序员工作效率和数据访问性能。 用服务来封装数据库，即能在需求变更或技术成熟后改换其所封装的数据库技术。可将应用程序各部分划分到不同服务中，以便为既有程序引入NoSQL数据库。 大部分应用程序，尤其是“非战略性的”应用程序，应该继续使用关系型数据库技术，至少在NoSQL技术环节尚未更加成熟时是如此。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://nullcc.github.io/categories/读书笔记/"}],"tags":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://nullcc.github.io/tags/NoSQL/"}]},{"title":"深入理解JavaScript中的this关键字","slug":"深入理解JavaScript中的this关键字","date":"2017-05-07T16:00:00.000Z","updated":"2022-04-15T03:41:13.034Z","comments":true,"path":"2017/05/08/深入理解JavaScript中的this关键字/","link":"","permalink":"https://nullcc.github.io/2017/05/08/深入理解JavaScript中的this关键字/","excerpt":"如果你问js的初学者js中什么东西比较难懂，this关键字应该会是众多回答中的一个。对于不了解this绑定规则的人来说，this常常使他们感到费解：this到底指代什么？会使人产生这种迷惑感是因为this在不同场景下所指向的对象不同。这里将介绍四种this的绑定规则，理解了这四种绑定规则，就可以对this了如指掌。","text":"如果你问js的初学者js中什么东西比较难懂，this关键字应该会是众多回答中的一个。对于不了解this绑定规则的人来说，this常常使他们感到费解：this到底指代什么？会使人产生这种迷惑感是因为this在不同场景下所指向的对象不同。这里将介绍四种this的绑定规则，理解了这四种绑定规则，就可以对this了如指掌。 四种this绑定规则 默认绑定 隐式绑定 显式绑定 new绑定 默认绑定默认绑定是无法应用其他调用规则时的绑定方式，看如下代码： 12345var a = 1;function foo()&#123; console.log(this.a);&#125;foo(); // 1 1234567\"use strict\"var a = 1;function foo()&#123; console.log(this.a);&#125;foo(); // TypeError: Cannot read property 'a' of undefined 这是最基本的一个函数调用，在第一张图中，非严格模式下，this绑定到全局对象，因此this.a指向全局变量a。第二段代码中，严格模式下，全局对象无法使用默认绑定，因此this会绑定到undefined。 那么如何判断是默认绑定呢，其实很简单，我们观察foo的调用位置，这里foo是直接被调用的，foo没有被引用到任何其他对象或着被显式绑定到指定对象(显式绑定稍候会说明)，因此只能使用默认绑定规则。 隐式绑定隐式绑定需要考虑调用位置是否有上下文对象，或者说是被某个对象包含或拥有，比如以下代码： 12345678910function foo() &#123; console.log(this.a);&#125;const obj = &#123; a: 1, foo: foo,&#125;;obj.foo(); // 1 在声明obj时，包含了foo，因此调用obj.foo()时，this绑定到obj，this.a就是obj.a。如果有多个层级的包含关系，this会绑定到最后一层的上下文对象上，比如以下代码： 123456789101112131415function foo() &#123; console.log(this.a);&#125;const obj2 = &#123; a: 2, foo: foo,&#125;;const obj1 = &#123; a: 1, obj2: obj2,&#125;;obj1.obj2.foo(); // 2 此时this会绑定到obj2上。 还有一种情况，叫隐式绑定丢失，看如下代码： 123456789101112function foo() &#123; console.log(this.a);&#125;const obj = &#123; a: 1, foo: foo,&#125;;const a = 'global';const bar = obj.foo;bar(); // 严格模式下是undefined，非严格模式下是global 这里指定了bar为obj.foo的一个别名(或者说引用)，也就是说，这里bar实际上引用的是foo本身，所以这里调用bar()相当于一个默认绑定，适用于上面讲到的默认绑定规则。如果确实需要函数别名并且把this绑定到指定的对象上，可以使用显式绑定，比如bind、call、apply之类的，后面会陆续谈到。 隐式绑定丢失还会出现在传入回调函数的时候： 123456789101112131415function foo() &#123; console.log(this.a);&#125;function caller(func) &#123; func();&#125;const obj = &#123; a: 1, foo: foo,&#125;;var a = 'global';caller(obj.foo); // 严格模式下是undefined，非严格模式下是global 在前端的js编程中，由于是事件驱动的，调用回调函数经常发生在用户交互之后，由于绑定丢失，我们经常需要手动把this绑定到某个对象上： 1this.onClickBtn = this.onClickBtn.bind(this); 显式绑定如果我们想在某个对象上强制调用函数，可以是用显式绑定。js中的函数的原型是Function对象，它提供了一些通用方法。就显式绑定来说，我们可以使用apply和call这两个方法，具体用法是： 12func.apply(obj, [arg1, arg2,...]);func.call(obj, arg1, arg2,...); apply和call只是在传参格式上不一样而已，其他方面完全一样。apply和call都是将func的this绑定到第一个参数obj上。看以下代码： 12345678910function foo() &#123; console.log(this.a);&#125;const obj = &#123; a: 1,&#125;;const a = 'global';foo.call(obj); // 1 此时foo的this绑定到了obj上面。 apply和call的第一个参数也可以是null，即不绑定到任何对象，但实际上在非严格模式下这样做会绑定到全局对象： 12345678910function foo() &#123; console.log(this.a);&#125;const obj = &#123; a: 1,&#125;;const a = 'global';foo.call(null); // 严格模式下是undefined，非严格模式下是global 虽然apply和call可以把this绑定到指定对象，但是还是没有解决回调函数的问题，因为apply和call都是立刻执行的，而回调函数的执行时间是不确定的。而且回调函数的上下文也是不确定的，在回调函数的上下文中可能很难获得我们想要的那个this绑定对象。 为了解决回调函数绑定丢失的问题，我们可以使用硬绑定bind。bind很有用，它可以对this强制绑定一个对象，而且绑定后无法修改。这对我们事件驱动的编程模型很有帮助，可以大量运用在回调函数中。另外bind在js的函数式编程中也是一项利器。看以下代码： 123456789101112131415function foo()&#123; console.log(this.a);&#125;const obj1 = &#123; a: 1,&#125;;const obj2 = &#123; a: 2,&#125;;const bar = foo.bind(obj1); // bar的this永远只会指向obj1bar(); // 1bar.call(obj2); // 1 因为无法改变bind后的this绑定，所以还是1 new绑定new绑定是使用new操作符对函数进行调用产生的绑定。js中的new和其他面向对象编程语言的new不同。一般的面向对象语言中new操作符会调用类的构造函数，生成一个全新的类实例。js中没有类，也没有构造函数，我们在ES 6中看到的class和contructor方法更多地还是使用js模仿传统OOP的构造对象的方式，实际情况大相径庭，这部分内容(prorotype)可以说是js中最具迷惑性也最多人误解的一件事，我将在之后的文章中详细解释prorotype。回到正题，实际上使用new操作符调用函数实际上做了以下4件事情： (1) 创建一个全新的对象。 (2) 这个新对象会和它的原型对象进行连接。 (3) 这个新对象会被绑定到函数调用的this。 (4) 如果函数没有返回其他对象，那这个new表达式将自动返回这个新对象。 代码如下： 123456function foo(a) &#123; this.a = a;&#125;const bar = new foo(1);console.log(bar.a); // 1 总结本文介绍了this的四种绑定情形。需要再次强调的是，判断this的绑定，不要看函数被定义的地方，而要看函数被调用的地方，或者说上下文。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"js","slug":"js","permalink":"https://nullcc.github.io/tags/js/"}]},{"title":"理解JavaScript的立即调用函数表达式(IIFE)","slug":"理解JavaScript的立即调用函数表达式(IIFE)","date":"2017-05-07T16:00:00.000Z","updated":"2022-04-15T03:41:13.036Z","comments":true,"path":"2017/05/08/理解JavaScript的立即调用函数表达式(IIFE)/","link":"","permalink":"https://nullcc.github.io/2017/05/08/理解JavaScript的立即调用函数表达式(IIFE)/","excerpt":"首先这是js的一种函数调用写法，叫立即执行函数表达式(IIFE，即immediately-invoked function expression)。顾名思义IIFE可以让你的函数立即得到执行(废话)。","text":"首先这是js的一种函数调用写法，叫立即执行函数表达式(IIFE，即immediately-invoked function expression)。顾名思义IIFE可以让你的函数立即得到执行(废话)。 一般来说，IIFE有以下几种用途： 创建只使用一次的函数，并立即执行它。 创建闭包，保存状态，隔离作用域。 作为独立模块存在(例子如jQuery)，防止命名冲突，命名空间注入(模块解耦)。 1.创建只使用一次的函数，并立即执行它创建只使用一次的函数比较好理解，在需要调用函数的地方使用IIFE，类似内联的效果： 1234(function()&#123; var a = 1, b = 2; console.log(a+b); // 3&#125;)(); 还可以传入参数： 1234(function(c)&#123; var a = 1, b = 2; console.log(a+b+c); // 6&#125;)(3); IIFE比较常见的形式是匿名函数，但是也可以是命名的函数： 123(function adder(a, b)&#123; console.log(a+b); // 7&#125;)(3, 4); 在js中应该尽量使用命名函数，因为匿名函数在堆栈跟踪的时候会造成一些不便。 2.创建闭包，保存状态，隔离作用域隔离作用域比较复杂一点，在ES6以前，JS没有块级作用域，只有函数作用域，作为一种对块级作用域的模拟就只能用function模拟一个作用域，比如如下代码： 123456789101112131415161718var myBomb = (function()&#123; var bomb = \"Atomic Bomb\" return &#123; get: function()&#123; return bomb &#125;, set: function(val)&#123; bomb = val &#125;, &#125;&#125;)()console.log(myBomb.get()) // Atomic BombmyBomb.set(\"h-bomb\")console.log(myBomb.get()) // h-bombconsole.log(bomb) // ReferenceError: bomb is not definedbomb = \"none\"console.log(bomb) // none 可以看到一个比较奇特的现象，按照常理，一个函数执行完毕，在它内部声明的变量都会被销毁，但是这里变量bomb却可以通过myBomb.get和myBomb.set去读写，但是从外部直接去读和写却不行，这是闭包造成的典型效果。要清楚解释闭包到底是什么，这里有一篇文章学习Javascript闭包（Closure），上面的代码已经用到了闭包。所有闭包都有一个特点，就是可以通过导出方法从函数外部改变函数内部变量的值，因此可以利用这个特点来隔离作用域，模拟一种“私有”的效果。 举一个IIFE保存变量的例子，我们要写入三个文件，先定义了一个内容数组，然后用for循环遍历这个数组写入文件，最后依次用for循环的下标打印出”File i is written.”： 1234567891011var fs = require('fs');var fileContents = [\"text1\", \"text2\", \"text3\"];for (var i = 0; i &lt; fileContents.length; i++) &#123; fs.writeFile(\"file\"+i+\".txt\", fileContents[i], function(err)&#123; if (err) &#123; console.log(err) &#125; console.log(\"File \" + i + \" is written.\") &#125;)&#125; 这段代码结果是： File 3 is written.File 3 is written.File 3 is written. 很明显和我们的意愿相违背，打印了3次”File 3 is written.”。我们希望的是每个文件的下标索引打印一次。 原因在于写文件是个异步操作，在写完文件调用回调函数时，for循环已经遍历完毕，此时i=3。要解决这个问题，可以使用IIFE： 1234567891011121314var fs = require('fs');var fileContents = [\"text1\", \"text2\", \"text3\"];for (var i = 0; i &lt; fileContents.length; i++) &#123; (function(index)&#123; var fileIndex = index; fs.writeFile(\"file\"+fileIndex+\".txt\", fileContents[fileIndex], function(err)&#123; if (err) &#123; console.log(err) &#125; console.log(\"File \" + fileIndex + \" is written.\") &#125;) &#125;)(i)&#125; 这次结果是正确的(尽管不是按序，这不在我们考虑范围内)： File 1 is written.File 2 is written.File 0 is written. 可以看到这里用IIFE做了一个变量捕获，或者说保存。 再回到myBomb那个例子，这其中用到了一个模式，叫Module模式，很多js模块都是这么写，在IIFE中定义一些私有变量或者私有函数，然后在return的时候导出(一般用一个Object导出)需要暴露给外部的方法。另外在IIFE中定义的变量和函数也不会污染全局作用域，它们都通过统一的入口访问。 3.作为独立模块存在，防止命名冲突，命名空间注入(模块解耦)可以使用以下代码为ns这个命名空间注入变量和方法： 123456789var ns = ns || &#123;&#125;;(function (ns)&#123; ns.name = 'Tom'; ns.greet = function()&#123; console.log('hello!'); &#125;&#125;)(ns);console.log(ns); // &#123; name: 'Tom', greet: [Function] &#125; 还可以扩展到更多的用途： 12345678910111213141516171819202122232425(function (ns, undefined)&#123; var salary = 5000; // 私有属性 ns.name = 'Tom'; // 公有属性 ns.greet = function()&#123; // 公有方法 console.log('hello!'); &#125; ns.externalEcho = function(msg)&#123; console.log('external echo: ' + msg); insideEcho(msg); &#125; function insideEcho(msg)&#123; // 私有方法 console.log('inside echo: ' + msg); &#125;&#125;)(window.ns = window.ns || &#123;&#125;);console.log(ns.name); // Tomns.greet(); // hellons.age = 25;console.log(ns.age); // 25console.log(ns.salary); // undefinedns.externalEcho('JavaScript'); // external echo: JavaScript/inside echo: JavaScriptinsideEcho('JavaScript'); // Uncaught ReferenceError: insideEcho is not definedns.insideEcho('JavaScript'); // Uncaught TypeError: ns.insideEcho is not a function 在这里，命名空间可以在局部被修改而不重写函数外面的上下文，起到了防止命名冲突的作用。 注(如果不感兴趣可以直接忽略)：还需要解释一下上面IIFE中第二个参数undefined。在js中，undefined表示值的空缺，是预定义的全局变量，它并不是关键字： 123console.log(typeof a); // undefinedvar a;console.log(a); // undefined undefined有多重含义，第一种是一个数据类型叫做undefined，另一种是表示undefined这个数据类型中的唯一值undefined。我们在js代码中看到的undefined一般是全局对象的一个属性，该属性的初始值就是undefined，另一种情况是，这个undefined是个局部变量，和普通变量一样，它的值可以是undefined，也可以是别的。 在ECMAScript 3中undefined是可变的，这意味着你可以给undefined赋值，但在ECMAScript 5标准下，无法修改全局的undefined： 123console.log(window.undefined); // undefinedwindow.undefined = 1;console.log(window.undefined); // undefined 严格模式下则会直接报错： 12345'use strict'console.log(window.undefined); // undefinedwindow.undefined = 1;console.log(window.undefined); // Uncaught TypeError: Cannot assign to read only property 'undefined' of object '#&lt;Window&gt;' 因此我们需要保护这个局部的undefined： 123(function (window, document, undefined) &#123; // ...&#125;)(window, document); 这时候就算有人给undefined赋值也没有问题： 1234undefined = true;(function (window, document, undefined) &#123; // undefined指向的还是一个本地的undefined变量&#125;)(window, document); 不过随着ECMAScript 5的普及(现在几乎没有哪款浏览器不支持ECMAScript 5了)，这种担忧基本没有必要了，jQuery也是为了最大程度的兼容性才这么做。 以上例子说明我们可以把命名空间作为参数传给IIFE，以对其进行扩展和装饰： 1234567891011121314151617181920212223242526(function (ns, undefined)&#123; var salary = 5000; // 私有属性 ns.name = 'Tom'; // 公有属性 ns.greet = function()&#123; // 公有方法 console.log('hello!'); &#125; ns.externalEcho = function(msg)&#123; console.log('external echo: ' + msg); insideEcho(msg); &#125; function insideEcho(msg)&#123; console.log('inside echo: ' + msg); &#125; &#125;)(window.ns = window.ns || &#123;&#125;);(function (ns, undefined)&#123; ns.talk = function()&#123; console.log(ns.name + ' says hello.'); console.log(ns.name + ' says goodbye.'); // 注意这里不能调用私有函数insideEcho，否则会报错，因为talk和insideEcho不在同一个闭包中 &#125;&#125;)(window.ns = window.ns || &#123;&#125;);ns.talk(); // Tom says hello. Tom says goodbye. 命名空间注入命名空间注入是IIFE作为命名空间的装饰器和扩展器的一个变体，使其更具有通用性。作用是可以在一个IIFE(这里可以把它理解成一个函数包装器)内部为一个特定的命名空间注入变量/属性和方法，并且在内部使用this指向该命名空间： 12345678910111213141516171819202122232425262728var app = app || &#123;&#125;;app.view = &#123;&#125;;(function ()&#123; var name = 'main'; this.getName = function()&#123; return name; &#125; this.setName = function(newName)&#123; name = newName; &#125; this.tabs = &#123;&#125;;&#125;).apply(app.view);(function ()&#123; var selectedIndex = 0; this.getSelectedIndex = function()&#123; return selectedIndex; &#125; this.setSelectedIndex = function(index)&#123; selectedIndex = index; &#125;&#125;).apply(app.view.tabs);console.log(app.view.getName()); // mainconsole.log(app.view.tabs.getSelectedIndex()); // 0app.view.tabs.setSelectedIndex(1);console.log(app.view.tabs.getSelectedIndex()); // 1 我们还可以写一个模块构造器来批量生产模块： 1234567891011121314151617181920212223var ns1 = ns1 || &#123;&#125;, ns2 = ns2 || &#123;&#125;;var creator = function(val)&#123; var val = val || 0; this.getVal = function()&#123; return val; &#125; this.increase = function()&#123; val += 1; &#125; this.reduce = function()&#123; val -= 1; &#125; this.reset = function()&#123; val = 0; &#125;&#125;creator.call(ns1);creator.call(ns2, 100);console.log(ns1.getVal()); // 0ns1.increase();console.log(ns1.getVal()); // 1console.log(ns2.getVal()); // 100 对某个私有变量，用API的形式对其进行读写，这其实就是OOP的一些思想在js的应用了。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"js","slug":"js","permalink":"https://nullcc.github.io/tags/js/"}]},{"title":"谈谈Golang中goroutine的调度问题","slug":"谈谈golang中goroutine的调度","date":"2017-05-07T16:00:00.000Z","updated":"2022-04-15T03:41:13.043Z","comments":true,"path":"2017/05/08/谈谈golang中goroutine的调度/","link":"","permalink":"https://nullcc.github.io/2017/05/08/谈谈golang中goroutine的调度/","excerpt":"goroutine的调度问题，同样也是我之前面试的问题，不过这个问题我当时并不是很清楚，回来以后立马查阅资料，现整理出来备忘。","text":"goroutine的调度问题，同样也是我之前面试的问题，不过这个问题我当时并不是很清楚，回来以后立马查阅资料，现整理出来备忘。 有一些预备知识需要说明，就是操作系统中的线程。操作系统中的线程分为两种：内核线程和用户线程。用户平时使用的线程并不是内核线程，而是存在于用户态的用户线程。用户线程并不一定在操作系统内核中对用同等数量的内核线程。这里有三个模型： 1.一对一模型(1:1) 2.多对一模型(N:1) 3.多对多模型(N:M) 下面就先来谈谈这三种线程模型。 1.一对一模型(1:1)对于支持线程的操作系统来说，一对一模型是最简单的一种线程模型了，一个用户线程唯一对应一个内核线程，但反过来却不一定，一个内核线程并不一定有对应的用户线程存在。这样一来，由于一个内核线程至多只对应一个用户线程，线程之间可以做到最大程度的并发，不同线程之间不会相互影响，比如一个线程阻塞了也不会影响到其他线程的执行。对于多处理器，一对一的线程模型效率更高。但是很多操作系统限制了内核线程的数量，如果采用一对一模型，用户线程的数量也会受到比较大的限制。而且很多操作系统的内核线程在调度时开销较大，这也会影响用户线程的效率。 2.多对一模型(N:1)多对一模型意味着多个用户线程对应一个内核线程，用户线程间的切换由代码控制，因为线程间切换的效率比较高(不用陷入内核区去切换)。不过如果其中一个用户线程阻塞了，则和它对应相同内核线程的那些用户线程也都会阻塞，因为内核线程是被共用的(且是绑定的)，此时它无法抽身出来。而且增加处理器个数对于多对一线程模型帮助也不大，毕竟在这种情况下，一个线程阻塞，相关线程也跟着遭殃的事实和处理器个数关系不大。这种模型的好处是线程间切换开销低，且线程数量可以很多。 3.多对多模型(N:M)多对多线程模型可以说是上面两种模型的结合，也是最复杂的，它把多个用户线程对应到多个内核线程上，且很多时候不是唯一绑定的。因此一个内核线程在一个时间点可以对应0到多个用户线程。且在运行期间，系统可以根据线程执行情况做合理分配。比如用户线程1、用户线程2和用户线程3对应到一个内核线程1，如果用户线程1阻塞了，系统可以调度用户线程2和用户线程3到其他内核线程上去，这是个动态的过程。多对多线程模型的优势是可以让系统资源得到比较均衡的使用，用户线程之间互相影响比较小，且在多处理器上表现不错(虽然增加处理器个数对它性能提升可能不如一对一模型那么高)，关键是它很灵活。 Golang的goroutine调度和多对多模型密切相关，Golang自己有自己的调度器scheduler。Golang的调度器内部有三个重要结构：M、P和G。 M: 代表内核线程。 G: 代表一个goroutine，它有自己的栈，指令指针和一些基本信息，用于被调度。 P: 代表调度的上下文，是Golang内部的调度器，负责让多个goroutine在一个内核线程上运行，它实现了N:1到N:M。 可以看到在某个时刻，一个M对应到一个P，一个P上有一个正在运行的G(蓝色的G)，且这个P上可能还有多个G等待被调度(灰色的G)，P维护着这个调度队列(runqueue)。P的数量可以通过GOMAXPROCS()来设置，它其实也就代表了真正的并发度，即有多少个goroutine可以同时运行。不过需要注意，GOMAXPROCS()的最大值是256。当要启动一个goroutine时，只需要用go function(args)即可，一但我们启动了一个goroutine，就会在runqueue队尾加入这个goroutine，P会负责调度这些goroutine。 那么如果在某个M被阻塞了呢？这时候就是N:M模型的关键之处了，此时P可以被安排到其他M上去执行，由于P内部维护着一些G的信息，这些G都有独立的栈和指令指针这些基本信息，所以可以很方便地直接换到另一个未被阻塞的M下。 上图描述了这种情况，在左边，G0正在运行，当G0由于系统调用被阻塞时，调度器会创建或者从线程缓存中取得一个线程M1，转投M1。当G0返回时，它必须获得一个P来执行，此时一般是先查看系统中有没有空闲的P，如果有，就获得一个P，用这个P来执行，如果没能获得一个P，这个G0只能暂时放置到一个全局的执行队列(global runqueue)中，它所处的线程M0也就sleep了。系统中的P们会周期性地检查这个队列，取出里面的G来运行。 如上图，还有一种情况是，某个M上的P被分配的G很快就被执行完了，这时M和P都处于闲置状态，无事可做。但也不可能看着其他的P忙碌自己不帮忙吧，此时会先去global runqueue中检查是否有G可以拿，如果并没有，只好尝试去其他P上“偷”一些G来执行，一般就是偷对方runqueue大小的一半，如果还是没偷到，那该只好sleep，该P放入一个闲置的P队列结构等待再次被调度。 这样处理，就保证了系统中的G能被有效快速地执行，充分利用了系统资源。 注：以上部分信息和图片来自The Go scheduler。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://nullcc.github.io/categories/编程语言/"}],"tags":[{"name":"go","slug":"go","permalink":"https://nullcc.github.io/tags/go/"}]},{"title":"CSRF原理浅析","slug":"CSRF原理浅析","date":"2016-09-06T16:00:00.000Z","updated":"2022-04-15T03:41:13.013Z","comments":true,"path":"2016/09/07/CSRF原理浅析/","link":"","permalink":"https://nullcc.github.io/2016/09/07/CSRF原理浅析/","excerpt":"简单介绍了CSRF的原理和防御方式。","text":"简单介绍了CSRF的原理和防御方式。 CSRF简介CSRF是什么CSRF(Cross Site Request Forgery跨站点请求伪造)，又叫one click attack/session riding，一般缩写为CSRF。 CSRF的危害CSRF是一种常见的web攻击方式，但是很多程序员甚至是安全工程师都不太理解它的利用条件和危害，因此CSRF也是web安全中最容易被忽略的一种攻击方式。但是在某些条件下，CSRF能够产生很强的破坏性。 CSRF实际上是盗用了被害者的身份，以被害者的身份发送恶意请求给服务器。常见的CSRF攻击能够造成攻击者以被害者的名义发送消息、邮件、转账、购买物品等等。尽管CSRF的热门程度可能不如XSS高，但是在现在的互联网环境中，CSRF的危害不亚于XSS。 CSRF的基本原理虽然从表面上看它和之前讲到的XSS很相似(因为都有一个Cross Site的缘故)，但是实际上它和XSS的攻击方式不同。XSS是利用网站受信任的用户来对目标用户进行攻击，而CSRF则是伪装成网站受信任用户的请求来利用网站。如图： 从图中我们可以看出，CSRF攻击需要有以下两个步骤： 受害者先登陆受信网站A，网站A在本地生成cookie。 在不登出A(或者A的cookie有效)的情况下，访问恶意网站B。 这个时候一种直观的预防措施可能是，只要不满足上述两点其中一点，就可以防止CSRF攻击了，实际上这种观点存在很多误区，原因有以下几个： 用户无法确保在打开站点A后不再开启新标签浏览站点B； 用户无法确保在不登出站点A的情况下不浏览站点B。 用户不能确保当关闭浏览器后，站点A的cookie立即过期。实际上，很多站点都有这样的特点，当你关闭浏览器时，它的cookie并不立即失效，经常可以看到有的站点在你登陆后关闭浏览器，下次再访问时仍然是已登陆状态(比如微博)。这个是大多数人存在的误区，以为关闭浏览器以后所有cookie立即过期(这个要看网站的设定)。 那些存在“恶意行为”的网站有可能是一个可信任的网站，只是因为可能是存在LD并被利用了。 综合以上几点可以看到，CSRF攻击可以说是防不胜防。 CSRF案例用一个真实例子感受一下CSRF，这里已经搭建好了一个站点，这个网站的后台存在CSRF LD。在这个网站的后台，可以构造一个表单来提交，添加一个管理员账号。我们在一个网站www.b.com根目录下放一个xiao5u.html文件，内容为: 引用的 http://www.b.com/test.html 内容为： 目的是添加一个账号为test，密码为123456的管理员账号。现在先登陆后台，目前只有一个管理员账号： 新开一个标签页，访问 http://www.b.com/xiao5u.html ，抓包： 再回来看后台： CSRF攻击成功，添加了一个test账户的管理员。 CSRF进阶浏览器中的cookie策略在刚才的例子里，CSRF攻击之所以能成功是因为用户的浏览器成功发送了cookie。 浏览器中的cookie分为两种，一种是Session Cookie，又称“临时 cookie”,另一种是“Third-party Cookie”，又称“本地 cookie”。两者的 区别在于，Third-party Cookie是客户端和服务器在交互时，服务器在 Set-Cookie时指定了Expire时间，因此只有过了Expire时间，Third-party Cookie才会失效，所以这种Cookie保存在本地。Session Cookie是没有指 定Expire时间的，所以在浏览器关闭后，Session Cookie就失效了。 在用户浏览网站的过程中，如果一个网站设置了Session Cookie，那么在浏览器进程的生命周期内，即使打开了新的标签页，Session Cookie也是有效的。Session Cookie保存在浏览器进程的内存空间中，Third-party Cookie保存在本地。如果浏览器从一个域的页面中，要加载另一个域的资源，出于安全方面的考虑，有些浏览器会阻止Third-party Cookie的发送。目前默认阻止Third-party Cookie的浏览器有IE 6、IE7、IE8、safari，默认不阻止的有Firefox、Opera、Chrome等。 下面来看一个简单的例子体会一下这种策略： 首先建立一个网站www.a.com，在本地或者互联网上都可以。然后在根目录下建立一个a.php，这个脚本会给浏览器写入两个cookie，一个是Session Cookie，一个是Third-party Cookie： 先用IE 8访问 http://www.a.com/a.php ，通过抓包可以看到： 浏览器接收了这两个cookie。这是不要关闭这个页面，再打开一个新的标签页，访问此域下的其他页面，比如 http://www.a.com ，由于这个新标签页和原来的标签页同属于一个浏览器进程，因此Session Cookie会被发送(当然由于是同域，Third-party Cookie也会被发送)： 建立另一个网站www.b.com，在根目录下创建一个csrf.html，在里面构造一个标签： 再开一个新标签页，访问 http://www.b.com/csrf.html ，抓包： 发现只有Session Cookie被发送了，因为IE浏览器默认会阻止跨域请求时发送Third-party Cookie。实际上，IE出于安全考虑，默认阻止了&lt;img&gt;、&lt;iframe&gt;、&lt;script&gt;、&lt;link&gt;等html标签发送跨域请求时携带Third-party Cookie。 再来看看Firefox，默认允许在发送跨域请求时携带Third-party Cookie： 由此可见，在Firefox中，由于默认不阻止Third-party Cookie的跨域发送，浏览器可以成功发送用于验证的Third-party Cookie，导致CSRF比较容易成功。而对于IE浏览器，攻击者则必须诱使受害者先访问目标站点，使得Session Cookie有效，再进行CSRF攻击。 P3P头某些浏览器默认不发送Third-party Cookie确实能在一定程度上防止CSRF的发生，但是W3C有一项关于隐私的标准，全称为The Platform for Privacy Preferences，简称P3P。 如果网站返回给浏览器的HTTP中包含P3P头，将允许浏览器跨域发送Third-party Cookie。在一些网站中，P3P主要用于类似广告等需要跨域访问的页面，但是一旦设置了P3P头，此影响将扩大到该域的所有页面中，因为cookie是以域和path为单位的，从某种程度上来说，P3P头将增加CSRF攻击成功的概率。 示例：现在有www.a.com和www.b.com两个域，在www.b.com上，有一个页面包含一个指向www.a.com的``标签。http://www.b.com/p3p.html 的代码为： http://www.a.com/p3p_cookie.php 会对a.com这个域设置cookie，代码如下： 当请求 http://www.b.com/p3p.html 时，&lt;img&gt;标签会跨域去请求 http://www.a.com/p3p_cookie.php ，这个脚本会尝试Set-Cookie，这时浏览器会收到一个cookie： 如果Set-Cookie成功，当下次再访问 http://www.b.com/p3p.html 时，浏览器会发送这个cookie。但是由于这里存在跨域限制，因此这种情况下Set-Cookie是不会成功的，浏览器也不会发送cookie，不管是Session Cookie还是Third-party Cookie都一样不会被发送： 现在加上P3P头，修改 http://www.a.com/p3p_cookie.php 的内容为： 重复刚才的步骤，先访问 http://www.b.com/p3p.html ，浏览器收到cookie： 再访问 http://www.a.com： 发现IE访问 http://www.a.com/p3p_cookie.php 时发送了Third-party Cookie。 P3P头的加入改变了a.com的隐私策略，从而使得IE不再阻止Third-party Cookie的跨域发送。由于P3P头的应用很广泛，因此不能认为靠浏览器的拦截机制就可以阻止Third-party Cookie的发送。 POST与GET在防御CSRF的时候有时候存在一个误解，有的开发人员认为CSRF攻击只能由GET请求发起，只要把一些重要的操作改为POST提交方式就可以防御CSRF攻击。 会存在这种误解的原因在于，很多时候CSRF是利用&lt;img&gt;、&lt;iframe&gt;的src属性来发起GET请求，这种属性不能发起POST请求。不过有一些网站的某些操作并未区分GET和POST，攻击者可以用GET方式对表单的提交地址进行请求。比如在PHP中，如果在服务端使用$_REQUEST而不是$_POST来获取数据就会出现这个问题。假设有一个表单： 攻击者可以尝试用GET方式提交 http://www.xxx.com/delete.php?id=123456 如果服务端的代码没有使用$_POST而是使用$_REQUEST来获得提交的数据，就会使这个请求成功。 但是如果服务器区分了$_POST和$_REQUEST，攻击者还可以构造出POST请求来提交： 这个页面会自动用POST方法提交表单，而且如果放在一个width和height都为0的不可见iframe中，用户很难察觉到。 Flash CSRFFlash也可以发起网络请求，GET或POST。 比如： 除了URLRequest外，Flash还可以用getURL、loadVars等方式发起请求，如： 在IE6、IE7中，Flash发起网络请求均可以带上本地cookie，但从IE8开始，Flash发起网络请求不会发送本地cookie。 CSRF的防御验证码验证码被认为是对抗CSRF最简单有效的方法。 在CSRF攻击过程中，用户在不知情的情况下发起了请求。如果使用验证码，可以强制用户与网站进行交互才能进行正确的请求。所以利用验证码这种方式可以很容易的防止CSRF的发生。 但是，一般出于交互体验的考虑，不可能在所以请求中都加入验证码的限制。因此验证码只能是作为一种防止CSRF的辅助手段，不能作为主要的解决方案。 Referer Check(来源检查)Referer Check可以用来检查请求是否来自于合法的源。 常见的互联网应用中，页面与页面之间都具有一定的逻辑关系，这就使得每个正常请求的referer具有一定的规律。比如在一个论坛中发帖，用户一般需要登陆后访问具有发帖功能的页面，这就使得这些这些发帖的请求的referer为发帖表单所在的页面，这都是有规律可循的。所以如果发现referer不是这些页面，甚至不是发帖网站的域，则很有可能是CSRF在作怪。 不过即使我们能够通过检查referer是否合法来判断用户是否被CSRF攻击，但这还不够。因为服务器并非在任何时候都能接收到referer，很多情况下出于对隐私的保护，限制了referer的发送。在某些情况下，浏览器甚至不发送referer，比如从https跳转到http。 而且在flash的某一些版本中，曾经可以自定义referer发送，不过后来的新版本取消了这种行为，但我们很难保证所有用户都更新到了新版本。 综上几点，利用referer来防御CSRF完全是不够的，不过可以利用referer来有效地监控CSRF的发生，第一时间发现LD并修复它。 Token刚才讨论的几种防止CSRF攻击的解决方案都有这样那样的不足，在实际中很少被采用，目前业界比较通用的做法是使用Token。在详细说明Token之前先来看一下为什么CSRF攻击会成功。 在一次成功的CSRF攻击中，攻击者要正确构造出URL和参数值，否则攻击无法成功。那么如果参数是加密的或者随机的，攻击者无法猜测到，就可以有效防止CSRF攻击了，这就是“不可预测性原则”的一种应用。 例如一个没有做任何处理的URL：http://www.xxx.com/delete.php?user=test&amp;id=123456现在把参数改成：http://www.xxx.com/delete.php?user=md5(salt+test)&amp;id=123456 在攻击者不知道salt和加密方式的情况下，无法构造出这个URL，CSRF攻击不会成功。在服务器端，可以从Session或者cookie中获得user=test，再加上salt后进行md5散列，确认请求的合法性。 加入随机值或加密值的思想是正确的，但是上面这种实现方法有一个缺点，URL很复杂很不友好，而且如果用户想收藏网址也会变得无效，其次这对数据采集和分析工作会造成障碍，因为数据采集和分析都需要明文的数据。 业界比较好的方法是采用我们上面说到的Token：http://www.xxx.com/delete.php?user=test&amp;id=123456&amp;token=[randomValue]Token的值需要足够随机，这个随机算法需要经过一番推敲和验证。Token的值只有用户和服务器两者你知我知，第三方是不能知道的。Token可以放在Session Cookie或者本地cookie中。正是由于Token的存在，攻击者无法构造出正确的URL。 Token需要同时放在表单和Session(Session Cookie或服务器Session)或本地Cookie中，提交表单时，服务器需要验证表单中的Token和Session Cookie或服务器Session(或本地Cookie)中的Token是否一致，如果一致则认为是合法请求，如果不一致或有任何一个为空，就判断为不合法，此时有可能是CSRF攻击。 比如这样： 表单中有一个hidden隐藏字段，value是Token的值。下面是淘宝的一个表单的内容，在表单中有一个hidden隐藏域，正是Token。 在Session Cookie也可以看到_tb_token_： Token的使用原则 Token一定要足够随机。 可以允许在一个用户的有效生命周期内，在Token消耗掉之前都使用同一个Token，但如果用户提交了则这个Token已消耗，应该重新生成一个Token。 如果Token保存在Cookie而非服务器Session中，会有一个问题。如果用户同时打开几个标签页，当某个页面消耗掉Token后，其他页面的Token还是原来的，如果这时候提交会造成Token无效。这时可以考虑每个页面生成一个不同的Token。 注意Token的保密性，不要把Token放在URL中，不然会有通过referer泄露的危险。最好尽量把Token放在表单中，提交方式为POST。但是如果页面存在XSS LD，Token还是有可能泄露。如果存在XSS，攻击几乎可以模拟用户做任何操作。加入Token只能防止CSRF，对XSS无效。","categories":[{"name":"web前端","slug":"web前端","permalink":"https://nullcc.github.io/categories/web前端/"}],"tags":[{"name":"CSRF","slug":"CSRF","permalink":"https://nullcc.github.io/tags/CSRF/"},{"name":"web前端攻防","slug":"web前端攻防","permalink":"https://nullcc.github.io/tags/web前端攻防/"}]}]}